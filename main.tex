% -*- LaTeX -*-
\documentclass[11pt,a4paper]{article}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{bm}
\usepackage{natbib}
\usepackage{tabularx}
\usepackage{graphicx,lscape}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{enumitem}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{color}
\usepackage{cleveref}
\usepackage{longtable}



\def\OPFONT{\rm}
\def\E{\mathop{\OPFONT I\hspace{-1.5pt}E\hspace{.13pt}}}
\def\P{\mathop{\OPFONT I\hspace{-1.5pt}P\hspace{.13pt}}}
\def\CAS{\stackrel{\OPFONT a.s.}{\longrightarrow}}
\def\CMS{\stackrel{\OPFONT m.s.}{\longrightarrow}}
\def\CP{\stackrel{p}{\longrightarrow}}
\def\CD{\stackrel{d}{\longrightarrow}}
\def\ED{\stackrel{d}{=}}
\def\SA{\stackrel{A}{\sim}}
\newcommand{\VAR}{\mathop{\OPFONT Var}\nolimits}
\newcommand{\COV}{\mathop{\OPFONT Cov}\nolimits}
\newcommand{\CORR}{\mathop{\OPFONT Corr}\nolimits}
\newcommand{\SPAN}{\mathop{\OPFONT Span}\nolimits}
\newcommand{\RANK}{\mathop{\OPFONT Rank}\nolimits}
\newcommand{\DIAG}{\mathop{\OPFONT Diag}\nolimits}
\newcommand{\TRACE}{\mathop{\OPFONT Trace}\nolimits}
\newcommand{\VEC}{\mathop{\OPFONT vec}\nolimits}

\newcommand{\veps}{\varepsilon}
\newcommand{\Bveps}{\boldsymbol{\varepsilon}}
\newcommand{\Balpha}{\boldsymbol{\alpha}}
\newcommand{\Bbeta}{\boldsymbol{\beta}}
\newcommand{\Bgamma}{\boldsymbol{\gamma}}
\newcommand{\Bdelta}{\boldsymbol{\delta}}
\newcommand{\Bepsilon}{\boldsymbol{\epsilon}}
\newcommand{\Bzeta}{\boldsymbol{\zeta}}
\newcommand{\Beta}{\boldsymbol{\eta}}
\newcommand{\Btheta}{\boldsymbol{\theta}}
\newcommand{\Bvartheta}{\boldsymbol{\vartheta}}
\newcommand{\Biota}{\boldsymbol{\iota}}
\newcommand{\Bkappa}{\boldsymbol{\kappa}}
\newcommand{\Blambda}{\boldsymbol{\lambda}}
\newcommand{\Bmu}{\boldsymbol{\mu}}
\newcommand{\Bnu}{\boldsymbol{\nu}}
\newcommand{\Bxi}{\boldsymbol{\xi}}
\newcommand{\Bpi}{\boldsymbol{\pi}}
\newcommand{\Bvarpi}{\boldsymbol{\varpi}}
\newcommand{\Brho}{\boldsymbol{\rho}}
\newcommand{\Bvarrho}{\boldsymbol{\varrho}}
\newcommand{\Bsigma}{\boldsymbol{\sigma}}
\newcommand{\Bvarsigma}{\boldsymbol{\varsigma}}
\newcommand{\Btau}{\boldsymbol{\tau}}
\newcommand{\Bupsilon}{\boldsymbol{\upsilon}}
\newcommand{\Bphi}{\boldsymbol{\phi}}
\newcommand{\Bvarphi}{\boldsymbol{\varphi}}
\newcommand{\Bchi}{\boldsymbol{\chi}}
\newcommand{\Bpsi}{\boldsymbol{\psi}}
\newcommand{\Bomega}{\boldsymbol{\omega}}

\newcommand{\BGamma}{\boldsymbol{\Gamma}}
\newcommand{\BDelta}{\boldsymbol{\Delta}}
\newcommand{\BTheta}{\boldsymbol{\Theta}}
\newcommand{\BLambda}{\boldsymbol{\Lambda}}
\newcommand{\BXi}{\boldsymbol{\Xi}}
\newcommand{\BPi}{\boldsymbol{\Pi}}
\newcommand{\BSigma}{\boldsymbol{\Sigma}}
\newcommand{\BUpsilon}{\boldsymbol{\Upsilon}}
\newcommand{\BPhi}{\boldsymbol{\Phi}}
\newcommand{\BPsi}{\boldsymbol{\Psi}}
\newcommand{\BOmega}{\boldsymbol{\Omega}}

\newcommand{\MBzero}{\mathbf{0}}
\newcommand{\MBone}{\mathbf{1}}
\newcommand{\MBA}{\mathbf{A}}
\newcommand{\MBa}{\mathbf{a}}
\newcommand{\MBB}{\mathbf{B}}
\newcommand{\MBb}{\mathbf{b}}
\newcommand{\MBC}{\mathbf{C}}
\newcommand{\MBc}{\mathbf{c}}
\newcommand{\MBD}{\mathbf{D}}
\newcommand{\MBd}{\mathbf{d}}
\newcommand{\MBE}{\mathbf{E}}
\newcommand{\MBe}{\mathbf{e}}
\newcommand{\MBF}{\mathbf{F}}
\newcommand{\MBf}{\mathbf{f}}
\newcommand{\MBG}{\mathbf{G}}
\newcommand{\MBg}{\mathbf{g}}
\newcommand{\MBH}{\mathbf{H}}
\newcommand{\MBh}{\mathbf{h}}
\newcommand{\MBI}{\mathbf{I}}
\newcommand{\MBi}{\mathbf{i}}
\newcommand{\MBJ}{\mathbf{J}}
\newcommand{\MBj}{\mathbf{j}}
\newcommand{\MBK}{\mathbf{K}}
\newcommand{\MBk}{\mathbf{k}}
\newcommand{\MBL}{\mathbf{L}}
\newcommand{\MBl}{\mathbf{l}}
\newcommand{\MBM}{\mathbf{M}}
\newcommand{\MBm}{\mathbf{m}}
\newcommand{\MBN}{\mathbf{N}}
\newcommand{\MBn}{\mathbf{n}}
\newcommand{\MBO}{\mathbf{O}}
\newcommand{\MBo}{\mathbf{o}}
\newcommand{\MBP}{\mathbf{P}}
\newcommand{\MBp}{\mathbf{p}}
\newcommand{\MBQ}{\mathbf{Q}}
\newcommand{\MBq}{\mathbf{q}}
\newcommand{\MBR}{\mathbf{R}}
\newcommand{\MBr}{\mathbf{r}}
\newcommand{\MBS}{\mathbf{S}}
\newcommand{\MBs}{\mathbf{s}}
\newcommand{\MBT}{\mathbf{T}}
\newcommand{\MBt}{\mathbf{t}}
\newcommand{\MBU}{\mathbf{U}}
\newcommand{\MBu}{\mathbf{u}}
\newcommand{\MBV}{\mathbf{V}}
\newcommand{\MBv}{\mathbf{v}}
\newcommand{\MBW}{\mathbf{W}}
\newcommand{\MBw}{\mathbf{w}}
\newcommand{\MBX}{\mathbf{X}}
\newcommand{\MBx}{\mathbf{x}}
\newcommand{\MBY}{\mathbf{Y}}
\newcommand{\MBy}{\mathbf{y}}
\newcommand{\MBZ}{\mathbf{Z}}
\newcommand{\MBz}{\mathbf{z}}
\newcommand{\bed}{\begin{displaymath}}
\newcommand{\eed}{\end{displaymath}}
\newcommand{\bel}{\begin{eqalign}}
\newcommand{\eel}{\end{eqalign}}
\newcommand{\bee}{\begin{equation}}
\newcommand{\eee}{\end{equation}}
\newcommand{\tp}{\mathsf{T}}
\newcommand{\si}{$^{*}$}
\newcommand{\ssi}{$^{**}$}
\newcommand{\sssi}{$^{***}$}
\newcommand{\sn}[1]{$\times10^{#1}$}
\newcommand*\oline[1]{%
  \vbox{%
    \hrule height 0.1pt%                  % Line above with certain width
    \kern0.25ex%                          % Distance between line and content
    \hbox{%
      \kern-0.1em%                        % Distance between content and left side of box, negative values for lines shorter than content
      \ifmmode#1\else\ensuremath{#1}\fi%  % The content, typeset in dependence of mode
      \kern-0.1em%                        % Distance between content and left side of box, negative values for lines shorter than content
    }
  }
}
%\numberwithin{equation}{section}
\theoremstyle{definition}
\newtheorem{Ass}{Assumption}%[section]
\newtheorem{thm}{Theorem}%[section]
\newtheorem{lem}{Lemma}%[section]
\newtheorem{coro}{Corollary}%[section]
\newtheorem{Remark}{Remark}%[section]
\newtheorem{Discussion}{Discussion}%[section]
\newtheorem{Prop}{Proposition}
\newtheorem{defin}{Definition}
\newtheorem{prove}{proof}
\newtheorem{Result}{Result}
\newtheorem{Case}{Case}%[section]
\newtheorem{assump}{Assumption}
\newenvironment{myassump}[2][]
  {\renewcommand\theassump{#2}\begin{assump}[#1]}
  {\end{assump}}

\renewcommand{\baselinestretch}{1.2}
\newcommand{\citeposs}[1]{\citeauthor{#1}'s \citeyearpar{#1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\begin{document}

\vspace{-1em}
\title{\huge Regularized Estimation in Dynamic Panel with a Multifctotor Structure~(Midterm Report)\\\vspace{0.3em}}
\author{\Large{
Shou-Yung Yin\thanks{Department of Economics, National Taipei University. Email: syyin@mail.ntpu.edu.tw.}}\vspace{1em}}
\date{\vspace{0.3em}\today}
\maketitle


\begin{abstract}
In this report, we derive the limiting behavior~(consistency) of the regularized estimation in dynamic panel with a multifctotor structure. Compared with the OLS type common correlated effects estimator~(CCE), the regularized CCE approach does not need to assume that the sample size should be larger than the number of the variables of the approximated model. We show that as long as the dynamic property is not so persistent. The consistency can be established under regularized CCE approach. The simulation results confirm that regularized CCE approach can outperform OLS CCE approach and is robust even when $p$ is relatively small. 
\end{abstract}

%\\
\noindent
{\bf Keywords:} Cross-sectional Dependence, Common Correlated Effects, Regularization, Factor Structure.


\vspace{1em}

\thispagestyle{empty}
\newpage
\pagenumbering{arabic}

\section{Introduction}\label{Sec:Intro}
In this paper we propose a regularized common correlated effects~(CCE) estimator for dynamic panel data with a multifactor error structure. In the literature, the presence of multifactor error structure provides not only the phenomenon of cross-sectional dependence but also a possible correlation between regressors and unobserved factors. These issues have been studied in \citet{Pesaran2006} and are extended to dynamic panel in \citet{Chudik2015}. In the latter case, an infinite lagged order of cross-sectional averaged dependent variable and regressors are used as a proxy of unobserved factors and it is practically impossible. Therefore, a truncated version of regression is suggested by \citet{Chudik2015}. However, even though we can truncate the regression for each individual, it is still possible that the number of regressors is greater than the sample size. For example, let $p$ denote the number of regressors and $k_T$ denote the truncated order, we can still have $p\times k_T>T$ when both $p$ and $k_T$ are less than $T$. Accordingly, we refer to this situation as a high-dimensional problem and it has not been discussed under this framework.

Over the last few years, a great deal of attention has been focused on the high-dimensional regression, more specifically, the case that $p>T$. To deal with this issue, sparsity of coefficient space is often imposed then $\ell_1$ penalization procedure can be implemented to obtain a consistent estimate. Several related approaches have been introduced, see \citet{Efron2004}, \citet{Zhao2006}, \citet{Candes2007}, \citet{Zou2006}, \citet{Yuan2006}, \citet{Fan2001}, among others. The details of theoretical properties can be founded in \citet{Bickel2009} and \citet{buhlmann2011statistics}.

However, these researches are based on an important assumption that the samples are independent and identically distributed~(i.i.d). The assumption of i.i.d. apparently is challenged because of the presence of serial correlation and cross-sectional dependence in a dynamic panel with a multifactor structure.  Recently, a few papers have attempted to relax the i.i.d assumption and discussed the impacts on the estimate. \citet{Basu2015} introduced using spectral properties of stationary process to measure the stability which is used to derive the upper bounds of the estimation error and to obtain the consistency under a VAR framework. \citet{Han2020} established the consistency using a dependence adjusted norm and did not assume the restricted eigenvalue condition on either the sample or the population covariance matrix.\footnote{\citet{Wang2007} established the consistency of lasso under time series framework when $p<T$.}

Furthermore, the statistical inference in terms of $p$-values and constructing confidence regions is less developed after a penalization procedure. The lack of statistical inference accounting uncertainty makes the usage of such penalization procedure less appealing. To fill this gap, \citet{Zhang2014}, \citet{VanDeGeer2014} and \citet{Javanmard2014} have suggested several ways including de-biased method and multi-sample splitting approach. However, their methods are also based on i.i.d. assumption.

To handle the high-dimensional problem under dynamic panel data with a multifactor error structure framework, we attempt to propose a regularized CCE estimator. Particularly, we rewrite the original regression with factor structure as an approximation by using an infinite lagged order of cross-sectional averaged dependent variable and regressors. To apply the regularization, we relax the sparsity of the slope coefficient space by discussing the decay rate of slope coefficients and use the dependence adjusted norm suggested by \citet{Han2020} to characterize the dependence among the regressors caused by common factors~(common correlated effects).  Then we can bound the estimation error without using restricted eigenvalue assumption and establish the consistency. The advantages of the proposed method are threefold: (i) The  approach inherits the advantage of the CCE estimator proposed by \citet{Chudik2015}, it can eliminate the unobserved factors asymptotically as the number of cross-sectional units, $N$ goes to infinity. This implies that there is no endogenous problem caused by the correlation between factors and regressors. (ii) This approach can handle with the problem when  $p\times k_T>T$ or even $p>T$ as $T\rightarrow\infty$ under a certain sparsity approximation. (iii) Instead of using the same approximation of unobserved factors  for all individuals, the proposed approach can have different shrinkage property for each individual. Therefore we can expect that the estimate is much more efficient than the CCE estimator.

The remainder of this proposal is organized as follows. In Section 2, we introduce the model and briefly review the CCE estimator. In Section 3, we first build the link between augmented regression used to approximate factors and high-dimensional regression, and introduce the regularized estimator. In Section 4, we use simulation to investigate the finite property of the proposed estimator . Section 5 summarizes 
the preliminary findings.


\section{Model and the Review of CCE Estimator}\label{Sec:model}
In this section, we first introduce the dynamic panel with weak exogenous regressors and a multifactor error structure and briefly discuss the main result from \citet{Chudik2015}.

\subsection{Dynamic Panels with Weak Exogenous Regressors and a Multifactor Error Structure}
Consider the following covariance stationary dynamic panels for $i=1,...,N$ and $t=1,...,T$:
\begin{align}
y_{it}&=\rho_iy_{it-1}+\MBx_{it}^{\tp}\Bbeta_{0i}+\MBx_{it-1}^{\tp}\Bbeta_{1i}+u_{it},\label{eq:y}\\
u_{it}&=\Bgamma_{i}^{\tp}\MBf_t+\epsilon_{it},\label{eq:u}
\end{align}
where $y_{it}$ denotes the dependent variable and $\MBx_{it}$ is a $p\times 1$ vector representing the regressors/covariates. To capture the dynamic effects, the lagged dependent variable and regressors are included. $\Bbeta_{0i}$ and $\Bbeta_{1i}$ represent  the corresponding slope coefficients which are unknown, and $u_{it}$ is an error with a multi-factor structure. More specifically, the factor structure is characterized by an unobserved common factors $\MBf_t$ which is an $r\times 1$ vector with the factor loadings $\Bgamma_i$. $\epsilon_{it}$ is an idiosyncratic error.

We further model $\MBx_{it}$ as the following specification:
\begin{align}
\MBx_{it}=\Balpha_iy_{it-1}+\BGamma_i^{\tp}\MBf_t+\MBv_{it}, \label{eq:x}
\end{align}
where $\Balpha_i$ is a $p\times 1$ vector of unknown coefficients, $\BGamma_i$ is an $r\times p$ matrix of factor loadings and $\MBv_{it}$ is a $p\times 1$ vector of idiosyncratic errors. Equations \eqref{eq:y} to \eqref{eq:x} are general enough for providing features including heterogeneous slope coefficients and possible correlation between regressors and multifactor errors.

\subsection{Common Correlated Effects Estimator}
The main interest of this project is to estimate the unknown coefficients $\Bpi_i=(\rho_i, \Bbeta_{0i}^{\tp}, \Bbeta_{1i}^{\tp})^{\tp}$. However, it is not trivial because the unobserved common factors play an important role leading to an endogeneity and therefore biased estimates could be obtained if these unobserved common factors are not controlled well. Common correlated effects estimator proposed by \cite{Chudik2015} is designed for overcoming this issue. 

Let $\MBz_{it}=(y_{it},\MBx_{it}^{\tp})_{}^{\tp}$ and we can rewrite equations \eqref{eq:y} to \eqref{eq:x}  as
\begin{align}
\MBA_{0i}\MBz_{it}=\MBA_{1i}\MBz_{it-1}+\MBC_{i}\MBf_{t}+\MBe_{it},
\end{align}
where $\MBC_{i}=(\Bgamma_i,\BGamma_i)^{\tp}$,
\begin{align*}
\MBA_{0i}=\begin{bmatrix}
1 & -\Bbeta_{0i}^{\tp}\\
\MBzero & \MBI
\end{bmatrix},\quad \MBA_{1i}=\begin{bmatrix}
\rho_i & \Bbeta_{1i}^{\tp}\\
\Balpha_i & \MBzero
\end{bmatrix},
\end{align*}
and $\MBe_{it}=(\epsilon_{it},\MBv_{it}^{\tp})^{\tp}$. By imposing the assumption that $\MBA_{0i}$ is invertible  for any $i$, we can obtain the following VAR(1) representation on $\MBz_{it}$,
\begin{align}
\MBz_{it}=\MBA_{i}\MBz_{it-1}+\MBA_{0i}^{-1}\MBC_{i}\MBf_{t}+\MBe_{z,it},
\end{align}
where $\MBA_{i}=\MBA_{0i}^{-1}\MBA_{1i}$ and $\MBe_{z,it}=\MBA_{0i}^{-1}\MBe_{it}$. Assume that the support of $\MBA_{i}$ lies in the unit circle for all $i$, this VAR(1) representation can further rewrite as
\begin{align}
\MBz_{it}=\sum_{\ell=0}^{\infty}\MBA_{i}^{\ell}\left(\MBA_{0i}^{-1}\MBC_{i}\MBf_{t-\ell}+\MBe_{z,it-\ell}\right).
\end{align}
Under the large $N$ framework, the cross-sectional average of $\MBz_{it}$ follows that
\begin{align}
\bar\MBz_{t}&=\sum_{\ell=0}^{\infty}\mathbb{E}\left(\MBA_{i}^{\ell}\MBA_{0i}^{-1}\MBC_{i}\right)\MBf_{t-\ell}+O_p(N^{-1/2})\notag\\
                          &=\BLambda(L)\MBC\MBf_{t}+O_p(N^{-1/2}),\label{eq:zbar}
\end{align}
where $\BLambda(L)=\mathbb{E}\left(\sum_{\ell=0}^{\infty}\MBA_{i}^{\ell}\MBA_{0i}^{-1}L^{\ell}\right)$ and $\MBC=\mathbb{E}(\MBC_{i})$. Multiplying the above equation by the inverse of $\BLambda(L)$ yields the key relationship between the unobserved factors and the cross-sectional average of $\MBz_{it}$, that is,
\begin{align}
\MBf_{t}=(\MBC^{\tp}\MBC)^{-1}\MBC^{\tp}\BLambda(L)^{-1}\bar\MBz_{t}+O_p(N^{-1/2}).\label{eq:fhat}
\end{align}
The above equation provides that the linear combination of cross-sectional average of $\MBz_{it}$ and their lags can be regarded as proxies of $\MBf_t$ when the rank condition holds~($\RANK(\MBC)=r$). By plug-inning \eqref{eq:fhat} into \eqref{eq:y}, we can obtain the following result,
\begin{align}
y_{it}&=\rho_iy_{it-1}+\MBx_{it}^{\tp}\Bbeta_{0i}+\MBx_{it-1}^{\tp}\Bbeta_{1i}+\Bdelta_{i}^{\tp}(L)\bar\MBz_{t}+\epsilon_{it}+O_p(N^{-1/2}),\label{eq:y*}
\end{align}
where $\Bdelta_{i}^{\tp}(L)=\sum_{\ell=0}^{\infty}\Bdelta_{i\ell}^{\tp}L^{\ell}=\Bgamma_{i}^{\tp}\left((\MBC^{\tp}\MBC)^{-1}\MBC^{\tp}\BLambda(L)^{-1}\right)$. Practically, it is impossible to consider an infinite lagged $\bar\MBz_{t}$, therefore, a common strategy is to consider a truncated version using a finite lagged $\bar\MBz_{t}$. Theoretically, the truncated order can be much lower than $T$, say $T^{1/3}$, and it still provides nice statistical properties in terms of consistency and asymptotic normality. Therefore, we can consider the following truncated augmented regression:
\begin{align}
y_{it}&=\rho_iy_{it-1}+\MBx_{it}^{\tp}\Bbeta_{0i}+\MBx_{it-1}^{\tp}\Bbeta_{1i}+\sum_{\ell=0}^{k_{T}}\Bdelta_{i\ell}^{\tp}\bar\MBz_{t-\ell}+\tilde\epsilon_{it},\label{eq:yhat}
\end{align}

Now, let
\begin{align}
\MBM_{\mathbf{z}}=\MBI_{T-k_T} - \bar{\MBQ}(\bar{\MBQ}'\bar{\MBQ})^{-}\bar{\MBQ}',
\end{align}
where
\begin{align*}
\bar{\MBQ}=\begin{bmatrix}
1& \bar\MBz_{k_T+1}^{\tp}& \bar\MBz_{k_T}^{\tp}&\dots& \bar\MBz_{1}^{\tp}\\
1& \bar\MBz_{k_T+2}^{\tp}& \bar\MBz_{k_T+1}^{\tp}&\dots& \bar\MBz_{2}^{\tp}\\
\vdots & \vdots & \vdots & &\vdots \\
1& \bar\MBz_{T}^{\tp}& \bar\MBz_{T-1}^{\tp}&\dots& \bar\MBz_{T-k_T}^{\tp}
\end{bmatrix}
\end{align*}
and $(\bar{\MBQ}'\bar{\MBQ})^{-}$ denotes the Moore-Penrose generalized inverse of $\bar{\MBQ}'\bar{\MBQ}$. Also let
\begin{align*}
\BXi_i =
\begin{bmatrix}
y_{ik_T} & \MBx_{ik_T+1}^{\tp} & \MBx_{ik_T}^{\tp}\\
y_{ik_T+1} & \MBx_{ik_T+2}^{\tp} & \MBx_{ik_T+1}^{\tp}\\
\vdots & \vdots & \vdots\\
y_{iT-1} & \MBx_{iT}^{\tp} & \MBx_{iT-1}^{\tp}\\
\end{bmatrix}
\end{align*}
and $\MBy_{i}=(y_{ik_T+1}, \cdots,  y_{iT})^{\tp}$.
We obtain the CCE estimator of $\Bpi_i=(\rho_i, \Bbeta_{0i}^{\tp}, \Bbeta_{1i}^{\tp})^{\tp}$:
\begin{equation}
\hat\Bpi_{i} = (\BXi_i^{\tp} \MBM_{\MBz} \BXi_i)^{-1}(\BXi_i^{\tp} \MBM_{\MBz} \MBy_i).
\label{eq:CCE}
\end{equation}
Thus, the CCEMG estimator is
\begin{equation}
\hat\Bpi_{\mathrm{MG}} = \frac{1}{N}\sum_{i=1}^N \hat\Bpi_{i}.
\label{eq:CCEMG}
\end{equation}

\begin{Remark}
Under the assumptions stated in \citet{Chudik2015} and when the rank condition holds, the CCE estimator defined in equation \eqref{eq:CCE}  is a consistent estimator, that is $\hat\Bpi_{i}\CP\Bpi_{i}$ when $(N,T,k_T)\rightarrow\infty$ jointly such that $k_T^3/T\rightarrow\kappa$, $0<\kappa<\infty$. The intuition is that the effects from unobserved factors can be removed asymptotically when $N\rightarrow\infty$. Therefore, individual's estimation can be treated as a time series regression and the consistency is based on large $T$. This nice property can be directly applied to establish the asymptotic normality of the CCEMG estimator defined in equation \eqref{eq:CCEMG}.  That is $\sqrt{N}(\hat\Bpi_{\mathrm{MG}}-\Bpi)\CD\mathrm{N}(\MBzero, \BOmega_{\Bpi})$ and $\BOmega_{\Bpi}$ is a symmetric nonnegative definite matrix characterizing the random deviation of $\Bpi_i$ and $\Bpi=\mathbb{E}(\Bpi_i)$.
\end{Remark}

\begin{Remark}
The rank condition plays an important role on deriving the consistency and asymptotic normality of $\hat\Bpi_{\mathrm{MG}}$. When the rank condition does not hold, a more restrictive assumption that the unobserved factors are serially uncorrelated should be imposed. \citet{Yin2019} have discussed this assumption and showed that CCEMG estimator is still valid when there is a small number of factors that they are persistent. In the whole paper, we assume that the rank condition holds.
\end{Remark}


\section{Regularized CCE Estimator}\label{Sec:plan1}
The CCE estimator proposed by \cite{Pesaran2006} and \cite{Chudik2015} provides a simple idea by using a projection matrix to control the unobserved factors in static and dynamic panels respectively. However, in the dynamic panel model, when the factors are represented by an infinite lagged order of $\bar\MBz_t$ in equation \eqref{eq:y*}, the infinite order refers to a high-dimensional regression issue that has not been discussed yet in this literature. In their paper, they avoid dealing with this issue by firstly considering a truncated version of infinite lagged order of $\bar\MBz_t$ and secondly calculating the projection matrix with Moore-Penrose generalized inverse when $\bar{\MBQ}^{\tp}\bar{\MBQ}$ is deficient. However, practically, it does not imply that \eqref{eq:yhat} would not suffer from the dramatic loss of degree of freedom. 

To emphasis this problem, we rewrite \eqref{eq:y*} as the following high-dimensional representation:
\begin{align}
y_{it}&=\Xi_{it}^{\tp}\Bpi_i+\tilde\Bdelta_{i}^{\tp}\tilde\MBz_{t}+\epsilon_{it}+O_p(N^{-1/2}),\label{eq:yhigh}
\end{align}
where $\Xi_{it}=(y_{it-1},\MBx_{it}^{\tp},\MBx_{it-1}^{\tp})^{\tp}$, $\tilde\Bdelta_{i}=(\Bdelta_{i0}^{\tp},\Bdelta_{i1}^{\tp},...,\Bdelta_{ik}^{\tp})^{\tp}$ and $\tilde\MBz_{t}=(\bar\MBz_{t}^{\tp},\bar\MBz_{t-1}^{\tp},...,\bar\MBz_{t-k}^{\tp})^{\tp}$. Recall that the dimension of $\MBx_{it}$ is $p$, we can immediately obtain that $\dim(\Xi_{it})=2p+1$ and $\dim(\tilde\MBz_{t})=p(k_{T}+1)$. Since $k_{T}\rightarrow\infty$, equation \eqref{eq:yhigh} can be regarded as a partial high-dimensional problem but  still have a finite dimension of $\MBx_{it}$ when $p$ is fixed. If we further let $p\rightarrow\infty$, \eqref{eq:yhigh} becomes a purely high-dimensional regression.

%\begin{Remark}
%It is worth to mention that under fixed $p$ framework,  
%\end{Remark}

%\subsection{When $p$ Is Fixed}
Under fixed $p$ and large $k_T$ framework, $k_T$ can be assumed that grows as fast as the sample size $T$. Estimation and inference can be made under the sparsity on the coefficients $\tilde\Bdelta_{i}$ for each $i$. In general, we assume that the number of elements in $S_{\delta}=\{j:\tilde\Bdelta_{i}\neq 0\}$ is small and does not grow with the sample size. Then we can impose a regularization with $\ell_1$ norm, that is we add an $\ell_1$ penalty to the linear regression. In this project, the problem can be written as:
\begin{align}
\min_{\Bpi_i\in\mathbb{R}^{2p+1},\tilde\Bdelta_{i}\in\mathbb{R}^{p(k+1)}}\frac{1}{2}\sum_{t=1}^T(y_{it}-\Xi_{it}^{\tp}\Bpi_i-\tilde\Bdelta_{i}^{\tp}\tilde\MBz_{t})^2+\lambda||\tilde\Bdelta_{i}||_1,\label{eq:lasso1}
\end{align}
where $\lambda\geq 0$ is a tuning parameter and it is chosen by a rule depending on the dependency of regressors. 

\begin{Remark}
Instead of pure sparsity, the above problem can be referred as the transformed generalized lasso, see \cite{Tibshirani2011}, where the sparsity has a certain structure corresponding to a desired behavior for the slope coefficients. That is in equation \eqref{eq:lasso1}, the penalty only covers a part of the coefficients. Clearly, we can have a solution for $\Bpi_i$,
\begin{align}
\hat\Bpi_i=(\BXi_i^{\tp}\BXi_i)^{-1}\BXi_i^{\tp}(\MBy-\tilde{\MBZ}\hat{\tilde\Bdelta}_{i}).\label{eq:est_pi}
\end{align}
Therefore, the above problem \eqref{eq:lasso1} can be rewritten as 
\begin{align}
\min_{\tilde\Bdelta_{i}\in\mathbb{R}^{p(k+1)}}\frac{1}{2}||\MBM_{\Xi}\MBy_{i}-\MBM_{\Xi}\tilde{\MBZ}\tilde\Bdelta_{i}||_2^2+\lambda||\tilde\Bdelta_{i}||_1\label{eq:lasso1_1}
\end{align}
where $\MBM_{\Xi}=\MBI-\MBP_{\Xi}$ and $\MBP_{\Xi}=\BXi_i(\BXi_i^{\tp}\BXi_i)^{-1}\BXi_i^{\tp}$ the projection matrix on the column space of $\BXi_i$.
\end{Remark}

\begin{Remark}
In this project, the main interest is $\Bpi_i$. However, to obtain the estimate of $\Bpi_i$, we need to plugin the estimates of $\tilde\Bdelta_{i}$ into equation \eqref{eq:est_pi}.  
Accordingly, we can observe that under fixed $p$ and large $k$ framework, the asymptotic property of $\hat\Bpi_i$ should be affected by the limited behavior of $\hat{\tilde\Bdelta}_{i}$ even though there is no sparsity assumption on the coefficient space of $\Bpi_i$.
\end{Remark}


\begin{Remark}
Alternatively, we can let $p$ grows faster than the sample size $T$. Let $\Btheta_i=(\Bpi_i^{\tp},\tilde\Bdelta_{i}^{\tp})^{\tp}$ and $\MBw_{it}=(\Xi_{it}^{\tp},\tilde\MBz_{t}^{\tp})^{\tp}$, we can further impose the sparsity on $\Btheta_i$, that is $S_{\Btheta}=\{j:\Btheta_i\neq 0\}$ and $|S_{\Btheta}|$ is fixed. Then we have a regular lasso problem,
\begin{align}
\min_{\Btheta_i\in\mathbb{R}^{k}}\frac{1}{2}\sum_{t=1}^T(y_{it}-\MBw_{it}^{\tp}\Btheta_i)^2+\lambda||\Btheta_i||_1.\label{eq:lasso2}
\end{align}
\end{Remark}

\begin{Remark}
The above problem is quite related to \cite{Hansen2019} who consider a panel partial factor model~(PPFM). More specifically, PPFM is defined as,\footnote{For simplicity, we remove time effects and individual effects and use their notations.}
\begin{align}
y_{it} &= \Balpha \MBd_{it}+\Bxi_i^{\tp}\MBf_t+\Btheta^{\tp}\MBu_{it}+\epsilon_{it},\label{eq:HL_y}\\
\MBd_{it} &= \Bdelta_{di}^{\tp}\MBf_t+\Bgamma_d^{\tp}\MBu_{it}+\Beta_{it},\label{eq:HL_d}\\
\MBx_{it} &= \Blambda_i^{\tp}\MBf_t+\MBu_{it}.\label{eq:HL_x}
\end{align}
In equation \eqref{eq:HL_y}, $\MBd_{it}$ represents a low-dimensional treatment variables, $\MBf_t$ denotes unobserved factors, and $\MBu_{it}$ is a high-dimensional unobserved effects from $\MBx_{it}$. In their paper, they focus on estimating $\Balpha$. The main difference is that we do not restrict the dimension of $\MBd_{it}$ and it is an important assumption which is used to obtain asymptotic normality. Instead, we can allow that $\dim(\MBd_{it})\rightarrow\infty$.
\end{Remark}

\section{Preliminary Results}

\subsection{Simulation}
In this section, Monte Carlo simulation is conducted to investigate the small sample properties. More specifically, we discuss the performance of the proposed estimator under different values of $p$~(the number of regressors). We first introduce the specification of the data generating process and then we discuss the resaults.

\noindent
\textbf{Data Generating Process:}
We consider the following data generating process,
for $i=1,...,N$ and $t=1,...,T$,
\begin{align*}
y_{it}=&\rho_iy_{it-1}+\sum_{j=1}^p\beta_{0ij}x_{ijt}+\sum_{j=1}^p\beta_{1ij}x_{ijt-1}+u_{it},\\
u_{it}=&\sum_{\ell=1}^{r_{s}}\gamma_{i\ell}f_{\ell t}+\sum_{\ell=1}^{r_{w}}\vartheta_{i\ell}n_{\ell t}+\veps_{it},
\end{align*}
and
\begin{align*}
x_{ijt}=\alpha_{ij}y_{it-1}+\sum_{\ell=1}^{r_s}\Gamma_{ij\ell}f_{\ell t}+\sum_{\ell=1}^{r_{w}}\Theta_{ij\ell}n_{\ell t}+v_{ijt}.
\end{align*}
We define two types of unobserved factors $f_{\ell t}$ and $n_{\ell t}$ representing strong and weak factors. They are generated  as an AR(1) process with unit variance:
\begin{align*}
f_{\ell t} =&\rho_{f\ell}f_{t-1, \ell} +\nu_{f\ell t},\quad \nu_{f\ell t}\sim \mathrm{IIDN} (0, 1-\rho^{2}_{f\ell}), \\
n_{\ell t} =&\rho_{n\ell}n_{t-1, \ell} +\nu_{n\ell t},\quad \nu_{n\ell t}\sim \mathrm{IIDN} (0, 1-\rho^{2}_{n\ell}).
\end{align*}
We assume that there are $r_{s}$ strong factors, therefore, the sum of the absolute values of the corresponding loadings is unbounded in $N$ as we mentioned in Definition 1, and it is generated as
\begin{align*}
\gamma_{i\ell} =& \gamma_{\ell}+\iota_{i\ell},\quad \iota_{i\ell} \sim \mathrm{IIDN} (0, \sigma^{2}_{\gamma\ell}),\\
\Gamma_{ij\ell} =& \Gamma_{j\ell}+\xi_{ij\ell},\quad \xi_{ij\ell} \sim \mathrm{IIDN} (0, \sigma^{2}_{\gamma x \ell}),
 \end{align*}
for $\ell=1,.., r_s$ and $i=1,\ldots,N$. We set $\sigma^{2}_{\gamma\ell}=\sigma^{2}_{\Gamma j\ell}=0.2^2$. To ensure that the variance of $u_{it}$ does not rise with $r_{s}$, we also set
$\gamma_{\ell}=\sqrt{b_{\gamma\ell}}$  and
$\Gamma_{j\ell}=\sqrt{\ell b_{\Gamma \ell}}$ with
$b_{\gamma\ell}=1/r - \sigma^{2}_{\gamma\ell}$ and
$b_{\Gamma\ell}=2/[r(r+1)]-2\sigma^{2}_{\Gamma  \ell}/(r+1)$. As for weak factors, the corresponding loadings are given by
\begin{align*}
\vartheta_{i\ell}=\frac{\eta_{\vartheta i\ell}}{2\sum_{i=1}^N\eta_{\vartheta i\ell}},\quad \eta_{\vartheta i\ell}\sim\mathrm{IIDU}(0,1), \\
\Theta_{ij\ell}=\frac{\eta_{\Theta i\ell}}{2\sum_{i=1}^N\eta_{\Theta i\ell}},\quad \eta_{\Theta i\ell}\sim\mathrm{IIDU}(0,1),
\end{align*}
for $\ell =1,...,r_{w}$ and $i=1,...,N$. We can observe that for each $\ell$, $\sum_{i=1}^N\vartheta_{i\ell}$ and $\sum_{i=1}^N\Theta_{i\ell}$ are $O(1)$. Therefore, as $N\rightarrow\infty$, the $R^2$~(R-squared) for each $i$ is affected by the strong factors even though $r_{w}\rightarrow\infty$.

We generate the dependent variable and regressors based on the following VAR(1) representation:
\[
\MBz_{it} =\MBA_{0i}^{-1}\MBA_{1i}\MBz_{it-1} +  \MBA_{0i}^{-1}\MBC_{i}^{s}\MBf_{t}+ \MBA_{0i}^{-1}\MBC_{i}^{w}\MBn_{t}+ \MBA_{0i}^{-1}\MBe_{it},
\]
where $\MBe_{it}=(\veps_{it}, \MBv_{it}^{\tp})^{\tp}$,
\begin{align*}
\MBA_{0i}= \left(\begin{array}{cc}
         1 & -\Bbeta_{0i}^{\tp} \\
                     0 & \MBI_{1\times p}  \\
                   \end{array}
                 \right),\quad \MBA_{1i}=\left(
        \begin{array}{cc}
          \rho_{i} & \Bbeta_{1i}^{\tp} \\
          \Balpha & \MBzero_{p\times p} \\
        \end{array}
      \right),
\end{align*}
$\MBC_{i}^{s}=(\Bgamma_{i},\BGamma_{i})^{\tp}$, $\MBC_{i}^{w}=(\Bvartheta_{i},\BTheta_{i})^{\tp}$, $\MBf_t=(f_{1 t},...,f_{r_s t})^{\tp}$  and $\MBn_t=(n_{1 t},...,n_{r_w t})^{\tp}$. Particularly, $\Bgamma_{i}=(\gamma_{i1},...,\gamma_{ir_s})^{\tp}$, $\BGamma_{i}=(\BGamma_{i1},...,\BGamma_{ip})$, $\Bvartheta_{i}=(\vartheta_{i1},...,\vartheta_{ir_w})^{\tp}$, $\BTheta_{i}=(\BTheta_{i1},...,\BTheta_{ip})$, $\BGamma_{ij}=(\Gamma_{ij1},...,\Gamma_{ijr_{s}})^{\tp}$ and $\BTheta_{ij}=(\Theta_{ij1},...,\Theta_{ijr_{w}})^{\tp}$.  For the errors in $x_{ijt}$, we assume that  $v_{ijt} =\rho_{xi}v_{ijt} +\nu_{ijt}$, $\Bnu_{it}=(\nu_{i1t},...,\nu_{ipt})^{\tp}$, and
\begin{align*}
\Bnu_{it}\sim \mathrm{IIDN} \left(\MBzero, \BSigma\right),
\end{align*}
where $\BSigma$ is a matrix that the diagonal elements are ones and off-diagonal elements are $\tau$'s. The errors $\veps_{it}$ in $y_{it}$ follows $\mathrm{IIDN} (0, 1)$.

As for the parameters in $\MBA_{0i}$ and $\MBA_{1i}$. To make $\{\MBz_{it}\}$ to be stationary, it is required that the eigenvalues of $\MBA_{0i}^{-1}\MBA_{1i}=\MBA_{i}$ lie inside the unit circle. More specifically, for each replication, we generate $\beta_{0i}$ as $\mathrm{IIDU}(0.3,1)$ and set
$\beta_{1i}=-0.3$. Furthermore, we generate $\phi_i$ from $\mathrm{IIDU}(0.7,0.9)$, and $\alpha_{ij}$ from $\mathrm{IIDU}(0,0.15)$.

\noindent
\textbf{Experiments:} We plan to investigate the performance of the proposed estimator based on the following cases with $r_s=5$ under different sample sizes of $T=(50,100,200)$ and $N=(50,100,200)$. 


\subsection{Simulation Results}

Tables \ref{Table_a} and \ref{Table_b} summarize the simulation results based on the data generating process introduced before. To understand the performance, we compare the bias and MSE~(mean squared error) of two estimators. $\hat{\beta}_i$ and $\hat{\rho}_i$ refer to as the usual OLS estimators of slope coefficient and autoregressive coefficient when the common correlated effects are under control while$ \hat{\beta}_{i,lasso}$ and $\hat{\rho}_{i,lasso}$ refer to the Lasso estimators of slope coefficient and autoregressive coefficient when the common correlated effects are under control. Since our asymptotic property is established based on $N,T\rightarrow\infty$, it is not straightforward to investigate only one estimated coefficent among all individuals. Therefore, we use the infinity norm to measure the the bias and the MSE among all estimated coefficients for all individuals. More specifically, $\mathrm{Bias}\norm{\Bbeta}_{\infty}=\max_i|\beta_i-\beta_0|$. This definition is used in the same mnner when we measure MSE and autoregressive coefficient.

In turns of different sample sizes~($T$ and $N$), we further consider different values of $p=(5,10,15)$ and the time dependency of the strong factors~($\rho_f=(0,0.5)$). The number of strong factors is $5$ which means that for all considerations of $p$, the rank condition is satisfied. This implies that the possible issues of the bias are from the case that the order of lags of the cross-sectional average of $y_{it}$ and $\MBx_{it}$ is not enough to control the unobserved factor structure. For each CCE estimator without using Lasso, we use $\kappa_{T}=\ceil{T^{1/3}}$ as the truncated lag order. For example, when $T=50$, $\kappa_{T}=4$. The total number of regressors including the cross-sectional average of $y_{it}$ and $\MBx_{it}$ is
$2p+1+(p+1)\times\kappa_{T}=35$ when $p=5$ and $2p+1+(p+1)\times\kappa_{T}=65$ when $p=10$. The former case implies that for each individual estimation, $T=50>35$ and noting is going wrong while when $p=10$, $T=50<65$ and it implies that the usual OLS does not work anymore. Therefore, when the total number of the variables on RHS is greater than $T$, we consider a smaller $\kappa_T$ until $2p+1+(p+1)\times\kappa_{T}
<T$. Under our designs, there three cases that $\kappa_T$ until $2p+1+(p+1)\times\kappa_{T}
>T$, and they are $(T=50,p=10)$, $(T=50,p=15)$ and $(T=100,p=15)$. 

We now focus on Table \ref{Table_a}. It shows the simulation results based on $\hat{\beta}_i$. When $p=5$, CCE estimator without Lasso penalty is still valid. We can observe that when $N$ is small with $\rho_f=0$, increasing $T$ does not improve the bias for both estimators and the difference between the CCE estimator and Lasso is very minor. However, when we consider the MSE, the MSE of Lasso est
imator outperforms the OLS type CCE estimator especially when $T=50$, for example, MSE of  $\norm{\hat\Bbeta_{lasso}}_{\infty}$ is 0.149 compared with 0.578 from the MSE of  $\norm{\hat\Bbeta_{\infty}}$. This provides a strong evidence that when $T$ is small, dramatically increased number of variables makes the variance of the OLS type estimator worse. The MSE of both estimators decreases in these two estimators when $T$ becomes larger. When $\rho_f=0.5$, we can observe a similar pattern between two estimators while the bias from Lasso estimator seems slightly larger than the OLS one. It is worth to mention that reason that the bias and the MSE become poor when $N$ becomes larger is because that the measure we consider is based on the infinity norm. It basically provides the worst result when we have more and more individuals.

When $p=10$, it is not surprising that the OLS type estimator delivers a very poor results for both bias and MSE when $T=50$. This is because under this case, the theoretical $\kappa_T$ should be $4$. However, to make the OLS type estimator valid, we implement the OLS with a smaller $\kappa_T=3$. Therefore the cross-sectional averages based on a smaller $\kappa_T=3$ cannot control the unobserved factor structure well. This situation leads to the biased estimates, and the MSE are ranged from $2.437$ to $2.798$ which are incredibly large. 
When we consider the Lasso type CCE estimator, we can observe that the bias and the MSE dramatically decrease. Especially, the MSE are ranged from $0.225$ to $0.272$ and the MSE does not change to much when $N$ increases. This implies that the estimator is very stable for all individuals compared to OLS CCE estimator~(it becomes worse when $N$ increases). When $T=100$, the OLS CCE is valid; however, the bias and MSE are still worse t
han the Lasso CCE estimator. When $T=200$, we can observe that the results from these two estimators are similar. 

As for the case that $p$ is lager~($p=15$). We can still observe that the bias and MSE are poor even $T=100$. The bias and the MSE are unstable when $N$ is getting larger. However the Lasso CCE, on the contrary, gives a stable results when $T=50$ and $T=100$. Even when $T=200$, the results are still comparable with OLS type estimator. This again shows that the Lasso CCE estimator is robust under different sample sizes
 and all considered specifications in our paper.
 
Now, we turn our focus onto the autoregressive coefficient. Overall, the bias of OLS type estimator are good when $T=50$ even when $p=15$. The only case that Lasso type CCE estimator outperforms the OLS is under the case that $T=100$ and $p=15$. However, the MSE of OLS type estimator are huge in those three cases we mentioned before and the Lasso type estimator are quite well. 

In sum, the simulation results suggest the theoretical result that while dynamic CCE approach ideally can control the unobserved factors but it has its limitation. By considering the Lasso type estimator, the CCE approach can inherit the advantage of CCE method but makes the estimation implementable when $T$ is small or $p$ is large.


\begin{landscape}
\begin{center}
\begin{longtable}{lllccccccccccccccc}
\caption{Simulation Results of $\hat\beta_i$} \label{Table_a} \\
\toprule
 & & & \multicolumn{3}{c}{Bias $\norm{\hat\beta_i}_{\infty}$} & \multicolumn{3}{c}{MSE $\norm{\hat\beta_i}_{\infty}$} & \multicolumn{3}{c}{Bias $\norm{\hat\beta_{i,lasso}}_{\infty}$} & \multicolumn{3}{c}{MSE $\norm{\hat\beta_{i,lasso}}_{\infty}$} \\
\midrule
        &               &         & $T=50$ &  $T=100$ & $T=200$& $T=50$ &  $T=100$ & $T=200$ & $T=50$ & $T=100$ & $T=200$ & $T=50$ & $T=100$ & $T=200$ \\
\cmidrule(lr){4-6}\cmidrule(lr){7-9}\cmidrule(lr){10-12}\cmidrule(lr){13-15}
 $p=5$ &    $\rho_f=0$ &   $N=50$ &  0.383 &  0.428 &  0.404 &    0.578 &    0.148 &  0.072 &  0.432 &  0.416 &  0.402 &  0.149 &  0.090 &  0.061 \\
        &               &  $N=100$ &  0.486 &  0.560 &  0.556 &    0.589 &    0.165 &  0.104 &  0.455 &  0.461 &  0.478 &  0.146 &  0.097 &  0.079 \\
        &               &  $N=200$ &  0.599 &  0.645 &  0.603 &    0.742 &    0.221 &  0.149 &  0.572 &  0.617 &  0.596 &  0.203 &  0.172 &  0.137 \\
\cmidrule(lr){4-6}\cmidrule(lr){7-9}\cmidrule(lr){10-12}\cmidrule(lr){13-15}
        &  $\rho_f=0.5$ &          &  0.344 &  0.388 &  0.398 &    0.595 &    0.137 &  0.072 &  0.411 &  0.417 &  0.412 &  0.139 &  0.095 &  0.064 \\
        &               &          &  0.516 &  0.490 &  0.496 &    0.549 &    0.153 &  0.090 &  0.464 &  0.436 &  0.449 &  0.146 &  0.097 &  0.074 \\
        &               &          &  0.615 &  0.584 &  0.573 &    0.697 &    0.208 &  0.139 &  0.555 &  0.610 &  0.592 &  0.221 &  0.169 &  0.132 \\
\midrule
$p=10$ &    $\rho_f=0$ &          &  0.755 &  0.619 &  0.564 &    2.437 &    0.674 &  0.153 &  0.568 &  0.549 &  0.549 &  0.225 &  0.132 &  0.092 \\
        &               &          &  0.809 &  0.773 &  0.740 &    2.523 &    0.708 &  0.205 &  0.672 &  0.710 &  0.738 &  0.254 &  0.199 &  0.154 \\
        &               &          &  0.867 &  0.800 &  0.758 &    2.782 &    0.740 &  0.201 &  0.693 &  0.694 &  0.783 &  0.259 &  0.174 &  0.134 \\
\cmidrule(lr){4-6}\cmidrule(lr){7-9}\cmidrule(lr){10-12}\cmidrule(lr){13-15}
        &  $\rho_f=0.5$ &          &  0.604 &  0.553 &  0.537 &    2.509 &    0.637 &  0.139 &  0.544 &  0.537 &  0.520 &  0.235 &  0.133 &  0.091 \\
        &               &          &  0.735 &  0.660 &  0.734 &    2.657 &    0.675 &  0.198 &  0.652 &  0.659 &  0.745 &  0.276 &  0.184 &  0.153 \\
        &               &          &  0.835 &  0.712 &  0.712 &    2.798 &    0.748 &  0.190 &  0.625 &  0.707 &  0.763 &  0.272 &  0.159 &  0.130 \\
\midrule
$p=15$ &    $\rho_f=0$ &          &  1.758 &  3.221 &  0.787 &   83.518 &  209.613 &  0.326 &  0.714 &  0.641 &  0.697 &  0.273 &  0.167 &  0.127 \\
        &               &          &  1.961 &  1.365 &  0.850 &   89.700 &   34.644 &  0.323 &  0.780 &  0.887 &  0.905 &  0.286 &  0.193 &  0.141 \\
        &               &          &  2.789 &  2.594 &  0.769 &   84.923 &  149.391 &  0.322 &  0.871 &  0.850 &  0.922 &  0.306 &  0.196 &  0.147 \\
\cmidrule(lr){4-6}\cmidrule(lr){7-9}\cmidrule(lr){10-12}\cmidrule(lr){13-15}
        &  $\rho_f=0.5$ &          &  1.712 &  1.514 &  0.710 &   25.596 &   22.713 &  0.315 &  0.706 &  0.644 &  0.687 &  0.277 &  0.167 &  0.121 \\
        &               &          &  1.548 &  1.692 &  0.774 &   36.897 &   53.966 &  0.313 &  0.802 &  0.854 &  0.861 &  0.319 &  0.189 &  0.135 \\
        &               &          &  3.452 &  1.635 &  0.750 &  280.188 &   47.908 &  0.321 &  0.833 &  0.846 &  0.893 &  0.330 &  0.200 &  0.142 \\
\bottomrule
\end{longtable}
\end{center}
\end{landscape}


\begin{landscape}
\begin{center}
\begin{longtable}{lllccccccccccccccc}
\caption{Simulation Results of $\hat\rho_i$} \label{Table_b} \\
\toprule
 & & & \multicolumn{3}{c}{Bias $\norm{\hat\rho_i}_{\infty}$} & \multicolumn{3}{c}{MSE $\norm{\hat\rho_i}_{\infty}$} & \multicolumn{3}{c}{Bias $\norm{\hat\rho_{i,lasso}}_{\infty}$} & \multicolumn{3}{c}{MSE $\norm{\hat\rho_{i,lasso}}_{\infty}$} \\
\midrule
        &               &         & $T=50$ &  $T=100$ & $T=200$& $T=50$ &  $T=100$ & $T=200$ & $T=50$ & $T=100$ & $T=200$ & $T=50$ & $T=100$ & $T=200$ \\
\cmidrule(lr){4-6}\cmidrule(lr){7-9}\cmidrule(lr){10-12}\cmidrule(lr){13-15}
  $p=5$ &    $\rho_f=0$ &   $N=50$ &  0.095 &  0.050 &  0.031 &   2.646 &    0.526 &  0.177 &  0.175 &  0.118 &  0.076 &  1.829 &  0.742 &  0.312 \\
        &               &  $N=100$ &  0.115 &  0.052 &  0.027 &   5.355 &    1.016 &  0.342 &  0.202 &  0.121 &  0.080 &  3.603 &  1.410 &  0.575 \\
        &               &  $N=200$ &  0.102 &  0.047 &  0.035 &  10.451 &    2.009 &  0.705 &  0.196 &  0.118 &  0.081 &  7.137 &  2.805 &  1.175 \\
\cmidrule(lr){4-6}\cmidrule(lr){7-9}\cmidrule(lr){10-12}\cmidrule(lr){13-15}
        &  $\rho_f=0.5$ &          &  0.102 &  0.040 &  0.049 &   2.781 &    0.473 &  0.162 &  0.182 &  0.106 &  0.064 &  1.437 &  0.496 &  0.186 \\
        &               &          &  0.128 &  0.050 &  0.064 &   5.683 &    0.971 &  0.341 &  0.183 &  0.110 &  0.068 &  2.883 &  0.938 &  0.356 \\
        &               &          &  0.127 &  0.055 &  0.058 &  11.467 &    2.002 &  0.676 &  0.172 &  0.101 &  0.067 &  5.700 &  1.891 &  0.702 \\
\midrule
 $p=10$ &     $rho_f=0$ &          &  0.087 &  0.064 &  0.041 &   4.928 &    1.387 &  0.235 &  0.192 &  0.117 &  0.079 &  1.812 &  0.672 &  0.277 \\
        &               &          &  0.121 &  0.077 &  0.033 &  10.128 &    2.767 &  0.479 &  0.194 &  0.118 &  0.079 &  3.616 &  1.371 &  0.546 \\
        &               &          &  0.103 &  0.065 &  0.039 &  19.921 &    5.420 &  0.941 &  0.201 &  0.131 &  0.080 &  7.207 &  2.662 &  1.068 \\
\cmidrule(lr){4-6}\cmidrule(lr){7-9}\cmidrule(lr){10-12}\cmidrule(lr){13-15}
        &  $\rho_f=0.5$ &          &  0.105 &  0.071 &  0.041 &   5.076 &    1.424 &  0.231 &  0.176 &  0.106 &  0.062 &  1.519 &  0.496 &  0.175 \\
        &               &          &  0.130 &  0.087 &  0.030 &  10.003 &    2.865 &  0.457 &  0.218 &  0.105 &  0.067 &  3.038 &  0.954 &  0.343 \\
        &               &          &  0.135 &  0.086 &  0.036 &  20.721 &    5.819 &  0.916 &  0.188 &  0.112 &  0.076 &  5.996 &  1.930 &  0.672 \\
\midrule
 $p=15$ &     $rho_f=0$ &          &  0.116 &  0.351 &  0.048 &  17.130 &   33.742 &  0.360 &  0.212 &  0.118 &  0.071 &  1.819 &  0.647 &  0.247 \\
        &               &          &  0.149 &  0.248 &  0.041 &  35.415 &   44.831 &  0.716 &  0.190 &  0.121 &  0.074 &  3.630 &  1.269 &  0.473 \\
        &               &          &  0.204 &  0.259 &  0.050 &  74.159 &  103.998 &  1.447 &  0.204 &  0.139 &  0.092 &  7.202 &  2.522 &  0.961 \\
\cmidrule(lr){4-6}\cmidrule(lr){7-9}\cmidrule(lr){10-12}\cmidrule(lr){13-15}
        &  $\rho_f=0.5$ &          &  0.160 &  0.180 &  0.050 &  18.018 &   24.814 &  0.367 &  0.190 &  0.114 &  0.068 &  1.535 &  0.480 &  0.162 \\
        &               &          &  0.226 &  0.181 &  0.045 &  41.635 &   46.184 &  0.749 &  0.206 &  0.118 &  0.066 &  3.195 &  0.965 &  0.329 \\
        &               &          &  0.197 &  0.212 &  0.041 &  77.322 &   90.855 &  1.476 &  0.212 &  0.115 &  0.064 &  6.240 &  1.876 &  0.628 \\
\bottomrule
\end{longtable}
\end{center}
\end{landscape}

\section{Concluding Remarks}\label{Sec:conclude}
In this paper, we use the simulation to investigate the finite sample properties of the proposed Lasso type CCE estimator. The simulation results suggest the theoretical result that while dynamic CCE approach ideally can control the unobserved factors but it has its limitation. By considering the Lasso type estimator, the CCE approach can inherit the advantage of CCE method but makes the estimation implementable when $T$ is small or $p$ is large.


\bigskip


\bibliographystyle{ecta}
\bibliography{lasso}

\end{document}












