@article{Wasserman2009,
abstract = {This paper explores the following question: what kind of statistical guarantees can be given when doing variable selection in high-dimensional models? In particular, we look at the error rates and power of some multi-stage regression methods. In the first stage we fit a set of candidate models. In the second stage we select one model by cross-validation. In the third stage we use hypothesis testing to eliminate some variables. We refer to the first two stages as "screening" and the last stage as "cleaning." We consider three screening methods: the lasso, marginal regression, and forward stepwise regression. Our method gives consistent variable selection under certain conditions.},
author = {Wasserman, Larry and Roeder, Kathryn},
doi = {10.1214/08-AOS646},
file = {:home/kian/文件/Mendeley Desktop/Wasserman, Roeder - 2009 - High-dimensional variable selection.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Lasso,Sparsity,Stepwise regression},
number = {5 A},
pages = {2178--2201},
title = {{High-dimensional variable selection}},
volume = {37},
year = {2009}
}


@article{Zhang2014,
abstract = {The purpose of this paper is to propose methodologies for statistical inference of low-dimensional parameters with high-dimensional data. We focus on constructing confidence intervals for individual coefficients and linear combinations of several of them in a linear regression model, although our ideas are applicable in a much broad context. The theoretical results presented here provide sufficient conditions for the asymptotic normality of the proposed estimators along with a consistent estimator for their finite-dimensional covariance matrices. These sufficient conditions allow the number of variables to far exceed the sample size. The simulation results presented here demonstrate the accuracy of the coverage probability of the proposed confidence intervals, strongly supporting the theoretical results.},
author = {Zhang, Cun Hui and Zhang, Stephanie S.},
doi = {10.1111/rssb.12026},
file = {:home/kian/文件/Mendeley Desktop/Zhang, Zhang - 2014 - Confidence intervals for low dimensional parameters in high dimensional linear models.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Confidence interval,High dimension,Linear regression model,Statistical inference,p-value},
number = {1},
pages = {217--242},
title = {{Confidence intervals for low dimensional parameters in high dimensional linear models}},
volume = {76},
year = {2014}
}


@article{VandeGeer2007,
abstract = {We study high-dimensional generalized linear models and empirical risk minimization using the Lasso. An oracle in-equality is presented, under a so called compatibility con-dition. Our aim is three fold: to proof a result announced in van de Geer (2007), to provide a simple proof with simple constants, and to separate the stochastic problem from the deterministic one.},
author = {van de Geer, S},
file = {:home/kian/文件/Mendeley Desktop/van de Geer - 2007 - The Deterministic Lasso.pdf:pdf},
journal = {Seminar f{\"{u}}r Statistik, Eidgen{\"{o}}ssische Technische Hochschule (ETH) Z{\"{u}}rich.},
keywords = {coherence,lasso,oracle,sparsity},
number = {140},
title = {{The Deterministic Lasso}},
year = {2007}
}
@book{buhlmann2011statistics,
  title={Statistics for high-dimensional data: methods, theory and applications},
  author={B{\"u}hlmann, Peter and Van De Geer, Sara},
  year={2011},
  publisher={Springer Science \& Business Media}
}

@article{Buhlmann2014,
abstract = {We review variable selection and variable screening in high-dimensional linear models. Thereby, a major focus is an empirical comparison of various estimation methods with respect to true and false positive selection rates based on 128 different sparse scenarios from semi-real data (real data covariables but synthetic regression coefficients and noise). Furthermore, we present some theoretical bounds for the bias in subsequent least squares estimation, using the selected variables from the first stage, which have direct implications for construction of p-values for regression coefficients.},
author = {B{\"{u}}hlmann, Peter and Mandozzi, Jacopo},
doi = {10.1007/s00180-013-0436-3},
file = {:home/kian/文件/Mendeley Desktop/B{\"{u}}hlmann, Mandozzi - 2014 - High-dimensional variable screening and bias in subsequent inference, with an empirical comparison.pdf:pdf},
issn = {16139658},
journal = {Computational Statistics},
keywords = {Elastic net,Lasso,Linear model,Ridge,Sparsity,Sure independence screening,Variable selection},
number = {3-4},
pages = {407--430},
title = {{High-dimensional variable screening and bias in subsequent inference, with an empirical comparison}},
volume = {29},
year = {2014}
}
@article{Tibshirani2011,
abstract = {We present a path algorithm for the generalized lasso problem. This problem penalizes the {\$}\backslashell{\_}1{\$} norm of a matrix {\$}D{\$} times the coefficient vector, and has a wide range of applications, dictated by the choice of {\$}D{\$}. Our algorithm is based on solving the dual of the generalized lasso, which facilitates computation and conceptual understanding of the path. For {\$}D=I{\$} (the usual lasso), we draw a connection between our approach and the well-known LARS algorithm. For an arbitrary {\$}D{\$}, we derive an unbiased estimate of the degrees of freedom of the generalized lasso fit. This estimate turns out to be quite intuitive in many applications.},
author = {Tibshirani, R. J. and Taylor, J.},
doi = {10.1214/11-AOS878},
file = {:home/kian/文件/Mendeley Desktop/Tibshirani, Taylor - 2011 - The solution path of the generalized lasso.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Degrees of freedom,Lagrange dual,Lars,Lasso,Path algorithm},
number = {3},
pages = {1335--1371},
title = {{The solution path of the generalized lasso}},
volume = {39},
year = {2011}
}
@article{She2010,
abstract = {This paper studies a generic sparse regression problem with a customizable sparsity pattern matrix, motivated by, but not limited to, a supervised gene clustering problem in microarray data analysis. The clustered lasso method is proposed with the l 1-type penalties imposed on both the coefficients and their pairwise differences. Somewhat surprisingly, it behaves differently than the lasso or the fused lasso-the exact clustering effect expected from the l 1 penalization is rarely seen in applications. An asymp-totic study is performed to investigate the power and limitations of the l 1-penalty in sparse regression. We propose to combine data-augmentation and weights to improve the l 1 technique. To address the computational issues in high dimensions, we successfully generalize a popular iterative algorithm both in practice and in theory and propose an 'annealing' algorithm applicable to generic sparse regressions (including the fused/clustered lasso). Some effective accelerating techniques are further investigated to boost the convergence. The accelerated annealing (AA) algorithm, involving only matrix multiplications and thresholdings, can handle a large design matrix as well as a large sparsity pattern matrix.},
author = {She, Yiyuan},
doi = {10.1214/10-EJS578},
file = {:home/kian/文件/Mendeley Desktop/She - 2010 - Sparse regression with exact clustering.pdf:pdf},
issn = {19357524},
journal = {Electronic Journal of Statistics},
keywords = {Clustering,Lasso,Sparsity,Thresholding},
pages = {1055--1096},
title = {{Sparse regression with exact clustering}},
volume = {4},
year = {2010}
}
@article{Lam2015,
abstract = {1 Distribution data were assembled for non-volant small mammals along$\backslash$nelevational gradients on mountain ranges in the western U.S.A.$\backslash$nElevational distributions in the species-rich Uinta Mountains were$\backslash$ncompared to those on smaller mountain ranges with varying degrees of$\backslash$nhistorical isolation from the Uintas.$\backslash$n2 For mountain ranges supporting the richest faunas, species richness is$\backslash$nhighest over a broad low- to mid-elevation zone and declines at both$\backslash$nlower and higher elevations. Patterns on other mountain ranges are$\backslash$nsimilar but reflect lower overall species richness.$\backslash$n3 A basic relationship between elevational and geographical distribution$\backslash$nis apparent in the occurrence patterns of mammals on regional mountains.$\backslash$nFaunas on mountains that have had low levels of historical isolation$\backslash$nappear to be influenced by immigration rather than extinction. Species$\backslash$nrestricted to high elevations in the Uintas are poorly represented on$\backslash$nhistorically isolated mountains and form a portion of local faunas$\backslash$nshaped by extinction. Species occurring at lower elevations in the$\backslash$nUintas have better representation on isolated mountains and apparently$\backslash$nmaintain populations through immigration.$\backslash$n4 Several widespread species show substantial variation in maximum$\backslash$nelevation records on different mountain ranges. This involves (1) an$\backslash$nupward shift in habitat zones on small, isolated mountain ranges,$\backslash$nallowing greater access by low-elevation species, and (2) expansion of$\backslash$ncertain low- and mid-elevation species into habitats normally occupied$\backslash$nby absent high-elevation taxa.$\backslash$n5 Results indicate that montane mammal faunas of the intermountain$\backslash$nregion have been shaped by broad-scale historical processes, unique$\backslash$nregional geography and local ecological dynamics. Parallel examples$\backslash$namong mammals of the Philippine Islands suggest that such patterns may$\backslash$ncharacterize many insular faunas.},
author = {Lam, Clifford and Souza, Pedro Cl},
doi = {10.1046/j.1466-822x.2001.00223.x},
file = {:home/kian/文件/Mendeley Desktop/Lam, Souza - 2015 - One-Step Regularized Spatial Weight Matrix and Fixed Effects Estimation with Instrumental Variables.pdf:pdf},
issn = {0960-7447},
keywords = {adaptive lasso,and phrases,fixed effects,instrumental variables,partial sign consistency,spatial,spatial econometrics,spatial weight matrices},
number = {August},
title = {{One-Step Regularized Spatial Weight Matrix and Fixed Effects Estimation with Instrumental Variables}},
year = {2015}
}
@article{VandeGeer2011,
abstract = {We revisit the adaptive Lasso as well as the thresholded Lasso with refitting, in a high-dimensional linear model, and study prediction error, ℓ q -error (q ∈ {\{}1, 2{\}}), and number of false positive selections. Our theoretical results for the two methods are, at a rather fine scale, comparable. The differences only show up in terms of the (minimal) restricted and sparse eigenvalues, favoring thresholding over the adaptive Lasso. As regards prediction and estimation, the difference is virtually negligible, but our bound for the number of false positives is larger for the adaptive Lasso than for thresholding. We also study the adaptive Lasso under beta-min conditions, which are conditions on the size of the coefficients. We show that for exact variable selection, the adaptive Lasso generally needs more severe beta-min conditions than thresholding. Both the two-stage methods add value to the one-stage Lasso in the sense that, under appropriate restricted and sparse eigenvalue conditions, they have similar prediction and estimation error as the one-stage Lasso but substantially less false positives. Regarding the latter, we provide a lower bound for the Lasso with respect to false positive selections.},
author = {van de Geer, Sara and B{\"{u}}hlmann, Peter and Zhou, Shuheng},
doi = {10.1214/11-EJS624},
file = {:home/kian/文件/Mendeley Desktop/van de Geer, B{\"{u}}hlmann, Zhou - 2011 - The adaptive and the thresholded Lasso for potentially misspecified models (and a lower bound for.pdf:pdf},
issn = {19357524},
journal = {Electronic Journal of Statistics},
keywords = {Adaptive Lasso,Estimation,Prediction,Restricted eigenvalue,Thresholding,Variable selection},
pages = {688--749},
title = {{The adaptive and the thresholded Lasso for potentially misspecified models (and a lower bound for the Lasso)}},
volume = {5},
year = {2011}
}
@article{Belloni2016,
abstract = {This paper considers generalized linear models in the presence of many controls. We lay out a general methodology to estimate an effect of interest based on the construction of an instrument that immunize against model selection mistakes and apply it to the case of logistic binary choice model. More specifically we propose new methods for estimating and constructing confidence regions for a regression parameter of primary interest {\$}\backslashalpha{\_}0{\$}, a parameter in front of the regressor of interest, such as the treatment variable or a policy variable. These methods allow to estimate {\$}\backslashalpha{\_}0{\$} at the root-{\$}n{\$} rate when the total number {\$}p{\$} of other regressors, called controls, potentially exceed the sample size {\$}n{\$} using sparsity assumptions. The sparsity assumption means that there is a subset of {\$}s{\textless}n{\$} controls which suffices to accurately approximate the nuisance part of the regression function. Importantly, the estimators and these resulting confidence regions are valid uniformly over {\$}s{\$}-sparse models satisfying {\$}s{\^{}}2\backslashlog{\^{}}2 p = o(n){\$} and other technical conditions. These procedures do not rely on traditional consistent model selection arguments for their validity. In fact, they are robust with respect to moderate model selection mistakes in variable selection. Under suitable conditions, the estimators are semi-parametrically efficient in the sense of attaining the semi-parametric efficiency bounds for the class of models in this paper.},
archivePrefix = {arXiv},
arxivId = {arXiv:1304.3969v3},
author = {Belloni, Alexandre and Chernozhukov, Victor and Wei, Ying},
doi = {10.1080/07350015.2016.1166116},
eprint = {arXiv:1304.3969v3},
file = {:home/kian/文件/Mendeley Desktop/Belloni, Chernozhukov, Wei - 2016 - Post-Selection Inference for Generalized Linear Models With Many Controls.pdf:pdf},
issn = {15372707},
journal = {Journal of Business and Economic Statistics},
keywords = {Double selection,Instruments,Model selection,Neymanization,Optimality,Sparsity,Uniformly valid inference},
number = {4},
pages = {606--619},
title = {{Post-Selection Inference for Generalized Linear Models With Many Controls}},
volume = {34},
year = {2016}
}
@article{Belloni2016a,
abstract = {We consider estimation and inference in panel data models with additive unobserved individual specific heterogeneity in a high dimensional setting. The setting allows the number of time varying regressors to be larger than the sample size. To make informative estimation and inference feasible, we require that the overall contribution of the time varying variables after eliminating the individual specific heterogeneity can be captured by a relatively small number of the available variables whose identities are unknown. This restriction allows the problem of estimation to proceed as a variable selection problem. Importantly, we treat the individual specific heterogeneity as fixed effects which allows this heterogeneity to be related to the observed time varying variables in an unspecified way and allows that this heterogeneity may be non-zero for all individuals. Within this framework, we provide procedures that give uniformly valid inference over a fixed subset of parameters in the canonical linear fixed effects model and over coefficients on a fixed vector of endogenous variables in panel data instrumental variables models with fixed effects and many instruments. An input to developing the properties of our proposed procedures is the use of a variant of the Lasso estimator that allows for a grouped data structure where data across groups are independent and dependence within groups is unrestricted. We provide formal conditions within this structure under which the proposed Lasso variant selects a sparse model with good approximation properties. We present simulation results in support of the theoretical developments and illustrate the use of the methods in an application aimed at estimating the effect of gun prevalence on crime rates.},
author = {Belloni, Alexandre and Chernozhukov, Victor and Hansen, Christian and Kozbur, Damian},
doi = {10.1080/07350015.2015.1102733},
file = {:home/kian/文件/Mendeley Desktop/Belloni et al. - 2016 - Inference in High-Dimensional Panel Models With an Application to Gun Control.pdf:pdf},
issn = {15372707},
journal = {Journal of Business and Economic Statistics},
keywords = {Clustered standard errors,Fixed effects,High-dimensional-sparse regression,Inference under imperfect model selection,Instrumental variables,Panel data,Partially linear model,Uniformly valid inference after model selection},
number = {4},
pages = {590--605},
title = {{Inference in High-Dimensional Panel Models With an Application to Gun Control}},
volume = {34},
year = {2016}
}
@article{Zhu2015,
abstract = {We consider high-dimensional panel data models (large cross sections and long time horizons) with interactive fixed effects and allow the covariate/slope coefficients to vary over time without any restrictions. The parameter of interest is the vector that contains all the covariate effects across time. This vector has dimensionality tending to infinity, potentially much faster than the cross-sectional sample size. We develop methods for the estimation and inference of this high-dimensional vector, i.e., the entire trajectory of time variation in covariate effects. We show that both the consistency of our estimator and the asymptotic accuracy of the proposed inference procedure hold uniformly in time. Our methodology can be applied to several important issues in econometrics, such as constructing confidence bands for the entire path of covariate coefficients across time, testing the time-invariance of slope coefficients and estimation and inference of patterns of time variations, including structural breaks and regime switching. An important feature of our method is that it provides inference procedures for the time variation in prespecified components of slope coefficients while allowing for arbitrary time variation in other components. Computationally, our procedures do not require any numerical optimization and are very simple to implement. Monte Carlo simulations demonstrate favorable properties of our methods in finite samples. We illustrate our methods through empirical applications in finance and economics.},
author = {Zhu, Yinchu},
doi = {10.2139/ssrn.2665374},
file = {:home/kian/文件/Mendeley Desktop/Zhu - 2015 - High-Dimensional Panel Data with Time Heterogeneity Estimation and Inference.pdf:pdf},
journal = {Ssrn},
keywords = {C14,C23,Inference in high dimensions,interactive fixed effects,time-heterogeneity},
number = {2015},
title = {{High-Dimensional Panel Data with Time Heterogeneity: Estimation and Inference}},
year = {2015}
}
@article{Bonhomme2015,
abstract = {This paper introduces time-varying grouped patterns of heterogeneity in linear panel data models. A distinctive feature of our approach is that group membership is left unrestricted. We estimate the parameters of the model using a “grouped fixed-effects” estimator that minimizes a least squares criterion with respect to all possible groupings of the cross-sectional units. Recent advances in the clustering literature allow for fast and efficient computation. We provide conditions under which our estimator is consistent as both dimensions of the panel tend to infinity, and we develop inference methods. Finally, we allow for grouped patterns of unobserved heterogeneity in the study of the link between income and democracy across countries.},
author = {Bonhomme, St{\'{e}}phane and Manresa, Elena},
doi = {10.3982/ecta11319},
file = {:home/kian/文件/Mendeley Desktop/Bonhomme, Manresa - 2015 - Grouped Patterns of Heterogeneity in Panel Data.pdf:pdf},
journal = {Econometrica},
number = {3},
pages = {1147--1184},
title = {{Grouped Patterns of Heterogeneity in Panel Data}},
volume = {83},
year = {2015}
}
@article{M.Arelano1987,
author = {M.Arelano},
file = {:home/kian/文件/Mendeley Desktop/M.Arelano - 1987 - Computing Robust Standard Errors for Within-groups.pdf:pdf},
journal = {Oxford Bulletin of Economics and Statistics},
number = {49},
pages = {431--434},
title = {{Computing Robust Standard Errors for Within-groups}},
volume = {4},
year = {1987}
}
@article{Fan2014,
abstract = {Most papers on high-dimensional statistics are based on the assumption that none of the regressors are correlated with the regression error, namely, they are exogenous. Yet, endogeneity can arise incidentally from a large pool of regressors in a high-dimensional regression. This causes the inconsistency of the penalized least-squares method and possible false scientific discoveries. A necessary condition for model selection consistency of a general class of penalized regression methods is given, which allows us to prove formally the inconsistency claim. To cope with the incidental endogeneity, we construct a novel penalized focused generalized method of moments (FGMM) criterion function. The FGMM effectively achieves the dimension reduction and applies the instrumental variable methods. We show that it possesses the oracle property even in the presence of endogenous predictors, and that the solution is also near global minimum under the over-identification assumption. Finally, we also show how the semi-parametric efficiency of estimation can be achieved via a two-step approach.},
author = {Fan, Jianqing and Liao, Yuan},
doi = {10.1214/13-AOS1202},
file = {:home/kian/文件/Mendeley Desktop/Fan, Liao - 2014 - Endogeneity in high dimensions.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Conditional moment restriction,Endogenous variables,Estimating equation,Focused GMM,Global minimization,Oracle property,Over identification,Semiparametric efficiency,Sparsity recovery},
number = {3},
pages = {872--917},
title = {{Endogeneity in high dimensions}},
volume = {42},
year = {2014}
}
@article{VandeGeer2009,
abstract = {Oracle inequalities and variable selection properties for the Lasso in linear models have been established under a variety of different assumptions on the design matrix. We show in this paper how the different conditions and concepts relate to each other. The restricted eigenvalue condition (Bickel et al., 2009) or the slightly weaker compatibility condition (van de Geer, 2007) are sufficient for oracle results. We argue that both these conditions allow for a fairly general class of design matrices. Hence, optimality of the Lasso for prediction and estimation holds for more general situations than what it appears from coherence (Bunea et al, 2007b,c) or restricted isometry (Candes and Tao, 2005) assumptions.},
author = {{Van de Geer}, Sara A. and B{\"{u}}hlmann, Peter},
doi = {10.1214/09-EJS506},
file = {:home/kian/文件/Mendeley Desktop/Van de Geer, B{\"{u}}hlmann - 2009 - On the conditions used to prove oracle results for the lasso.pdf:pdf},
issn = {19357524},
journal = {Electronic Journal of Statistics},
keywords = {Coherence,Compatibility,Irrepresentable condition,Lasso,Restricted eigenvalue,Restricted isometry,Sparsity},
pages = {1360--1392},
title = {{On the conditions used to prove oracle results for the lasso}},
volume = {3},
year = {2009}
}
@article{Phillips1998,
abstract = {Estimated impulse responses and forecast error decompositions are shown to be inconsistent at long horizons in unrestricted VARs with some unit roots. Predictions from unrestricted VARs also do not converge to the optimal predictors over long forecast horizons. In contrast, reduced rank regressions produce impulse responses and forecast error variance estimates that are consistent and predictions that are asymptotically optimal, provided the cointegrating rank is correctly specified or consistently estimated by an order selector such as PIC. Some simulations show these findings to be relevant in finite samples in VARs with some unit roots and cointegration. {\textcopyright} 1998 Elsevier Science S.A.},
author = {Phillips, Peter C.B.},
doi = {10.1016/S0304-4076(97)00064-X},
file = {:home/kian/文件/Mendeley Desktop/Phillips - 1998 - Impulse response and forecast error variance asymptotics in nonstationary VARs.pdf:pdf},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {Error correction model,Forecast error variance decomposition asymptotics,Impulse response asymptotics,Reduced rank regression,Unit-root asymptotics,Vector autoregression},
number = {1-2},
pages = {21--56},
title = {{Impulse response and forecast error variance asymptotics in nonstationary VARs}},
volume = {83},
year = {1998}
}
@article{Hsu2008,
abstract = {A subset selection method is proposed for vector autoregressive (VAR) processes using the Lasso [Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B 58, 267-288] technique. Simply speaking, Lasso is a shrinkage method in a regression setup which selects the model and estimates the parameters simultaneously. Compared to the conventional information-based methods such as AIC and BIC, the Lasso approach avoids computationally intensive and exhaustive search. On the other hand, compared to the existing subset selection methods with parameter constraints such as the top-down and bottom-up strategies, the Lasso method is computationally efficient and its result is robust to the order of series included in the autoregressive model. We derive the asymptotic theorem for the Lasso estimator under VAR processes. Simulation results demonstrate that the Lasso method performs better than several conventional subset selection methods for small samples in terms of prediction mean squared errors and estimation errors under various settings. The methodology is applied to modeling U.S. macroeconomic data for illustration. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
author = {Hsu, Nan Jung and Hung, Hung Lin and Chang, Ya Mei},
doi = {10.1016/j.csda.2007.12.004},
file = {:home/kian/文件/Mendeley Desktop/Hsu, Hung, Chang - 2008 - Subset selection for vector autoregressive processes using Lasso.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
number = {7},
pages = {3645--3657},
title = {{Subset selection for vector autoregressive processes using Lasso}},
volume = {52},
year = {2008}
}
@article{Quaedvlieg2019,
abstract = {We introduce tests for multi-horizon superior predictive ability. Rather than comparing forecasts of different models at multiple horizons individually, we propose to jointly consider all horizons within a forecast path. We define the concepts of uniform and average superior predictive ability. The former entails superior performance at each individual horizon, while the latter allows inferior performance at some horizons to be compensated by others. We show that the tests lead to more coherent conclusions, and are better able to differentiate models than the single-horizon tests. We provide an extension of the Model Confidence Set to allow for multi-horizon comparison of more than two models. Simulations demonstrate appropriate size and high power. An illustration of the tests on a large set of macroeconomic variables demonstrates the empirical benefits of multi-horizon comparison. },
author = {Quaedvlieg, Rogier},
doi = {10.1080/07350015.2019.1620074},
file = {:home/kian/文件/Mendeley Desktop/Quaedvlieg - 2019 - Multi-Horizon Forecast Comparison.pdf:pdf},
issn = {0735-0015},
journal = {Journal of Business {\&} Economic Statistics},
keywords = {forecasting,long-horizon,multiple testing,path forecasts,superior predictive ability},
number = {0},
pages = {1--34},
publisher = {Taylor {\&} Francis},
title = {{Multi-Horizon Forecast Comparison}},
url = {https://doi.org/10.1080/07350015.2019.1620074},
volume = {0},
year = {2019}
}
@article{Lin2017,
abstract = {Dynamical systems comprising of multiple components that can be partitioned into distinct blocks originate in many scientific areas. A pertinent example is the interactions between financial assets and selected macroeconomic indicators, which has been studied at aggregate level---e.g. a stock index and an employment index---extensively in the macroeconomics literature. A key shortcoming of this approach is that it ignores potential influences from other related components (e.g. Gross Domestic Product) that may exert influence on the system's dynamics and structure and thus produces incorrect results. To mitigate this issue, we consider a multi-block linear dynamical system with Granger-causal ordering between blocks, wherein the blocks' temporal dynamics are described by vector autoregressive processes and are influenced by blocks higher in the system hierarchy. We derive the maximum likelihood estimator for the posited model for Gaussian data in the high-dimensional setting based on appropriate regularization schemes for the parameters of the block components. To optimize the underlying non-convex likelihood function, we develop an iterative algorithm with convergence guarantees. We establish theoretical properties of the maximum likelihood estimates, leveraging the decomposability of the regularizers and a careful analysis of the iterates. Finally, we develop testing procedures for the null hypothesis of whether a block "Granger-causes" another block of variables. The performance of the model and the testing procedures are evaluated on synthetic data, and illustrated on a data set involving log-returns of the US S{\&}P100 component stocks and key macroeconomic variables for the 2001--16 period.},
archivePrefix = {arXiv},
arxivId = {1708.05879},
author = {Lin, Jiahe and Michailidis, George},
eprint = {1708.05879},
file = {:home/kian/文件/Mendeley Desktop/Lin, Michailidis - 2017 - Regularized Estimation and Testing for High-Dimensional Multi-Block Vector-Autoregressive Models.pdf:pdf},
keywords = {block-coordinate descent,consistency,stability,vector-autoregression},
pages = {1--49},
title = {{Regularized Estimation and Testing for High-Dimensional Multi-Block Vector-Autoregressive Models}},
url = {http://arxiv.org/abs/1708.05879},
volume = {18},
year = {2017}
}
@article{Hall2016,
abstract = {Vector autoregressive models characterize a variety of time series in which linear combinations of current and past observations can be used to accurately predict future observations. For instance, each element of an observation vector could correspond to a different node in a network, and the parameters of an autoregressive model would correspond to the impact of the network structure on the time series evolution. Often these models are used successfully in practice to learn the structure of social, epidemiological, financial, or biological neural networks. However, little is known about statistical guarantees on estimates of such models in non-Gaussian settings. This paper addresses the inference of the autoregressive parameters and associated network structure within a generalized linear model framework that includes Poisson and Bernoulli autoregressive processes. At the heart of this analysis is a sparsity-regularized maximum likelihood estimator. While sparsity-regularization is well-studied in the statistics and machine learning communities, those analysis methods cannot be applied to autoregressive generalized linear models because of the correlations and potential heteroscedasticity inherent in the observations. Sample complexity bounds are derived using a combination of martingale concentration inequalities and modern empirical process techniques for dependent random variables. These bounds, which are supported by several simulation studies, characterize the impact of various network parameters on estimator performance.},
archivePrefix = {arXiv},
arxivId = {1605.02693},
author = {Hall, Eric C. and Raskutti, Garvesh and Willett, Rebecca},
eprint = {1605.02693},
file = {:home/kian/文件/Mendeley Desktop/Hall, Raskutti, Willett - 2016 - Inference of High-dimensional Autoregressive Generalized Linear Models.pdf:pdf},
title = {{Inference of High-dimensional Autoregressive Generalized Linear Models}},
url = {http://arxiv.org/abs/1605.02693},
year = {2016}
}
@article{Shang2018,
abstract = {Model averaging combines forecasts obtained from a range of models, and it often produces more accurate forecasts than a forecast from a single model. The crucial part of forecast accuracy improvement in using the model averaging lies in the determination of optimal weights from a finite sample. If the weights are selected sub-optimally, this can affect the accuracy of the model-averaged forecasts. Instead of choosing the optimal weights, we consider trimming a set of models before equally averaging forecasts from the selected superior models. Motivated by Hansen, Lunde and Nason (2011), we apply and evaluate the model confidence set procedure when combining mortality forecasts. The proposed model averaging procedure is motivated by Samuels and Sekkel (2017) based on the concept of model confidence sets as proposed by Hansen et al. (2011) that incorporates the statistical significance of the forecasting performance. As the model confidence level increases, the set of superior models generally decreases. The proposed model averaging procedure is demonstrated via national and sub-national Japanese mortality for retirement ages between 60 and 100+. Illustrated by national and sub-national Japanese mortality for ages between 60 and 100+, the proposed model-average procedure gives the smallest interval forecast errors, especially for males. We find that robust out-of-sample point and interval forecasts may be obtained from the trimming method. By robust, we mean robustness against model misspecification.},
archivePrefix = {arXiv},
arxivId = {arXiv:1809.10838v1},
author = {Shang, Han Lin and Haberman, Steven},
doi = {10.1186/s41118-018-0043-9},
eprint = {arXiv:1809.10838v1},
file = {:home/kian/文件/Mendeley Desktop/Shang, Haberman - 2018 - Model confidence sets and forecast combination an application to age-specific mortality.pdf:pdf},
issn = {20355556},
journal = {Genus},
keywords = {Equal predictability test,Japanese human mortality database,Mean interval score,Model averaging,Root mean square forecast error},
number = {1},
pages = {1--32},
title = {{Model confidence sets and forecast combination: an application to age-specific mortality}},
volume = {74},
year = {2018}
}

@article{Dezeure2015,
abstract = {We present a (selective) review of recent frequentist high-dimensional inference methods for constructing {\$}p{\$}-values and confidence intervals in linear and generalized linear models. We include a broad, comparative empirical study which complements the viewpoint from statistical methodology and theory. Furthermore, we introduce and illustrate the R-package hdi which easily allows the use of different methods and supports reproducibility.},
author = {Dezeure, Ruben and B{\"{u}}hlmann, Peter and Meier, Lukas and Meinshausen, Nicolai},
doi = {10.1214/15-sts527},
file = {:home/kian/文件/Mendeley Desktop/Dezeure et al. - 2015 - High-Dimensional Inference Confidence Intervals, {\$}p{\$}-Values and R-Software hdi.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,clustering,confidence interval,generalized linear,high-dimensional statistical inference,linear model,model,multiple testing,p-value,r-software},
number = {4},
pages = {533--558},
title = {{High-Dimensional Inference: Confidence Intervals, p-Values and R-Software hdi}},
volume = {30},
year = {2015}
}
@article{Fildes2002,
abstract = {Title from title screen (JSTOR, viewed June 8, 2005).},
author = {Fildes, Robert},
doi = {10.1016/0169-2070(94)90039-6},
file = {:home/kian/文件/Mendeley Desktop/Fildes - 2002 - Journal of Business and Economic Statistics.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
number = {4},
pages = {649},
title = {{Journal of Business and Economic Statistics}},
volume = {10},
year = {2002}
}

@article{Melnyk2016,
abstract = {While considerable advances have been made in estimating high-dimensional structured models from independent data using Lasso-type models, limited progress has been made for settings when the samples are dependent. We consider estimating structured VAR (vector auto-regressive models), where the structure can be captured by any suitable norm, e.g., Lasso, group Lasso, order weighted Lasso, sparse group Lasso, etc. In VAR setting with correlated noise, although there is strong dependence over time and covariates, we establish bounds on the non-asymptotic estimation error of structured VAR parameters. Surprisingly, the estimation error is of the same order as that of the corresponding Lasso-type estimator with independent samples, and the analysis holds for any norm. Our analysis relies on results in generic chaining, sub-exponential martingales, and spectral representation of VAR models. Experimental results on synthetic data with a variety of structures as well as real aviation data are presented, validating theoretical results.},
archivePrefix = {arXiv},
arxivId = {1602.06606},
author = {Melnyk, Igor and Banerjee, Arindam},
eprint = {1602.06606},
file = {:home/kian/文件/Mendeley Desktop/Melnyk, Banerjee - 2016 - Estimating Structured Vector Autoregressive Model.pdf:pdf},
title = {{Estimating Structured Vector Autoregressive Model}},
url = {http://arxiv.org/abs/1602.06606},
year = {2016}
}
@article{Medeiros2017,
abstract = {In this paper we show the validity of the adaptive LASSO procedure in estimating stationary ARDL(p,q) models with GARCH innovations. We show that, given a set of initial weights, the adaptive Lasso selects the relevant variables with probability converging to one. Afterwards, we show that the estimator is oracle, meaning that its distribution converges to the same distribution of the oracle assisted least squares, i.e., the least squares estimator calculated as if we knew the set of relevant variables beforehand. Finally, we show that the LASSO estimator can be used to construct the initial weights. The performance of the method in finite samples is illustrated using Monte Carlo simulation.},
author = {Medeiros, Marcelo C. and Mendes, Eduardo F.},
doi = {10.1080/07474938.2017.1307319},
file = {:home/kian/文件/Mendeley Desktop/Medeiros, Mendes - 2017 - Adaptive LASSO estimation for ARDL models with GARCH innovations.pdf:pdf},
issn = {15324168},
journal = {Econometric Reviews},
keywords = {ARDL,GARCH,LASSO,adaLASSO,shrinkage,sparse models,time series},
number = {6-9},
pages = {622--637},
title = {{Adaptive LASSO estimation for ARDL models with GARCH innovations}},
volume = {36},
year = {2017}
}
@article{Lyziak2010,
abstract = {M Me ea as su ur ri in ng g c co on ns su um me er r i in nf fl la at ti io on n e ex xp pe ec ct ta at ti io on ns s i in n E Eu ur ro op pe e a an nd d e ex xa am mi in ni in ng g t th he ei ir r f fo or rw wa ar rd d--l lo oo ok ki in ng gn ne es ss s Abstract This paper presents numerical measures of European consumers' inflation expectations derived on the basis of European Commission qualitative survey data with different quantification methods, i.e. with the probability method, the regression method and the logistic (and linear) function method. The study aims at assessing differences between those measures and the result-ing uncertainty in measuring inflation expectations of this group of economic agents. Moreover, in the empirical part of the paper the formation of expectations by consumers in European economies is examined, with a particular focus on estimating the degree of forward-lookingness of expectations.},
author = {Lyziak, Tomasz and {\L}yziak, Tomasz},
file = {:home/kian/文件/Mendeley Desktop/Lyziak, {\L}yziak - 2010 - M P RA Measuring consumer inflation expectations in Europe and examining their forward-lookingness.pdf:pdf},
number = {18890},
title = {{M P RA Measuring consumer inflation expectations in Europe and examining their forward-lookingness}},
url = {http://mpra.ub.uni-muenchen.de/18890/},
year = {2010}
}
@article{Nicholson2017,
abstract = {The vector autoregression (VAR) has long proven to be an effective method for modeling the joint dynamics of macroeconomic time series, as well as for forecasting. One major shortcoming of the VAR that has limited its applicability is its heavy parameterization: the parameter space grows quadratically with the number of series included, quickly exhausting the available degrees of freedom. Consequently, using VARs for forecasting is intractable for low-frequency, high-dimensional macroeconomic data. However, empirical evidence suggests that VARs that incorporate more component series tend to result in more accurate forecasts. Most conventional methods that allow for the estimation of large VARs either require ad hoc subjective specifications or are computationally infeasible. Moreover, as global economies become more intricately intertwined, there has been a substantial interest in incorporating the impact of stochastic, unmodeled exogenous variables. Vector autoregression with exogenous variables (VARX) extends the VAR to allow for the inclusion of unmodeled variables, but faces similar dimensionality challenges. This paper introduces the VARX-L framework, a structured family of VARX models, and provides a methodology that allows for both efficient estimation and accurate forecasting in high-dimensional analysis. VARX-L adapts several prominent scalar regression regularization techniques to a vector time series context, which greatly reduces the parameter space of VAR and VARX models. We also highlight a compelling extension that allows for shrinking toward reference models, such as a vector random walk. We demonstrate the efficacy of VARX-L in both low- and high-dimensional macroeconomic forecasting applications and simulated data examples. Our methodology is easy to reproduce in a publicly available R package.},
author = {Nicholson, William B. and Matteson, David S. and Bien, Jacob},
doi = {10.1016/j.ijforecast.2017.01.003},
file = {:home/kian/文件/Mendeley Desktop/Nicholson, Matteson, Bien - 2017 - VARX-L Structured regularization for large vector autoregressions with exogenous variables.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Big data,Forecasting,Group lasso,Macroeconometrics,Time series},
number = {3},
pages = {627--651},
publisher = {Elsevier B.V.},
title = {{VARX-L: Structured regularization for large vector autoregressions with exogenous variables}},
url = {http://dx.doi.org/10.1016/j.ijforecast.2017.01.003},
volume = {33},
year = {2017}
}
@article{Murasawa2017,
abstract = {The Bank of England/GfK NOP Inflation Attitudes Survey asks individuals about their inflation perceptions and expectations in eight ordered categories with known boundaries except for an indifference limen. With enough categories for identification, one can fit a mixture distribution to such data, which can be multi-modal. Thus Bayesian analysis of a normal mixture model for interval data with an indifference limen is of interest. This paper applies the No-U-Turn Sampler (NUTS) for Bayesian computation, and estimates the distributions of public inflation perceptions and expectations in the UK during 2001Q1--2015Q4. The estimated means are useful for measuring information rigidity.},
author = {Murasawa, Yasutomo},
doi = {10.1007/s00181-019-01675-8},
file = {:home/kian/文件/Mendeley Desktop/Murasawa - 2017 - Measuring the Distributions of Public Inflation Perceptions and Expectations in the UK.pdf:pdf},
issn = {1435-8921},
journal = {Empirical Economics},
keywords = {Bayesian,Indifference limen,Information rigidity,I,bayesian,c25,c46,c82,e31,indifference limen,information rigidity,interval data,jel classification c11,no-u-turn sampler,normal mixture},
number = {74203},
pages = {8--9},
publisher = {Springer Berlin Heidelberg},
title = {{Measuring the Distributions of Public Inflation Perceptions and Expectations in the UK}},
url = {https://doi.org/10.1007/s00181-019-01675-8},
year = {2017}
}
@article{Zhang2018,
author = {Zhang, Xinyu and Chiou, Jeng Min and Ma, Yanyuan},
doi = {10.1093/biomet/asy041},
file = {:home/kian/文件/Mendeley Desktop/Zhang, Chiou, Ma - 2018 - Functional prediction through averaging estimated functional linear regression models.pdf:pdf},
issn = {14643510},
journal = {Biometrika},
keywords = {Asymptotic optimality,Crossvalidation,Functional data,Model averaging,Weighting.},
number = {4},
pages = {945--962},
title = {{Functional prediction through averaging estimated functional linear regression models}},
volume = {105},
year = {2018}
}
@article{Song2019,
author = {Song, Xiaojun},
file = {:home/kian/文件/Mendeley Desktop/Song - 2019 - Nonparametric Trending Regression Estimation and Testing More Efficient Estimation in Nonparametric Trending Regression.pdf:pdf},
pages = {1--48},
title = {{Nonparametric Trending Regression : Estimation and Testing More Efficient Estimation in Nonparametric Trending Regression}},
year = {2019}
}
@article{GouletCoulombe2019,
abstract = {We move beyond Is Machine Learning Useful for Macroeconomic Forecasting? by adding the how. The current forecasting literature has focused on matching specific variables and horizons with a particularly successful algorithm. To the contrary, we study a wide range of horizons and variables and learn about the usefulness of the underlying features driving ML gains over standard macroeconometric methods. We distinguish 4 so-called features (nonlinearities, regularization, cross-validation and alternative loss function) and study their behavior in both the data-rich and data-poor environments. To do so, we carefully design a series of experiments that easily allow to identify the treatment effects of interest. The simple evaluation framework is a fixed-effects regression that can be understood as an extension of the Diebold and Mariano (1995) test. The regression setup prompt us to use a novel visualization technique for forecasting results that conveys all the relevant information in a digestible format. We conclude that (i) more data and non-linearities are very useful for real variables at long horizons, (ii) the standard factor model remains the best regularization, (iii) cross-validations are not all made equal (but K-fold is as good as BIC) and (iv) one should stick with the standard L 2 loss.},
author = {{Goulet Coulombe}, Philippe and Leroux, Maxime and Stevanovic, Dalibor and Surprenant, St{\'{e}}phane},
file = {:home/kian/文件/Mendeley Desktop/Goulet Coulombe et al. - 2019 - How is Machine Learning Useful for Macroeconomic Forecasting.pdf:pdf},
keywords = {Big Data,Forecasting * ‡ Corresponding Author: dstevanovice,Machine Learning,UQAM},
pages = {1--56},
title = {{How is Machine Learning Useful for Macroeconomic Forecasting? *}},
url = {https://economics.sas.upenn.edu/system/files/2019-03/GCLSS{\_}MC{\_}MacroFcst.pdf},
year = {2019}
}
@article{Fiorentini2019,
author = {Fiorentini, Gabriele and Sentana, Enrique},
doi = {10.1002/jae.2678},
file = {:home/kian/文件/Mendeley Desktop/Fiorentini, Sentana - 2019 - Dynamic specification tests for dynamic factor models.pdf:pdf},
issn = {10991255},
journal = {Journal of Applied Econometrics},
number = {3},
pages = {325--346},
title = {{Dynamic specification tests for dynamic factor models}},
volume = {34},
year = {2019}
}
@article{Pesaran2018,
abstract = {This paper proposes a new double-question survey method that elicits information about how individuals' subjective belief valuations are compared and related to their price expectations.  An individual respondent is presented with two sets of questions, one that asks about his/her belief regarding the value of an asset (whether it is over- or under-valued), and another regarding his/her expectations of the future price of that asset.  Responses to these two questions are then used to measure the extent to which prices are likely to move towards or away from the subjectively perceived fundamental values.  Using a theoretical asset pricing model with heterogenous agents we show that there exists a negative relationship between the agents expectations of price changes and their asset valuation.  Double question surveys on equity, gold and house prices provide evidence in support of such relationships, particularly in the case of house price expectations.  The effects of demographic factors, such as sex, age, education, ethnicity, and income are also investigated.  It is shown that for house price expectations such demographic factors cease to be statistically significant once we condition on the respondents' location and their asset valuation indicator.  The results of the double-question surveys are then used to construct leading bubble and crash indicators, and their potential value is illustrated in the context of a dynamic panel regression of realized house price changes across a number of key Metropolitan Statistical Areas in the US.},
author = {Pesaran, M. Hashem and Johnsson, Ida},
doi = {10.1080/07350015.2018.1513845},
file = {:home/kian/文件/Mendeley Desktop/Pesaran, Johnsson - 2018 - Double-Question Survey Measures for the Analysis of Financial Bubbles and Crashes.pdf:pdf},
issn = {15372707},
journal = {Journal of Business and Economic Statistics},
keywords = {Belief valuations,Bubbles and crashes,House prices,Price expectations},
title = {{Double-Question Survey Measures for the Analysis of Financial Bubbles and Crashes}},
volume = {0015},
year = {2018}
}
@article{Gueuning2018,
abstract = {The focused information criterion for model selection is constructed to select the model that best estimates a particular quantity of interest, the focus, in terms of mean squared error. We extend this focused selection process to the high-dimensional regression setting with potentially a larger number of parameters than the size of the sample. We distinguish two cases: (i) the case where the considered submodel is of low-dimension and (ii) the case where it is of high-dimension. In the former case, we obtain an alternative expression of the low-dimensional focused information criterion that can directly be applied. In the latter case we use a desparsified estimator that allows us to derive the mean squared error of the focus estimator. We illustrate the performance of the high-dimensional focused information criterion with a numerical study and a real dataset.},
author = {Gueuning, Thomas and Claeskens, Gerda},
doi = {10.1111/sjos.12285},
file = {:home/kian/文件/Mendeley Desktop/Gueuning, Claeskens - 2018 - A High-dimensional Focused Information Criterion.pdf:pdf},
issn = {14679469},
journal = {Scandinavian Journal of Statistics},
keywords = {desparsified estimator,focused information criterion,high-dimensional data,variable selection},
number = {1},
pages = {34--61},
title = {{A High-dimensional Focused Information Criterion}},
volume = {45},
year = {2018}
}
@article{Chan2014,
abstract = {Abstract Consider a structural break autoregressive (SBAR) process where j=1, ?, m+1, {\{}t 1, ?, tm {\}} are change points, 1=t 0{\textless}t 1{\textless}???{\textless}t m+1=n+1, $\sigma$({\textperiodcentered}) is a measurable function on , and {\{}? t {\}} are white noise with unit variance. In practice, the number of change points m is usually assumed to be known and small, because a large m would involve a huge amount of computational burden for parameters estimation. By reformulating the problem in a variable selection context, the group least absolute shrinkage and selection operator (LASSO) is proposed to estimate an SBAR model when m is unknown. It is shown that both m and the locations of the change points {\{}t 1, ?, tm {\}} can be consistently estimated from the data, and the computation can be efficiently performed. An improved practical version that incorporates group LASSO and the stepwise regression variable selection technique are discussed. Simulation studies are conducted to assess the finite sample performance. Abstract Consider a structural break autoregressive (SBAR) process where j=1, ?, m+1, {\{}t 1, ?, tm {\}} are change points, 1=t 0{\textless}t 1{\textless}???{\textless}t m+1=n+1, $\sigma$({\textperiodcentered}) is a measurable function on , and {\{}? t {\}} are white noise with unit variance. In practice, the number of change points m is usually assumed to be known and small, because a large m would involve a huge amount of computational burden for parameters estimation. By reformulating the problem in a variable selection context, the group least absolute shrinkage and selection operator (LASSO) is proposed to estimate an SBAR model when m is unknown. It is shown that both m and the locations of the change points {\{}t 1, ?, tm {\}} can be consistently estimated from the data, and the computation can be efficiently performed. An improved practical version that incorporates group LASSO and the stepwise regression variable selection technique are discussed. Simulation studies are conducted to assess the finite sample performance.},
author = {Chan, Ngai Hang and Yau, Chun Yip and Zhang, Rong Mao},
doi = {10.1080/01621459.2013.866566},
file = {:home/kian/文件/Mendeley Desktop/Chan, Yau, Zhang - 2014 - Group LASSO for structural break time series.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Change-points,Information criterion,Nonstationary autoregressive process},
number = {506},
pages = {590--599},
title = {{Group LASSO for structural break time series}},
volume = {109},
year = {2014}
}
@article{Hansen2005,
abstract = {We propose a new test for superior predictive ability. The new test compares favorable to the reality check for data snooping (RC), because the former is more powerful and less sensitive to poor and irrelevant alternatives. The improvements are achieved by two modifications of the RC. We employ a studentized test statistic that reduces the influence of erratic forecasts and invoke a sample dependent null distribution. The advantages of the new test are confirmed by Monte Carlo experiments and in an empirical exercise, where we compare a large number of regression-based forecasts of annual US inflation to a simple random walk forecast. The random walk forecast is found to be inferior to regression-based forecasts and, interestingly, the best sample performance is achieved by models that have a Phillips curve structure.},
author = {Hansen, Peter Reinhard},
doi = {10.1198/073500105000000063},
file = {:home/kian/文件/Mendeley Desktop/Hansen - 2005 - A test for superior predictive ability.pdf:pdf},
issn = {07350015},
journal = {Journal of Business and Economic Statistics},
keywords = {Forecast evaluation,Forecasting,Inequality testing,Multiple comparison,Testing for superior predictive ability},
number = {4},
pages = {365--380},
title = {{A test for superior predictive ability}},
volume = {23},
year = {2005}
}
@article{Guo2016,
abstract = {We consider a class of vector autoregressive models with banded coefficient matrices. The setting represents a type of sparse structure for high-dimensional time series, though the implied autocovariance matrices are not banded. The structure is also practically meaningful when the order of component time series is arranged appropriately. The convergence rates for the estimated banded autoregressive coefficient matrices are established. We also propose a Bayesian information criterion for determining the width of the bands in the coefficient matrices, which is proved to be consistent. By exploring some approximate banded structure for the auto-covariance functions of banded vector autoregressive processes, consistent estimators for the auto-covariance matrices are constructed.},
author = {Guo, Shaojun and Wang, Yazhen and Yao, Qiwei},
doi = {10.1093/biomet/asw046},
file = {:home/kian/文件/Mendeley Desktop/Guo, Wang, Yao - 2016 - High-dimensional and banded vector autoregressions.pdf:pdf},
issn = {14643510},
journal = {Biometrika},
keywords = {Banded auto-coefficient matrix,Bayesian information criterion,Frobenius norm,Vector autoregressive model.},
number = {4},
pages = {889--903},
title = {{High-dimensional and banded vector autoregressions}},
volume = {103},
year = {2016}
}
@article{Hong2017,
abstract = {The key to time series clustering is how to characterize the similarity between any two time series. In this paper, we explore a new similarity metric called “cross-predictability”: the degree to which a future value in each time series is predicted by past values of the others. However, it is challenging to estimate such cross-predictability among time series in the high-dimensional regime, where the number of time series is much larger than the length of each time series. We address this challenge with a sparsity assumption: only time series in the same cluster have significant cross-predictability with each other. We demonstrate that this approach is computationally attractive, and provide a theoretical proof that the proposed algorithm will identify the correct clustering structure with high probability under certain conditions. To the best of our knowledge, this is the first practical high-dimensional time series clustering algorithm with a provable guarantee. We evaluate with experiments on both synthetic data and real-world data, and results indicate that our method can achieve more than 80{\%} clustering accuracy on real-world data, which is 20{\%} higher than the state-of-art baselines.},
author = {Hong, Dezhi and Gu, Quanquan and Whitehouse, Kamin},
file = {:home/kian/文件/Mendeley Desktop/Hong, Gu, Whitehouse - 2017 - High-dimensional Time Series Clustering via Cross-Predictability.pdf:pdf},
issn = {1938-7228},
journal = {Aistats},
pages = {642--651},
title = {{High-dimensional Time Series Clustering via Cross-Predictability}},
url = {http://proceedings.mlr.press/v54/hong17a.html},
volume = {54},
year = {2017}
}
@article{Rossi2011,
abstract = {This paper introduces the model confidence set (MCS) and applies it to the selection of models. An MCS is a set of models that is constructed so that it will contain the best model with a given level of confidence. The MCS is in this sense analogous to a confidence interval for a parameter. The MCS acknowledges the limitations of the data; uninformative data yield an MCS with many models whereas informative data yield an MCS with only a few models. The MCS procedure does not assume that a particular model is the true model; in fact, the MCS procedure can be used to compare more general objects, beyond the comparison of models. We apply the MCS procedure to two empirical problems. First, we revisit the inflation forecasting problem posed by Stock and Watson (1999) and compute the MCS for their set of inflation forecasts. Second, we compare a number of Taylor rule regressions and determine the MCS of the best in terms of in-sample likelihood criteria.},
author = {Rossi, Barbara and Stock, Jim and Wolf, Michael},
doi = {10.3982/ecta5771},
file = {:home/kian/文件/Mendeley Desktop/Rossi, Stock, Wolf - 2011 - The Model Confidence Set.pdf:pdf},
issn = {0012-9682},
journal = {Econometrica},
number = {2},
pages = {453--497},
title = {{The Model Confidence Set}},
volume = {79},
year = {2011}
}
@article{Inoue2019,
author = {Inoue, Atsushi and Kilian, Lutz},
doi = {10.2139/ssrn.3332545},
file = {:home/kian/文件/Mendeley Desktop/Inoue, Kilian - 2019 - The Uniform Validity of Impulse Response Inference in Autoregressions.pdf:pdf},
journal = {SSRN Electronic Journal},
keywords = {asymptotic normality,autoregression,bootstrap,impulse response,lag augmentation},
title = {{The Uniform Validity of Impulse Response Inference in Autoregressions}},
year = {2019}
}

@article{Javanmard2014,
abstract = {Fitting high-dimensional statistical models often requires the use of non-linear parameter estimation procedures. As a consequence, it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates. This in turn implies that it is extremely challenging to quantify the $\backslash$emph{\{}uncertainty{\}} associated with a certain parameter estimate. Concretely, no commonly accepted procedure exists for computing classical measures of uncertainty and statistical significance as confidence intervals or {\$}p{\$}-values for these models. We consider here high-dimensional linear regression problem, and propose an efficient algorithm for constructing confidence intervals and {\$}p{\$}-values. The resulting confidence intervals have nearly optimal size. When testing for the null hypothesis that a certain parameter is vanishing, our method has nearly optimal power. Our approach is based on constructing a `de-biased' version of regularized M-estimators. The new construction improves over recent work in the field in that it does not assume a special structure on the design matrix. We test our method on synthetic data and a high-throughput genomic data set about riboflavin production rate.},
archivePrefix = {arXiv},
arxivId = {1306.3171},
author = {Javanmard, Adel and Montanari, Andrea},
eprint = {1306.3171},
file = {:home/kian/文件/Mendeley Desktop/Javanmard, Montanari - 2014 - Confidence Intervals and Hypothesis Testing for High-Dimensional Regression.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {bias of an estimator,confidence intervals,high-dimensional models,hypothesis testing,lasso},
pages = {2869--2909},
title = {{Confidence Intervals and Hypothesis Testing for High-Dimensional Regression}},
url = {http://arxiv.org/abs/1306.3171},
volume = {15},
year = {2014}
}

@article{Chiou2007,
abstract = {A functional clustering (FC) method, k-centres FC, for longitudinal data is proposed. The k-centres FC approach accounts for both the means and the modes of variation differentials between clusters by predicting cluster membership with a reclassification step. The cluster membership predictions are based on a non-parametric iterative mean and covariance updating scheme. We show that, under the identifiability conditions derived, the k-centres FC method proposed can greatly improve cluster quality as compared with conventional clustering algorithms. Moreover, by exploring the mean and covariance functions of each cluster, the k-centres FC method provides an additional insight into cluster structures which facilitates functional cluster analysis. Practical performance of the k-centres FC method is demonstrated through simulation studies and data applications including growth curve and gene expression profile data.},
author = {Chiou, Jeng-min and Li, Pai-Ling},
file = {:home/kian/文件/Mendeley Desktop/Chiou, Li - 2007 - Functional clustering and identifying substructures.pdf:pdf},
journal = {Journal of Royal Statistical Society, Series B},
keywords = {classification,clustering,functional data,functional principal component,modes of variation,stochastic processes},
number = {4},
pages = {679--699},
title = {{Functional clustering and identifying substructures}},
volume = {69},
year = {2007}
}
@article{Lahiri2015,
abstract = {This paper provides a critical review of the popular Carlson-Parkin (CP) quantification method using household-level data from the University of Michigan's Survey of Consumers. We find strong evidence against the threshold constancy, symmetry, homogeneity, and overall unbiasedness assumptions of the CP method. To address these violations, we generalize the CP method using a hierarchical ordered probit (HOPIT) model. By comparing the quantified inflation expectations with quantitative expectations obtained from the same set of households directly, we show that the generalized model performs better than the CP method. In particular, when the CP unbiasedness assumption is replaced by a time-varying calibration, the resulting quantified series is found to track the quantitative benchmark well, over diverse time periods.},
author = {Lahiri, Kajal and Zhao, Yongchen},
doi = {10.1016/j.ijforecast.2014.06.003},
file = {:home/kian/文件/Mendeley Desktop/Lahiri, Zhao - 2015 - Quantifying survey expectations A critical review and generalization of the Carlson-Parkin method.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {HOPIT model,Household data,Inflation rate},
number = {1},
pages = {51--62},
publisher = {Elsevier B.V.},
title = {{Quantifying survey expectations: A critical review and generalization of the Carlson-Parkin method}},
url = {http://dx.doi.org/10.1016/j.ijforecast.2014.06.003},
volume = {31},
year = {2015}
}
@article{Kim2018,
abstract = {A number of recent studies in the economics literature have focused on the usefulness of factor models in the context of prediction using “big data” (see Bai and Ng, 2008; Dufour and Stevanovic, 2010; Forni, Hallin, Lippi, {\&} Reichlin, 2000; Forni et al., 2005; Kim and Swanson, 2014a; Stock and Watson, 2002b, 2006, 2012, and the references cited therein). We add to this literature by analyzing whether “big data” are useful for modelling low frequency macroeconomic variables, such as unemployment, inflation and GDP. In particular, we analyze the predictive benefits associated with the use of principal component analysis (PCA), independent component analysis (ICA), and sparse principal component analysis (SPCA). We also evaluate machine learning, variable selection and shrinkage methods, including bagging, boosting, ridge regression, least angle regression, the elastic net, and the non-negative garotte. Our approach is to carry out a forecasting “horse-race” using prediction models that are constructed based on a variety of model specification approaches, factor estimation methods, and data windowing methods, in the context of predicting 11 macroeconomic variables that are relevant to monetary policy assessment. In many instances, we find that various of our benchmark models, including autoregressive (AR) models, AR models with exogenous variables, and (Bayesian) model averaging, do not dominate specifications based on factor-type dimension reduction combined with various machine learning, variable selection, and shrinkage methods (called “combination” models). We find that forecast combination methods are mean square forecast error (MSFE) “best” for only three variables out of 11 for a forecast horizon of h=1, and for four variables when h=3 or 12. In addition, non-PCA type factor estimation methods yield MSFE-best predictions for nine variables out of 11 for h=1, although PCA dominates at longer horizons. Interestingly, we also find evidence of the usefulness of combination models for approximately half of our variables when h{\textgreater}1. Most importantly, we present strong new evidence of the usefulness of factor-based dimension reduction when utilizing “big data” for macroeconometric forecasting.},
author = {Kim, Hyun Hak and Swanson, Norman R.},
doi = {10.1016/j.ijforecast.2016.02.012},
file = {:home/kian/文件/Mendeley Desktop/Kim, Swanson - 2018 - Mining big data using parsimonious factor, machine learning, variable selection and shrinkage methods(2).pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Bagging,Bayesian model averaging,Boosting,Elastic net and non-negative garotte,Independent component analysis,Least angle regression,Prediction,Ridge regression,Sparse principal component analysis},
number = {2},
pages = {339--354},
publisher = {Elsevier B.V.},
title = {{Mining big data using parsimonious factor, machine learning, variable selection and shrinkage methods}},
url = {http://dx.doi.org/10.1016/j.ijforecast.2016.02.012},
volume = {34},
year = {2018}
}
@article{Wong2016,
abstract = {Many theoretical results on estimation of high dimensional time series require specifying an underlying data generating model (DGM). Instead, along the footsteps of{\~{}}$\backslash$cite{\{}wong2017lasso{\}}, this paper relies only on (strict) stationarity and {\$} \backslashbeta {\$}-mixing condition to establish consistency of lasso when data comes from a {\$}\backslashbeta{\$}-mixing process with marginals having subgaussian tails. Because of the general assumptions, the data can come from DGMs different than standard time series models such as VAR or ARCH. When the true DGM is not VAR, the lasso estimates correspond to those of the best linear predictors using the past observations. We establish non-asymptotic inequalities for estimation and prediction errors of the lasso estimates. Together with{\~{}}$\backslash$cite{\{}wong2017lasso{\}}, we provide lasso guarantees that cover full spectrum of the parameters in specifications of {\$} \backslashbeta {\$}-mixing subgaussian time series. Applications of these results potentially extend to non-Gaussian, non-Markovian and non-linear times series models as the examples we provide demonstrate. In order to prove our results, we derive a novel Hanson-Wright type concentration inequality for {\$}\backslashbeta{\$}-mixing subgaussian random vectors that may be of independent interest.},
archivePrefix = {arXiv},
arxivId = {1602.04265},
author = {Wong, Kam Chung and Li, Zifan and Tewari, Ambuj},
eprint = {1602.04265},
file = {:home/kian/文件/Mendeley Desktop/Wong, Li, Tewari - 2016 - Lasso Guarantees for Time Series Estimation Under Subgaussian Tails and {\$} beta {\$}-Mixing.pdf:pdf},
number = {February},
title = {{Lasso Guarantees for Time Series Estimation Under Subgaussian Tails and {\$} \backslashbeta {\$}-Mixing}},
url = {http://arxiv.org/abs/1602.04265},
year = {2016}
}
@article{Muller2016,
abstract = {Long-run forecasts of economic variables play an important role in policy, planning, and portfolio decisions. We consider long-horizon forecasts of average growth of a scalar variable, assuming that first differences are second-order stationary. The main contribution is the construction of predictive sets with asymptotic coverage over a wide range of data generating processes, allowing for stochastically trending mean growth, slow mean reversion and other types of long-run dependencies. We illustrate the method by computing predictive sets for 10 to 75 year average growth rates of U.S. real per-capita GDP, consumption, productivity, price level, stock prices and population.},
author = {M{\"{u}}ller, Ulrich K. and Watson, Mark W.},
doi = {10.1093/restud/rdw003},
file = {:home/kian/文件/Mendeley Desktop/M{\"{u}}ller, Watson - 2016 - Measuring uncertainty about long-run predictions.pdf:pdf},
issn = {1467937X},
journal = {Review of Economic Studies},
keywords = {Least favourable distribution,Low frequency,Prediction interval,Spectral analysis},
number = {4},
pages = {1711--1740},
title = {{Measuring uncertainty about long-run predictions}},
volume = {83},
year = {2016}
}
@article{Sin2019,
author = {yiu Sin, Chor and Yu, Shu Hui},
doi = {10.1007/s10182-018-00333-1},
file = {:home/kian/文件/Mendeley Desktop/Sin, Yu - 2019 - Order selection for possibly infinite-order non-stationary time series.pdf:pdf},
issn = {18638171},
journal = {AStA Advances in Statistical Analysis},
keywords = {Asymptotic efficiency,Lasso,Non-stationary,Possibly infinite-order,TSIC},
number = {2},
pages = {187--216},
publisher = {Springer Berlin Heidelberg},
title = {{Order selection for possibly infinite-order non-stationary time series}},
url = {https://doi.org/10.1007/s10182-018-00333-1},
volume = {103},
year = {2019}
}
@article{Dendramis2019,
abstract = {In the aftermath of the recent financial crisis there has been considerable focus on methods for predicting macroeconomic variables when their behavior is subject to abrupt changes, associated for example with crisis periods. In this paper we propose similarity based approaches as a way to handle parameter instability, and apply them to macroeconomic forecasting. The rationale is that clusters of past data that match the current economic conditions can be more informative for forecasting than the entire past behavior of the variable of interest. We apply our methods to predict both simulated variables in a set of Monte Carlo experiments, and a broad set of key US macroeconomic indicators. The forecast evaluation exercises indicate that similarity-based approaches perform well in general in comparison with other common time-varying forecasting methods, and particularly well during crisis episodes.},
author = {Dendramis, Yiannis and Kapetanios, George and Marcellino, Massimiliano},
doi = {10.2139/ssrn.3330082},
file = {:home/kian/文件/Mendeley Desktop/Dendramis, Kapetanios, Marcellino - 2019 - A Similarity-Based Approach for Macroeconomic Forecasting.pdf:pdf},
journal = {SSRN Electronic Journal},
number = {February},
title = {{A Similarity-Based Approach for Macroeconomic Forecasting}},
year = {2019}
}
@article{Tibshirani2016,
abstract = {We consider regression scenarios where it is natural to impose an order constraint on the coefficients. We propose an order-constrained version of L1-regularized regression for this problem, and show how to solve it efficiently using the well-known Pool Adjacent Violators Algorithm as its proximal operator. The main application of this idea is time-lagged regression, where we predict an outcome at time t from features at the previous K time points. In this setting it is natural to assume that the coefficients decay as we move farther away from t, and hence the order constraint is reasonable. Potential applications include financial time series and prediction of dynamic patient out- comes based on clinical measurements. We illustrate this idea on real and simulated data.},
author = {Tibshirani, Robert and Suo, Xiaotong},
doi = {10.1080/00401706.2015.1079245},
file = {:home/kian/文件/Mendeley Desktop/Tibshirani, Suo - 2016 - An Ordered Lasso and Sparse Time-Lagged Regression.pdf:pdf},
issn = {15372723},
journal = {Technometrics},
keywords = {Feature selection,Monotone coefficients,Penalized regression},
number = {4},
pages = {415--423},
title = {{An Ordered Lasso and Sparse Time-Lagged Regression}},
volume = {58},
year = {2016}
}
@article{James2019,
abstract = {We introduce a novel application of Support Vector Machines (SVM), an important Machine Learning algorithm, to determine the beginning and end of recessions in real time. Nowcasting, "forecasting" a condition about the present time because the full information about it is not available until later, is key for recessions, which are only determined months after the fact. We show that SVM has excellent predictive performance for this task, and we provide implementation details to facilitate its use in similar problems in economics and finance.},
author = {James, Alex and Abu-Mostafa, Yaser and Qiao, Xiao},
doi = {10.2139/ssrn.3316917},
file = {:home/kian/文件/Mendeley Desktop/James, Abu-Mostafa, Qiao - 2019 - Nowcasting Recessions Using the SVM Machine Learning Algorithm.pdf:pdf},
journal = {SSRN Electronic Journal},
title = {{Nowcasting Recessions Using the SVM Machine Learning Algorithm}},
year = {2019}
}
@article{Sutton2018,
abstract = {The Reinforcement Learning Problem 1 1.1 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 2 1.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.3 Elements of Reinforcement Learning . . . . . . . . . . . . . . 7 1.4 Limitations and Scope . . . . . . . . . . . . . . . . . . . . . . 9 1.5 An Extended Example: Tic-Tac-Toe . . . . . . . . . . . . . . 10 1.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 1.7 History of Reinforcement Learning . . . . . . . . . . . . . . . 16 1.8 Bibliographical Remarks . . . . . . . . . . . . . . . . . . . . . 25 I Tabular Solution Methods 27 2 Multi-arm Bandits 31 2.1 An n-Armed Bandit Problem . . . . . . . . . . . . . . . . . . 32 2.2 Action-Value Methods . . . . . . . . . . . . . . . . . . . . . . 33 2.3 Incremental Implementation . . . . . . . . . . . . . . . . . . . 36 2.4 Tracking a Nonstationary Problem . . . . . . . . . . . . . . . 38 2.5 Optimistic Initial Values . . . . . . . . . . . . . . . . . . . . . 39 2.6 Upper-Confidence-Bound Action Selection . . . . . . . . . . . 41 2.7 Gradient Bandits . . . . . . . . . . . . . . . . . . . . . . . . . 42 2.8 Associative Search (Contextual Bandits) . . . . . . . . . . . . 46 2.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 3 Finite Markov Decision Processes 53 3.1 The Agent–Environment Interface . . . . . . . . . . . . . . . . 53 3.2 Goals and Rewards . . . . . . . . . . . . . . . . . . . . . . . . 57 3.3 Returns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 3.4 Unified Notation for Episodic and Continuing Tasks . . . . . . 61 ∗3.5 The Markov Property . . . . . . . . . . . . . . . . . . . . . . . 62 3.6 Markov Decision Processes . . . . . . . . . . . . . . . . . . . . 67 3.7 Value Functions . . . . . . . . . . . . . . . . . . . . . . . . . . 70 3.8 Optimal Value Functions . . . . . . . . . . . . . . . . . . . . . 75 3.9 Optimality and Approximation . . . . . . . . . . . . . . . . . 79 3.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 4 Dynamic Programming 89 4.1 Policy Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 90 4.2 Policy Improvement . . . . . . . . . . . . . . . . . . . . . . . . 94 4.3 Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . 96 4.4 Value Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . 98 4.5 Asynchronous Dynamic Programming . . . . . . . . . . . . . . 101 4.6 Generalized Policy Iteration . . . . . . . . . . . . . . . . . . . 104 4.7 Efficiency of Dynamic Programming . . . . . . . . . . . . . . . 106 4.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 5 Monte Carlo Methods 113 5.1 Monte Carlo Prediction . . . . . . . . . . . . . . . . . . . . . . 114 5.2 Monte Carlo Estimation of Action Values . . . . . . . . . . . . 119 5.3 Monte Carlo Control . . . . . . . . . . . . . . . . . . . . . . . 120 5.4 Monte Carlo Control without Exploring Starts . . . . . . . . . 124 5.5 Off-policy Prediction via Importance Sampling . . . . . . . . . 127 5.6 Incremental Implementation . . . . . . . . . . . . . . . . . . . 133 5.7 Off-Policy Monte Carlo Control . . . . . . . . . . . . . . . . . 135 ∗5.8 Importance Sampling on Truncated Returns . . . . . . . . . . 136 5.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138 6 Temporal-Difference Learning 143 6.1 TD Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . 143 6.2 Advantages of TD Prediction Methods . . . . . . . . . . . . . 148 6.3 Optimality of TD(0) . . . . . . . . . . . . . . . . . . . . . . . 151 6.4 Sarsa: On-Policy TD Control . . . . . . . . . . . . . . . . . . 154 6.5 Q-Learning: Off-Policy TD Control . . . . . . . . . . . . . . . 157 6.6 Games, Afterstates, and Other Special Cases . . . . . . . . . . 160 6.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161 7 Eligibility Traces 167 7.1 n-Step TD Prediction . . . . . . . . . . . . . . . . . . . . . . . 168 7.2 The Forward View of TD($\lambda$) . . . . . . . . . . . . . . . . . . . 172 7.3 The Backward View of TD($\lambda$) . . . . . . . . . . . . . . . . . . 177 7.4 Equivalences of Forward and Backward Views . . . . . . . . . 181 7.5 Sarsa($\lambda$) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183 7.6 Watkins's Q($\lambda$) . . . . . . . . . . . . . . . . . . . . . . . . . . 186 7.7 Off-policy Eligibility Traces using Importance Sampling . . . . 188 7.8 Implementation Issues . . . . . . . . . . . . . . . . . . . . . . 189 ∗7.9 Variable $\lambda$ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190 7.10 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190 8 Planning and Learning with Tabular Methods 195 8.1 Models and Planning . . . . . . . . . . . . . . . . . . . . . . . 195 8.2 Integrating Planning, Acting, and Learning . . . . . . . . . . . 198 8.3 When the Model Is Wrong . . . . . . . . . . . . . . . . . . . . 203 Prioritized Sweeping . . . . . . . . . . . . . . . . . . . . . . . 206 8.5 Full vs. Sample Backups . . . . . . . . . . . . . . . . . . . . . 210 8.6 Trajectory Sampling . . . . . . . . . . . . . . . . . . . . . . . 213 8.7 Heuristic Search . . . . . . . . . . . . . . . . . . . . . . . . . . 217 8.8 Monte Carlo Tree Search . . . . . . . . . . . . . . . . . . . . . 220 8.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220 II Approximate Solution Methods 223 9 On-policy Approximation of Action Values 225 9.1 Value Prediction with Function Approximation . . . . . . . . 226 9.2 Gradient-Descent Methods . . . . . . . . . . . . . . . . . . . . 229 9.3 Linear Methods . . . . . . . . . . . . . . . . . . . . . . . . . . 232 9.4 Control with Function Approximation . . . . . . . . . . . . . . 241 9.5 Should We Bootstrap? . . . . . . . . . . . . . . . . . . . . . . 247 9.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249 10 Off-policy Approximation of Action Values 255 11 Policy Approximation 257 11.1 Actor–Critic Methods . . . . . . . . . . . . . . . . . . . . . . . 257 11.2 Eligibility Traces for Actor–Critic Methods . . . . . . . . . . . 259 11.3 R-Learning and the Average-Reward Setting . . . Frontiers 12 Psychology 13 Neuroscience 265 269 271 14 Applications and Case Studies 273 14.1 TD-Gammon . . . . . . . . . Samuel's Checkers Player . . . . . . . . . . . . . . . . . . . . . 279 14.3 The Acrobot . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282 14.4 Elevator Dispatching . . . . . . . . . . . . . . . . . . . . . . . 286 14.5 Dynamic Channel Allocation . . . . . . . . . . . . . . . . . . . 291 14.6 Job-Shop Scheduling . . . . . . . . . . . . . . . . . . . . . . . 295 15 Prospects 303 15.1 The Unified View . . . . . . . . . . . . . . . . . . . . . . . . . 303 15.2 State Estimation . . . . . . . . . . . . . . . . . . . . . . . . . 306 15.3 Temporal Abstraction . . . . . . . . . . . . . . . . . . . . . . 306 15.4 Predictive Representations . . . . . . . . . . . . . . . . . . . . 306 15.5 Other Frontier Dimensions . . . . .},
author = {Sutton, Richard and Barto, Andrew},
file = {:home/kian/文件/Mendeley Desktop/Sutton, Barto - 2018 - Reinforcment Learning.pdf:pdf},
isbn = {0262193981},
issn = {00976156},
journal = {MIT Press},
title = {{Reinforcment Learning}},
year = {2018}
}
@article{Ghysels2016,
abstract = {When modeling economic relationships it is increasingly common to encounter data sampled at different frequencies. We introduce the R package midasr which enables estimating regression models with variables sampled at different frequencies within a MIDAS regression framework put forward in work by Ghysels, Santa-Clara, and Valkanov (2002). In this article we define a general autoregressive MIDAS regression model with multiple variables of different frequencies and show how it can be specified using the familiar R formula interface and estimated using various optimization methods chosen by the researcher. We discuss how to check the validity of the estimated model both in terms of numerical convergence and statistical adequacy of a chosen regression specification, how to perform model selection based on a information criterion, how to assess forecasting accuracy of the MIDAS regression model and how to obtain a forecast aggregation of different MIDAS regression models. We illustrate the capabilities of the package with a simulated MIDAS regression model and give two empirical examples of application of MIDAS regression.},
author = {Ghysels, Eric and Kvedaras, Virmantas and Zemlys, Vaidotas},
doi = {10.18637/jss.v072.i04},
file = {:home/kian/文件/Mendeley Desktop/Ghysels, Kvedaras, Zemlys - 2016 - Mixed Frequency Data Sampling Regression Models The iRi Package bmidasrb.pdf:pdf},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {midas,specification test},
number = {4},
title = {{Mixed Frequency Data Sampling Regression Models: The {\textless}i{\textgreater}R{\textless}/i{\textgreater} Package {\textless}b{\textgreater}midasr{\textless}/b{\textgreater}}},
url = {http://www.jstatsoft.org/v72/i04/},
volume = {72},
year = {2016}
}
@article{Medeiros2019a,
abstract = {Inflation forecasting is an important but difficult task. We explore the advances in machine learning (ML) methods and the availability of new datasets to forecast US inflation. Despite the skepticism in the previous literature, we show that ML models with a large number of covariates are systematically more accurate than the benchmarks. The ML method that deserves more attention is the random forest, which dominated all other models. Its good performance is due not only to its specific method of variable selection but also the potential nonlinearities between past key macroeconomic variables and inflation. },
author = {Medeiros, Marcelo C. and Vasconcelos, Gabriel F. R. and Veiga, {\'{A}}lvaro and Zilberman, Eduardo},
doi = {10.1080/07350015.2019.1637745},
file = {:home/kian/文件/Mendeley Desktop/Medeiros et al. - 2019 - Forecasting Inflation in a Data-Rich Environment The Benefits of Machine Learning Methods(2).pdf:pdf},
issn = {0735-0015},
journal = {Journal of Business {\&} Economic Statistics},
number = {April},
pages = {1--45},
title = {{Forecasting Inflation in a Data-Rich Environment: The Benefits of Machine Learning Methods}},
url = {https://www.tandfonline.com/doi/full/10.1080/07350015.2019.1637745},
year = {2019}
}
@article{Hansheng2007,
abstract = {The least absolute shrinkage and selection operator ('lasso') has been widely used in regression shrinkage and selection. We extend its application to the regression model with autoregressive errors. Two types of lasso estimators are carefully studied. The first is similar to the traditional lasso estimator with only two tuning parameters (one for regression coefficients and the other for autoregression coefficients). These tuning parameters can be easily calculated via a data-driven method, but the resulting lasso estimator may not be fully efficient. To overcome this limitation, we propose a second lasso estimator which uses different tuning parameters for each coefficient. We show that this modified lasso can produce the estimator as efficiently as the oracle. Moreover, we propose an algorithm for tuning parameter estimates to obtain the modified lasso estimator. Simulation studies demonstrate that the modified estimator is superior to the traditional estimator. One empirical example is also presented to illustrate the usefulness of lasso estimators. The extension of the lasso to the autoregression with exogenous variables model is briefly discussed. {\textcopyright} 2007 Royal Statistical Society.},
author = {Hansheng, Wang and Guodong, Li and Tsai, Chin Ling},
doi = {10.1111/j.1467-9868.2007.00577.x},
file = {:home/kian/文件/Mendeley Desktop/Hansheng, Guodong, Tsai - 2007 - Regression coefficient and autoregressive order shrinkage and selection via the lasso.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Autoregression with exogenous variables,Lasso,Oracle estimator,Regression model with autoregressive errors},
number = {1},
pages = {63--78},
title = {{Regression coefficient and autoregressive order shrinkage and selection via the lasso}},
volume = {69},
year = {2007}
}
@article{Chen2016,
abstract = {This paper studies a Dantzig-selector type regularized estimator for linear functionals of high-dimensional linear processes. Explicit rates of convergence of the proposed estimator are obtained and they cover the broad regime from i.i.d. samples to long-range dependent time series and from sub-Gaussian innovations to those with mild polynomial moments. It is shown that the convergence rates depend on the degree of temporal dependence and the moment conditions of the underlying linear processes. The Dantzig-selector estimator is applied to the sparse Markowitz portfolio allocation and the optimal linear prediction for time series, in which the ratio consistency when compared with an oracle estimator is established. The effect of dependence and innovation moment conditions is further illustrated in the simulation study. Finally, the regularized estimator is applied to classify the cognitive states on a real fMRI dataset and to portfolio optimization on a financial dataset.},
author = {Chen, Xiaohui and Xu, Mengyu and Wu, Wei Biao},
doi = {10.1109/TSP.2016.2605079},
file = {:home/kian/文件/Mendeley Desktop/Chen, Xu, Wu - 2016 - Regularized Estimation of Linear Functionals of Precision Matrices for High-Dimensional Time Series.pdf:pdf},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {High-dimension,regularization,sparsity,time series},
number = {24},
pages = {6459--6470},
publisher = {IEEE},
title = {{Regularized Estimation of Linear Functionals of Precision Matrices for High-Dimensional Time Series}},
volume = {64},
year = {2016}
}
@article{Arai2002,
author = {Arai, Yoichi and Yamamoto, Taku},
doi = {10.1016/s0165-1765(99)00278-5},
file = {:home/kian/文件/Mendeley Desktop/Arai, Yamamoto - 2002 - Alternative representation for asymptotic distributions of impulse responses in cointegrated VAR systems.pdf:pdf},
issn = {01651765},
journal = {Economics Letters},
keywords = {c32,cointegration,impulse response,jel classification,reduced rank regression},
number = {3},
pages = {261--271},
title = {{Alternative representation for asymptotic distributions of impulse responses in cointegrated VAR systems}},
volume = {67},
year = {2002}
}
@article{Nardi2011,
abstract = {The Lasso is a popular model selection and estimation procedure for linear models that enjoys nice theoretical properties. In this paper, we study the Lasso estimator for fitting autoregressive time series models. We adopt a double asymptotic framework where the maximal lag may increase with the sample size. We derive theoretical results establishing various types of consistency. In particular, we derive conditions under which the Lasso estimator for the autoregressive coefficients is model selection consistent, estimation consistent and prediction consistent. Simulation study results are reported. {\textcopyright} 2010 Elsevier Inc.},
author = {Nardi, Y. and Rinaldo, A.},
doi = {10.1016/j.jmva.2010.10.012},
file = {:home/kian/文件/Mendeley Desktop/Nardi, Rinaldo - 2011 - Autoregressive process modeling via the Lasso procedure.pdf:pdf},
issn = {0047259X},
journal = {Journal of Multivariate Analysis},
keywords = {Autoregressive model,Estimation consistency,Lasso procedure,Model selection,Prediction consistency},
number = {3},
pages = {528--549},
publisher = {Elsevier Inc.},
title = {{Autoregressive process modeling via the Lasso procedure}},
url = {http://dx.doi.org/10.1016/j.jmva.2010.10.012},
volume = {102},
year = {2011}
}
@article{Andreou2017,
abstract = {This paper shows that an economy's behavior differs significantly
according to assumptions made on the formation of inflation
expectations. We analyzed the behavior of an open economy in a regime of
explicit inflation targeting with commitment. The economy is exposed to
three different shocks - demand, supply, and exchange rate - and its
reaction is analyzed under three different assumptions on
inflation-expectations formation: naive, rational, and adaptive
learning. The economy in which rational expectations were assumed showed
the least volatile development and minimized the central bank's loss
function. The stabilizing effect of this forward-looking type of
expectation was most evident in the case of supply shock. When naive
expectations were assumed, the economy reacted to all shocks with
significantly bigger and longer-lasting fluctuations. The worst results
were obtained assuming adaptive-learning expectations, where shocks lead
to large oscillations and the economy stabilized only several years
after the shock.},
author = {Andreou, Elena and Eminidou, Snezana and Zachariadis, Marios},
doi = {10.2139/ssrn.2790743},
file = {:home/kian/文件/Mendeley Desktop/Andreou, Eminidou, Zachariadis - 2017 - Inflation Expectations and Monetary Policy in Europe.pdf:pdf},
journal = {SSRN Electronic Journal},
number = {2},
pages = {122--132},
title = {{Inflation Expectations and Monetary Policy in Europe}},
volume = {20},
year = {2017}
}
@article{Ahn1990,
abstract = {For an m-variate ("partially") nonstationary vector autoregressive$\backslash$nprocess {\{}Yt{\}}, we consider the autoregressive model $\Phi$(L)Yt = $\epsilon$t, where$\backslash$n$\Phi$(L) = I - $\Phi$1L - ⋯ - $\Phi$pL p and $\backslash$det{\{}$\Phi$(z){\}} = 0 has d {\textless} m$\backslash$nroots equal to unity and all other roots are outside the unit circle.$\backslash$nIt is also assumed that $\backslash$operatorname{\{}rank{\}}{\{}$\Phi$(1){\}} = r, r =$\backslash$nm - d , so that each component of the first differences Wt = Yt$\backslash$n- Yt-1 is stationary. The relation of the model to error correction$\backslash$nmodels and co-integration (Engle and Granger 1987) is discussed.$\backslash$nThe process {\{}Yt{\}} has the error correction representation $\Phi$*(L)(1$\backslash$n- L)Yt = -P2(Ir - $\Lambda$r)Q'2Y t-1 + $\epsilon$t, where Q(I{\_}m - $\Phi$(1))P = J$\backslash$n= $\backslash$operatorname{\{}diag{\}}(I{\_}d, $\Lambda${\_}r) is in Jordan canonical form$\backslash$nand Q' = [ Q1, Q2]. It follows that the transformation Zt = QYt =$\backslash$n[ Z'1t, Z'2t]' is such that the d × 1 process Z1t is nonstationary$\backslash$nwith Z1t - Z1t-1 stationary while Z2t is stationary. Asymptotic distribution$\backslash$ntheory for least squares parameter estimators of the model is first$\backslash$nconsidered. A Gaussian partial reduced rank estimation procedure$\backslash$nthat explicitly incorporates the unit root structure in the model$\backslash$nis then presented, and an asymptotically equivalent two-step reduced$\backslash$nrank estimation procedure is also considered. A numerical example$\backslash$nis presented to illustrate the methods and concepts. The finite sample$\backslash$nproperties of the estimators are also briefly examined through a$\backslash$nsmall Monte Carlo sampling experiment.},
author = {Ahn, Sung K. and Reinsel, Gregory C.},
doi = {10.1080/01621459.1990.10474945},
file = {:home/kian/文件/Mendeley Desktop/Ahn, Reinsel - 1990 - Estimation for partially nonstationary multivariate autoregressive models.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Autoregressive process,Error correction model,Least squares estimation,Reduced rank estimator,Unit roots},
number = {411},
pages = {813--823},
title = {{Estimation for partially nonstationary multivariate autoregressive models}},
volume = {85},
year = {1990}
}
@article{Bloom2015,
abstract = {Between now and 2030, every country will experience population ageing - a trend that is both pronounced and historically unprecedented. Over the past six decades, countries of the world had experienced only a slight increase in the share of people aged 60 years and older, from 8{\%} to 10{\%}. But in the next four decades, this group is expected to rise to 22{\%} of the total population - a jump from 800 million to 2 billion people. Evidence suggests that cohorts entering older age now are healthier than previous ones. However, progress has been very uneven, as indicated by the wide gaps in population health (measured by life expectancy) between the worst (Sierra Leone) and best (Japan) performing countries, now standing at a difference of 36 years for life expectancy at birth and 15 years for life expectancy at age 60 years. Population ageing poses challenges for countries' economies, and the health of older populations is of concern. Older people have greater health and long-term care needs than younger people, leading to increased expenditure. They are also less likely to work if they are unhealthy, and could impose an economic burden on families and society. Like everyone else, older people need both physical and economic security, but the burden of providing these securities will be falling on a smaller portion of the population. Pension systems will be stressed and will need reassessment along with retirement policies. Health systems, which have not in the past been oriented toward the myriad health problems and long-term care needs of older people and have not sufficiently emphasised disease prevention, can respond in different ways to the new demographic reality and the associated changes in population health. Along with behavioural adaptations by individuals and businesses, the nature of such policy responses will establish whether population ageing will lead to major macroeconomic difficulties.},
author = {Bloom, David E. and Chatterji, Somnath and Kowal, Paul and Lloyd-Sherlock, Peter and McKee, Martin and Rechel, Bernd and Rosenberg, Larry and Smith, James P.},
doi = {10.1016/S0140-6736(14)61464-1},
file = {:home/kian/文件/Mendeley Desktop/Bloom et al. - 2015 - Macroeconomic implications of population ageing and selected policy responses.pdf:pdf},
issn = {1474547X},
journal = {The Lancet},
number = {9968},
pages = {649--657},
publisher = {World Health Organization. Published by Elsevier Ltd/Inc/BV. All rights reserved.},
title = {{Macroeconomic implications of population ageing and selected policy responses}},
url = {http://dx.doi.org/10.1016/S0140-6736(14)61464-1},
volume = {385},
year = {2015}
}
@article{Breitung2013,
abstract = {We study a matched sample of individual stock market forecasts consisting of both qualitative and quantitative forecasts. This allows us to test for the quality of forecast quantification methods by comparing quantified qualitative forecasts with actual quantitative forecasts. Focusing mainly on the widely used quantification framework advocated by. Carlson and Parkin (1975), the so-called "probability approach", we find that quantified expectations derived from the probability approach display a surprisingly weak correlation with the reported quantitative stock return forecasts. We trace the reason for this low correlation to the importance of asymmetric and time-varying thresholds, while individual heterogeneity across forecasters seems to play only a minor role. Hence, our results suggest that qualitative survey data may not be a very useful device for obtaining quantitative forecasts, and we suggest ways to remedy this problem when designing qualitative surveys. {\textcopyright} 2012 International Institute of Forecasters.},
author = {Breitung, J{\"{o}}rg and Schmeling, Maik},
doi = {10.1016/j.ijforecast.2012.07.005},
file = {:home/kian/文件/Mendeley Desktop/Breitung, Schmeling - 2013 - Quantifying survey expectations What's wrong with the probability approach.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Probability approach,Quantification,Stock market expectations},
number = {1},
pages = {142--154},
publisher = {Elsevier B.V.},
title = {{Quantifying survey expectations: What's wrong with the probability approach?}},
url = {http://dx.doi.org/10.1016/j.ijforecast.2012.07.005},
volume = {29},
year = {2013}
}
@article{Bickel2011,
abstract = {The paper addresses a ‘large p–small n' problem in a time series framework and considers properties of banded regularization of an empirical autocovariance matrix of a time series process. Utilizing the banded autocovariance matrix enables us to fit amuch longer auto- regressive AR(p) model to the observed data than typically suggested by the Akaike information criterion, while controlling how many parameters are to be estimated precisely and the level of accuracy.We present results on asymptotic consistency of banded autocovariance matrices under the Frobenius norm and provide a theoretical justification on optimal band selection by using cross-validation. Remarkably, the cross-validation loss function for banded predic- tion is related to the conditional mean-square prediction error and, thus, may be viewed as an alternative model selection criterion. The procedure proposed is illustrated by simulations and application to predicting the sea surface temperature index in the Ni{\~{n}}o 3.4 region},
author = {Bickel, Peter J. and Gel, Yulia R.},
doi = {10.1111/j.1467-9868.2011.00779.x},
file = {:home/kian/文件/Mendeley Desktop/Bickel, Gel - 2011 - Banded regularization of autocovariance matrices in application to parameter estimation and forecasting of time ser.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Autocovariance and covariance matrices,Cross-validation,Forecasting,Model selection,Regularization,Time series},
number = {5},
pages = {711--728},
title = {{Banded regularization of autocovariance matrices in application to parameter estimation and forecasting of time series}},
volume = {73},
year = {2011}
}
@article{Bun2005,
abstract = {This study develops a new bias-corrected estimator for the fixed-effects dynamic panel data model and derives its limiting distribution for finite number of time periods, T, and large number of cross-section units, N. The bias-corrected estimator is derived as a bias correction of the least squares dummy variable (within) estimator. It does not share some of the drawbacks of recently developed instrumental variables and generalized method-of-moments estimators and is relatively easy to compute. Monte Carlo experiments provide evidence that the bias-corrected estimator performs well even in small samples. The proposed technique is applied in an empirical analysis of unemployment dynamics at the U.S. state level for the 1991–2000 period.},
author = {Bun, Maurice J.G. and Carree, Martin A.},
doi = {10.1198/073500104000000532},
file = {:home/kian/文件/Mendeley Desktop/Bun, Carree - 2005 - Bias-corrected estimation in dynamic panel data models.pdf:pdf},
issn = {07350015},
journal = {Journal of Business and Economic Statistics},
keywords = {Bias correction,Dynamic panel data model,Unemployment dynamics},
number = {2},
pages = {200--210},
title = {{Bias-corrected estimation in dynamic panel data models}},
volume = {23},
year = {2005}
}
@article{Xue2010,
abstract = {We consider the generalized additive model when responses from the$\backslash$nsame cluster are correlated. Incorporating correlation in the estimation$\backslash$nof nonparametric components for the generalized additive model is$\backslash$nimportant because it improves estimation efficiency and increases$\backslash$nstatistical power for model selection. In our setting, there is no$\backslash$nspecified likelihood function for the generalized additive model,$\backslash$nbecause the outcomes could be nonnormal and discrete, which makes$\backslash$nestimation and model selection very challenging problems. We propose$\backslash$nconsistent estimation and model selection that incorporate the correlation$\backslash$nstructure. We establish an asymptotic property with L2-norm consistency$\backslash$nfor the nonparametric components, which achieves the optimal rate$\backslash$nof convergence. In addition, the proposed model selection strategy$\backslash$nis able to select the correct generalized additive model consistently.$\backslash$nThat is, with probability approaching to 1, the estimators for the$\backslash$nzero function components converge to 0 almost surely. We illustrate$\backslash$nour method using numerical studies with both continuous and binary$\backslash$nresponses, along with a real data application of binary periodontal$\backslash$ndata. Supplemental materials including technical details are available$\backslash$nonline.},
author = {Xue, Lan and Qu, Annie and Zhou, Jianhui},
doi = {10.1198/jasa.2010.tm10128},
file = {:home/kian/文件/Mendeley Desktop/Xue, Qu, Zhou - 2010 - Consistent model selection for marginal generalized additive model for correlated data.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {L
                        2-norm consistency,Model selection,Nonparametric function,Oracle property,Polynomial spline,Quadratic inference function,SCAD},
number = {492},
pages = {1518--1530},
title = {{Consistent model selection for marginal generalized additive model for correlated data}},
volume = {105},
year = {2010}
}
@article{Chatterjee2015,
abstract = {This paper considers post variable-selection inference in a high dimensional penalized regression model based on the ALASSO method of Zou (2006). It is shown that under suitable sparsity conditions, the residual empirical process based on the ALASSO provides valid inference methodology in very high dimensional regression problems where conventional methods fail. It is also shown that the ALASSO based residual empirical process satisfies a functional oracle property, i.e., in addition to selecting the set of relevant variables with probability tending to one, the ALASSO based residual empirical process converges to the same limiting Gaussian process as the OLS based residual empirical process under the oracle. The functional oracle property is critically exploited to construct asymptotically valid confidence bands for the error distribution function and prediction intervals for unobserved values of the response variable in the high dimensional set up, where traditional non-penalized methods are known to fail. Simulation results are presented illustrating finite sample performance of the proposed methodology.},
author = {Chatterjee, A. and Gupta, S. and Lahiri, S. N.},
doi = {10.1016/j.jeconom.2015.02.012},
file = {:home/kian/文件/Mendeley Desktop/Chatterjee, Gupta, Lahiri - 2015 - On the residual empirical process based on the ALASSO in high dimensions and its functional oracle pr.pdf:pdf},
issn = {18726895},
journal = {Journal of Econometrics},
keywords = {Asymptotic uniform linearity,Brownian bridge,Oracle property,Prediction intervals,Regularization,Weak convergence},
number = {2},
pages = {317--324},
publisher = {Elsevier B.V.},
title = {{On the residual empirical process based on the ALASSO in high dimensions and its functional oracle property}},
url = {http://dx.doi.org/10.1016/j.jeconom.2015.02.012},
volume = {186},
year = {2015}
}
@article{Chiou2008,
abstract = {A correlation-based functional clustering method is proposed for grouping curves with similar shapes. A correlation between two random functions defined through the functional inner product is used as a similarity measure. Curves with similar shapes are embedded in the cluster subspace spanned by a mean shape function and eigenfunctions of the covariance kernel. The cluster membership prediction for each curve attempts to maximize the functional correlation between the observed and predicted curves via shape standardization and subspace projection among all possible clusters. The proposed method accounts for shape differentials through the functional multiplicative random-effects shape function model for each cluster, which regards random scales and intercept shifts as a nuisance. A consistent estimate is proposed for the random scale effect, whose sample variance estimate is also consistent. The derived identifiability conditions for the clustering procedure unravel the predictability of cluster memberships. Simulation studies and a real data example illustrate the proposed method. {\textcopyright} 2008 American Statistical Association.},
author = {Chiou, Jeng Min and Li, Pai Ling},
doi = {10.1198/016214508000000814},
file = {:home/kian/文件/Mendeley Desktop/Chiou, Li - 2008 - Correlation-based functional clustering via subspace projection.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Functional correlation,Functional data,Functional principal component analysis,Projection,Random scale effects,Shape similarity.},
number = {484},
pages = {1684--1692},
title = {{Correlation-based functional clustering via subspace projection}},
volume = {103},
year = {2008}
}
@article{Belanger,
author = {B{\'{e}}langer, {\'{E}}ric and Lewis-beck, Michael S and Nadeau, Richard},
file = {:home/kian/文件/Mendeley Desktop/B{\'{e}}langer, Lewis-beck, Nadeau - Unknown - PDFlib PLOP PDF Linearization , Optimization , Protection Page inserted by evaluation version.pdf:pdf},
title = {{PDFlib PLOP : PDF Linearization , Optimization , Protection Page inserted by evaluation version A Political Economy Forecast for the 2005 British General Election}}
}
@article{Eminidou2019,
abstract = {This paper shows that an economy's behavior differs significantly according to assumptions made on the formation of inflation expectations. We analyzed the behavior of an open economy in a regime of explicit inflation targeting with commitment. The economy is exposed to three different shocks - demand, supply, and exchange rate - and its reaction is analyzed under three different assumptions on inflation-expectations formation: naive, rational, and adaptive learning. The economy in which rational expectations were assumed showed the least volatile development and minimized the central bank's loss function. The stabilizing effect of this forward-looking type of expectation was most evident in the case of supply shock. When naive expectations were assumed, the economy reacted to all shocks with significantly bigger and longer-lasting fluctuations. The worst results were obtained assuming adaptive-learning expectations, where shocks lead to large oscillations and the economy stabilized only several years after the shock.},
author = {Eminidou, Snezana and Zachariadis, Marios and Andreou, Elena},
doi = {10.1111/sjoe.12350},
file = {:home/kian/文件/Mendeley Desktop/Eminidou, Zachariadis, Andreou - 2019 - Inflation Expectations and Monetary Policy Surprises.pdf:pdf},
issn = {14679442},
journal = {Scandinavian Journal of Economics},
keywords = {Beliefs,crisis,imperfect information,rational inattention,shocks},
title = {{Inflation Expectations and Monetary Policy Surprises}},
year = {2019}
}
@article{Chiou2012,
abstract = {Motivated by the need for accurate traffic flow prediction in transportation management, we propose a functional data method to analyze traffic flow patterns and predict future traffic flow. In this study we approach the problem by sampling traffic flow trajectories from a mixture of stochastic processes. The proposed functional mixture prediction approach combines functional prediction with probabilistic functional classification to take distinct traffic flow patterns into account. The probabilistic classification procedure, which incorporates functional clustering and discrimination, hinges on subspace projection. The proposed methods not only assist in predicting traffic flow trajectories, but also identify distinct patterns in daily traffic flow of typical temporal trends and variabilities. The proposed methodology is widely applicable in analysis and prediction of longitudinally recorded functional data.},
author = {Chiou, Jeng Min},
doi = {10.1214/12-AOAS595},
file = {:home/kian/文件/Mendeley Desktop/Chiou - 2012 - Dynamical functional prediction and classification, with application to traffic flow prediction.pdf:pdf},
issn = {19417330},
journal = {Annals of Applied Statistics},
keywords = {Clustering,Discrimination,Functional regression,Intelligent transportation system,Mixture model,Subspace projection,Traffic flow rate},
number = {4},
pages = {1588--1614},
title = {{Dynamical functional prediction and classification, with application to traffic flow prediction}},
volume = {6},
year = {2012}
}
@article{Duan2016,
abstract = {This paper studies the intrinsic connection between a generalized LASSO and a basic LASSO formulation. The former is the extended version of the latter by introducing a regularization matrix to the coefficients. We show that when the regularization matrix is even- or under-determined with full rank conditions, the generalized LASSO can be transformed into the LASSO form via the Lagrangian framework. In addition, we show that some published results of LASSO can be extended to the generalized LASSO, and some variants of LASSO, e.g., robust LASSO, can be rewritten into the generalized LASSO form and hence can be transformed into basic LASSO. Based on this connection, many existing results concerning LASSO, e.g., efficient LASSO solvers, can be used for generalized LASSO.},
author = {Duan, Junbo and Soussen, Charles and Brie, David and Idier, J{\'{e}}r{\^{o}}me and Wan, Mingxi and Wang, Yu Ping},
doi = {10.1016/j.sigpro.2016.03.001},
file = {:home/kian/文件/Mendeley Desktop/Duan et al. - 2016 - Generalized LASSO with under-determined regularization matrices.pdf:pdf},
issn = {01651684},
journal = {Signal Processing},
keywords = {Deconvolution,Diagonally dominant,Generalized LASSO,LASSO,Robust LASSO,Solution path,Total variation},
pages = {239--246},
publisher = {Elsevier},
title = {{Generalized LASSO with under-determined regularization matrices}},
url = {http://dx.doi.org/10.1016/j.sigpro.2016.03.001},
volume = {127},
year = {2016}
}
@article{Lu2017,
abstract = {We consider a latent group panel structure as recently studied by Su,
Shi, and Phillips (2016), where the number of groups is unknown and has
to be determined empirically. We propose a testing procedure to
determine the number of groups. Our test is a residual-based Lagrange
multiplier-type test. We show that after being appropriately
standardized, our test is asymptotically normally distributed under the
null hypothesis of a given number of groups and has the power to detect
deviations from the null. Monte Carlo simulations show that our test
performs remarkably well in finite samples. We apply our method to study
the effect of income on democracy and find strong evidence of
heterogeneity in the slope coefficients. Our testing procedure
determines three latent groups among 74 countries.},
author = {Lu, Xun and Su, Liangjun},
doi = {10.3982/qe517},
file = {:home/kian/文件/Mendeley Desktop/Lu, Su - 2017 - Determining the number of groups in latent panel structures with an application to income and democracy.pdf:pdf},
journal = {Quantitative Economics},
keywords = {Classifier Lasso, dynamic panel, latent structure,},
number = {3},
pages = {729--760},
title = {{Determining the number of groups in latent panel structures with an application to income and democracy}},
volume = {8},
year = {2017}
}
@article{Chudik2013,
abstract = {This paper extends the analysis of infinite dimensional vector autoregressive (IVAR) models proposed in Chudik and Pesaran (2011) to the case where one of the variables or the cross-section units in the IVAR model is dominant or pervasive. It is an important extension from empirical as well theoretical perspectives. In the theory of networks a dominant unit is the centre node of a star network and arises as an efficient outcome of a distance-based utility model. Empirically, the extension poses a number of technical challenges that goes well beyond the analysis of IVAR models provided in Chudik and Pesaran. This is because the dominant unit influences the rest of the variables in the IVAR model both directly and indirectly, and its effects do not vanish as the dimension of the model (N) tends to infinity. The dominant unit acts as a dynamic factor in the regressions of the non-dominant units and yields an infinite order distributed lag relationship between the two types of units. Despite this it is shown that the effects of the dominant unit as well as those of the neighborhood units can be consistently estimated by running augmented least squares regressions that include distributed lag functions of the dominant unit and its neighbors (if any). The asymptotic distribution of the estimators is derived and their small sample properties investigated by means of Monte Carlo experiments. {\textcopyright} 2013 Copyright Taylor and Francis Group, LLC.},
author = {Chudik, Alexander and Pesaran, M. Hashem},
doi = {10.1080/07474938.2012.740374},
file = {:home/kian/文件/Mendeley Desktop/Chudik, Pesaran - 2013 - Econometric Analysis of High Dimensional VARs Featuring a Dominant Unit.pdf:pdf},
issn = {07474938},
journal = {Econometric Reviews},
keywords = {Dominant units,Factor models,IVAR models,Large panels,Spatial models,Star networks,Weak and strong cross-section dependence},
number = {5-6},
pages = {592--649},
title = {{Econometric Analysis of High Dimensional VARs Featuring a Dominant Unit}},
volume = {32},
year = {2013}
}
@article{Chudik2011,
abstract = {This paper proposes a novel approach for dealing with the 'curse of dimensionality' in the case of infinite-dimensional vector autoregressive (IVAR) models. It is assumed that each unit or variable in the IVAR is related to a small number of neighbors and a large number of non-neighbors. The neighborhood effects are fixed and do not change with the number of units (N), but the coefficients of non-neighboring units are restricted to vanish in the limit as N tends to infinity. Problems of estimation and inference in a stationary IVAR model with an unknown number of unobserved common factors are investigated. A cross-section augmented least-squares (CALS) estimator is proposed and its asymptotic distribution is derived. Satisfactory small-sample properties are documented by Monte Carlo experiments. An empirical illustration shows the statistical significance of dynamic spillover effects in modeling of US real house prices across the neighboring states. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
author = {Chudik, Alexander and Pesaran, M. Hashem},
doi = {10.1016/j.jeconom.2010.11.002},
file = {:home/kian/文件/Mendeley Desktop/Chudik, Pesaran - 2011 - Infinite-dimensional VARs and factor models.pdf:pdf},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {Factor models,Large N and T panels,Spatial models,VARs,Weak and strong cross-section dependence},
number = {1},
pages = {4--22},
publisher = {Elsevier B.V.},
title = {{Infinite-dimensional VARs and factor models}},
url = {http://dx.doi.org/10.1016/j.jeconom.2010.11.002},
volume = {163},
year = {2011}
}
@article{Hansen2019,
abstract = { We consider inference about coefficients on a small number of variables of interest in a linear panel data model with additive unobserved individual and time specific effects and a large number of additional time-varying confounding variables. We suppose that, in addition to unrestricted time and individual specific effects, these confounding variables are generated by a small number of common factors and high-dimensional weakly dependent disturbances. We allow that both the factors and the disturbances are related to the outcome variable and other variables of interest. To make informative inference feasible, we impose that the contribution of the part of the confounding variables not captured by time specific effects, individual specific effects, or the common factors can be captured by a relatively small number of terms whose identities are unknown. Within this framework, we provide a convenient inferential procedure based on factor extraction followed by lasso regression and show that the procedure has good asymptotic properties. We also provide a simple k -step bootstrap procedure that may be used to construct inferential statements about the low-dimensional parameters of interest and prove its asymptotic validity. We provide simulation evidence about the performance of our procedure and illustrate its use in an empirical application. },
author = {Hansen, Christian and Liao, Yuan},
doi = {10.1017/S0266466618000245},
file = {:home/kian/文件/Mendeley Desktop/Hansen, Liao - 2019 - The factor-lasso and k-step bootstrap approach for inference in high-dimensional economic applications.pdf:pdf},
issn = {14694360},
journal = {Econometric Theory},
number = {3},
pages = {465--509},
title = {{The factor-lasso and k-step bootstrap approach for inference in high-dimensional economic applications}},
volume = {35},
year = {2019}
}
@article{Hsu2011,
abstract = {The random design setting for linear regression concerns estimators based on a random sam-ple of covariate/response pairs. This work gives explicit bounds on the prediction error for the ordinary least squares estimator and the ridge regression estimator under mild assumptions on the covariate/response distributions. In particular, this work provides sharp results on the " out-of-sample " prediction error, as opposed to the " in-sample " (fixed design) error. Our anal-ysis also explicitly reveals the effect of noise vs. modeling errors. The approach reveals a close connection to the more traditional fixed design setting, and our methods make use of recent ad-vances in concentration inequalities (for vectors and matrices). We also describe an application of our results to fast least squares computations.},
author = {Hsu, Daniel and Kakade, Sham and Zhang, Tong},
file = {:home/kian/文件/Mendeley Desktop/Hsu, Kakade, Zhang - 2011 - An analysis of random design linear regression.pdf:pdf},
journal = {arXiv},
pages = {1--30},
title = {{An analysis of random design linear regression}},
url = {http://arxiv.org/abs/1106.2363},
year = {2011}
}
@article{Candes2007,
abstract = {In many important statistical applications, the number of variables or parameters {\$}p{\$} is much larger than the number of observations {\$}n{\$}. Suppose then that we have observations {\$}y=X\backslashbeta+z{\$}, where {\$}\backslashbeta\backslashin\backslashmathbf{\{}R{\}}{\^{}}p{\$} is a parameter vector of interest, {\$}X{\$} is a data matrix with possibly far fewer rows than columns, {\$}n\backslashll p{\$}, and the {\$}z{\_}i{\$}'s are i.i.d. {\$}N(0,\backslashsigma{\^{}}2){\$}. Is it possible to estimate {\$}\backslashbeta{\$} reliably based on the noisy data {\$}y{\$}? To estimate {\$}\backslashbeta{\$}, we introduce a new estimator--we call it the Dantzig selector--which is a solution to the {\$}\backslashell{\_}1{\$}-regularization problem $\backslash$[$\backslash$min{\_}{\{}$\backslash$tilde{\{}$\backslash$b eta{\}}$\backslash$in$\backslash$mathbf{\{}R{\}}{\^{}}p{\}}$\backslash$|$\backslash$tilde{\{}$\backslash$beta{\}}$\backslash$|{\_}{\{}$\backslash$ell{\_}1{\}}$\backslash$quad subject to$\backslash$quad $\backslash$|X{\^{}}*r$\backslash$|{\_}{\{}$\backslash$ell{\_}{\{}$\backslash$infty{\}}{\}}$\backslash$leq(1+t{\^{}}{\{}-1{\}})$\backslash$sqrt{\{}2$\backslash$log p{\}}$\backslash$cdot$\backslash$sigma,$\backslash$] where {\$}r{\$} is the residual vector {\$}y-X\backslashtilde{\{}\backslashbeta{\}}{\$} and {\$}t{\$} is a positive scalar. We show that if {\$}X{\$} obeys a uniform uncertainty principle (with unit-normed columns) and if the true parameter vector {\$}\backslashbeta{\$} is sufficiently sparse (which here roughly guarantees that the model is identifiable), then with very large probability, $\backslash$[$\backslash$|$\backslash$hat{\{}$\backslash$beta{\}}-$\backslash$beta$\backslash$|{\_}{\{}$\backslash$ell{\_}2{\}}{\^{}}2$\backslash$le C{\^{}}2$\backslash$cdot2$\backslash$log p$\backslash$cdot $\backslash$Biggl($\backslash$sigma{\^{}}2+$\backslash$sum{\_}i$\backslash$min($\backslash$beta{\_}i{\^{}}2,$\backslash$sigma{\^{}}2)$\backslash$Biggr).$\backslash$] Our results are nonasymptotic and we give values for the constant {\$}C{\$}. Even though {\$}n{\$} may be much smaller than {\$}p{\$}, our estimator achieves a loss within a logarithmic factor of the ideal mean squared error one would achieve with an oracle which would supply perfect information about which coordinates are nonzero, and which were above the noise level. In multivariate regression and from a model selection viewpoint, our result says that it is possible nearly to select the best subset of variables by solving a very simple convex program, which, in fact, can easily be recast as a convenient linear program (LP).},
author = {Candes, Emmanuel and Tao, Terence},
doi = {10.1214/009053606000001523},
file = {:home/kian/文件/Mendeley Desktop/Candes, Tao - 2007 - The Dantzig selector Statistical estimation when p is much larger than n.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Geometry in high dimensions,Ideal estimation,Linear programming,Model selection,Oracle inequalities,Random matrices,Restricted orthonormality,Sparse solutions to underdetermined systems,Statistical linear model,ℓ1- minimization},
number = {6},
pages = {2313--2351},
title = {{The Dantzig selector: Statistical estimation when p is much larger than n}},
volume = {35},
year = {2007}
}
@article{Nicholson2014,
abstract = {Vector autoregression (VAR) is a fundamental tool for modeling multivariate time series. However, as the number of component series is increased, the VAR model becomes overparameterized. Several authors have addressed this issue by incorporating regularized approaches, such as the lasso in VAR estimation. Traditional approaches address overparameterization by selecting a low lag order, based on the assumption of short range dependence, assuming that a universal lag order applies to all components. Such an approach constrains the relationship between the components and impedes forecast performance. The lasso-based approaches work much better in high-dimensional situations but do not incorporate the notion of lag order selection. We propose a new class of hierarchical lag structures (HLag) that embed the notion of lag selection into a convex regularizer. The key modeling tool is a group lasso with nested groups which guarantees that the sparsity pattern of lag coefficients honors the VAR's ordered structure. The HLag framework offers three structures, which allow for varying levels of flexibility. A simulation study demonstrates improved performance in forecasting and lag order selection over previous approaches, and a macroeconomic application further highlights forecasting improvements as well as HLag's convenient, interpretable output.},
archivePrefix = {arXiv},
arxivId = {1412.5250},
author = {Nicholson, William B. and Wilms, Ines and Bien, Jacob and Matteson, David S.},
eprint = {1412.5250},
file = {:home/kian/文件/Mendeley Desktop/Nicholson et al. - 2014 - High Dimensional Forecasting via Interpretable Vector Autoregression.pdf:pdf},
keywords = {group lasso,multivariate time series,variable selection},
title = {{High Dimensional Forecasting via Interpretable Vector Autoregression}},
url = {http://arxiv.org/abs/1412.5250},
year = {2014}
}
@article{Zheng2018,
abstract = {High-dimensional auto-regressive models provide a natural way to model influence between {\$}M{\$} actors given multi-variate time series data for {\$}T{\$} time intervals. While there has been considerable work on network estimation, there is limited work in the context of inference and hypothesis testing. In particular, prior work on hypothesis testing in time series has been restricted to linear Gaussian auto-regressive models. From a practical perspective, it is important to determine suitable statistical tests for connections between actors that go beyond the Gaussian assumption. In the context of $\backslash$emph{\{}high-dimensional{\}} time series models, confidence intervals present additional estimators since most estimators such as the Lasso and Dantzig selectors are biased which has led to $\backslash$emph{\{}de-biased{\}} estimators. In this paper we address these challenges and provide convergence in distribution results and confidence intervals for the multi-variate AR(p) model with sub-Gaussian noise, a generalization of Gaussian noise that broadens applicability and presents numerous technical challenges. The main technical challenge lies in the fact that unlike Gaussian random vectors, for sub-Gaussian vectors zero correlation does not imply independence. The proof relies on using an intricate truncation argument to develop novel concentration bounds for quadratic forms of dependent sub-Gaussian random variables. Our convergence in distribution results hold provided {\$}T = \backslashOmega((s \backslashvee \backslashrho){\^{}}2 \backslashlog{\^{}}2 M){\$}, where {\$}s{\$} and {\$}\backslashrho{\$} refer to sparsity parameters which matches existed results for hypothesis testing with i.i.d. samples. We validate our theoretical results with simulation results for both block-structured and chain-structured networks.},
archivePrefix = {arXiv},
arxivId = {1812.03659},
author = {Zheng, Lili and Raskutti, Garvesh},
eprint = {1812.03659},
file = {:home/kian/文件/Mendeley Desktop/Zheng, Raskutti - 2018 - Testing for high-dimensional network parameters in auto-regressive models.pdf:pdf},
title = {{Testing for high-dimensional network parameters in auto-regressive models}},
url = {http://arxiv.org/abs/1812.03659},
year = {2018}
}

@article{Wang2014,
abstract = {We propose generalized additive partial linear models for complex data which allow one to capture nonlinear patterns of some covariates, in the presence of linear components. The proposed method improves estimation efficiency and increases statistical power for correlated data through incorporating the correlation information. A unique feature of the proposed method is its capability of handling model selection in cases where it is difficult to specify the likelihood function. We derive the quadratic inference function-based estimators for the linear coefficients and the nonparametric functions when the dimension of covariates diverges, and establish asymptotic normality for the linear coefficient estimators and the rates of convergence for the nonparametric functions estimators for both finite and high-dimensional cases. The proposed method and theoretical development are quite challenging since the numbers of linear covariates and nonlinear components both increase as the sample size increases.We also propose a doubly penalized procedure for variable selection which can simultaneously identify nonzero linear and nonparametric components, and which has an asymptotic oracle property. Extensive Monte Carlo studies have been conducted and show that the proposed procedure works effectively even with moderate sample sizes. A pharmacokinetics study on renal cancer data is illustrated using the proposed method. {\textcopyright} Institute of Mathematical Statistics, 2014.},
author = {Wang, Li and Xue, Lan and Qu, Annie and Liang, Hua},
doi = {10.1214/13-AOS1194},
file = {:home/kian/文件/Mendeley Desktop/Wang et al. - 2014 - Estimation and model selection in generalized additive partial linear models for correlated data with diverging num.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Additive model,Group selection,Model selection,Oracle property,Partial linear models,Polynomial splines,Quadratic inference function,SCAD,Selection consistency},
number = {2},
pages = {592--624},
title = {{Estimation and model selection in generalized additive partial linear models for correlated data with diverging number of covariates}},
volume = {42},
year = {2014}
}
@article{VanDeGeer2014,
abstract = {We propose a general method for constructing confidence intervals and statistical tests for single or low-dimensional components of a large parameter vector in a high-dimensional model. It can be easily adjusted for multiplicity taking dependence among tests into account. For linear models, our method is essentially the same as in Zhang and Zhang [J. R. Stat. Soc. Ser. B Stat. Methodol. 76 (2014) 217-242]: we analyze its asymptotic properties and establish its asymptotic optimality in terms of semiparametric efficiency. Our method naturally extends to generalized linear models with convex loss functions. We develop the corresponding theory which includes a careful analysis for Gaussian, sub-Gaussian and bounded correlated designs.},
author = {{Van De Geer}, Sara and B{\"{u}}hlmann, Peter and Ritov, Ya'acov and Dezeure, Ruben},
doi = {10.1214/14-AOS1221},
file = {:home/kian/文件/Mendeley Desktop/Van De Geer et al. - 2014 - On asymptotically optimal confidence regions and tests for high-dimensional models.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Central limit theorem,Generalized linear model,Lasso,Linear model,Multiple testing,Semiparametric efficiency,Sparsity},
number = {3},
pages = {1166--1202},
title = {{On asymptotically optimal confidence regions and tests for high-dimensional models}},
volume = {42},
year = {2014}
}
@article{Basu2015,
abstract = {Many scientific and economic problems involve the analysis of high-dimensional time series datasets. However, theoretical studies in high-dimensional statistics to date rely primarily on the assumption of independent and identically distributed (i.i.d.) samples. In this work, we focus on stable Gaussian processes and investigate the theoretical properties of {\$}\backslashell {\_}1{\$}-regularized estimates in two important statistical problems in the context of high-dimensional time series: (a) stochastic regression with serially correlated errors and (b) transition matrix estimation in vector autoregressive (VAR) models. We derive nonasymptotic upper bounds on the estimation errors of the regularized estimates and establish that consistent estimation under high-dimensional scaling is possible via {\$}\backslashell{\_}1{\$}-regularization for a large class of stable processes under sparsity constraints. A key technical contribution of the work is to introduce a measure of stability for stationary processes using their spectral properties that provides insight into the effect of dependence on the accuracy of the regularized estimates. With this proposed stability measure, we establish some useful deviation bounds for dependent data, which can be used to study several important regularized estimates in a time series setting.},
author = {Basu, Sumanta and Michailidis, George},
journal = {Annals of Statistics},
doi = {10.1214/15-AOS1315},
file = {:home/kian/文件/Mendeley Desktop/Basu, Michailidis - 2015 - Regularized estimation in sparse high-dimensional time series models.pdf:pdf},
isbn = {9823010102},
issn = {00905364},
keywords = {Covariance estimation,High-dimensional time series,Lasso,Stochastic regression,Vector autoregression},
number = {4},
pages = {1535--1567},
title = {{Regularized estimation in sparse high-dimensional time series models}},
volume = {43},
year = {2015}
}
@article{Athey2019,
abstract = {We propose generalized random forests, a method for non-parametric statistical estimation based on random forests (Breiman, 2001) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. We propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically Gaussian, and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop new methods for three statistical tasks: non-parametric quantile regression, conditional average partial effect estimation, and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf for R and C++, is available from CRAN.},
author = {Athey, Susan and Tibshirani, Julie and Wager, Stefan},
doi = {10.1214/18-AOS1709},
file = {:home/kian/文件/Mendeley Desktop/Athey, Tibshirani, Wager - 2019 - Generalized random forests.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {And phrases,Asymptotic theory,Causal inference,Instrumental variable},
number = {2},
pages = {1179--1203},
title = {{Generalized random forests}},
volume = {47},
year = {2019}
}
@article{Medeiros2019,
abstract = {Inflation forecasting is an important but difficult task. We explore the advances in machine learning (ML) methods and the availability of new datasets to forecast US inflation. Despite the skepticism in the previous literature, we show that ML models with a large number of covariates are systematically more accurate than the benchmarks. The ML method that deserves more attention is the random forest, which dominated all other models. Its good performance is due not only to its specific method of variable selection but also the potential nonlinearities between past key macroeconomic variables and inflation. },
author = {Medeiros, Marcelo C. and Vasconcelos, Gabriel F. R. and Veiga, {\'{A}}lvaro and Zilberman, Eduardo},
doi = {10.1080/07350015.2019.1637745},
file = {:home/kian/文件/Mendeley Desktop/Medeiros et al. - 2019 - Forecasting Inflation in a Data-Rich Environment The Benefits of Machine Learning Methods.pdf:pdf},
issn = {0735-0015},
journal = {Journal of Business {\&} Economic Statistics},
pages = {1--45},
title = {{Forecasting Inflation in a Data-Rich Environment: The Benefits of Machine Learning Methods}},
url = {https://www.tandfonline.com/doi/full/10.1080/07350015.2019.1637745},
year = {2019}
}

@article{Wu2016,
author = {Wu, Wei Biao and Wu, Ying Nian},
doi = {10.1214/16-EJS1108},
file = {:home/kian/文件/Mendeley Desktop/Wu, Wu - 2016 - Performance bounds for parameter estimates of high-dimensional linear models with correlated errors.pdf:pdf},
issn = {19357524},
journal = {Electronic Journal of Statistics},
keywords = {Consistency,Dependence-adjusted norm,Exponential inequality,Functional and predictive dependence measures,Highdimensional time series,Impulse response function,Nagaev inequality,Predictive persistence,Support recovery},
number = {1},
pages = {352--379},
title = {{Performance bounds for parameter estimates of high-dimensional linear models with correlated errors}},
volume = {10},
year = {2016}
}
@article{Han2017,
abstract = {In the last few years, an extensive literature has been focused on the {\$}\backslashell{\_}1{\$} penalized least squares (Lasso) estimators of high dimensional linear regression when the number of covariates {\$}p{\$} is considerably larger than the sample size {\$}n{\$}. However, there is limited attention paid to the properties of the estimators when the errors or/and the covariates are serially dependent. In this study, we investigate the theoretical properties of the Lasso estimators for linear regression with random design under serially dependent and/or non-sub-Gaussian errors and covariates. In contrast to the traditional case in which the errors are i.i.d and have finite exponential moments, we show that {\$}p{\$} can at most be a power of {\$}n{\$} if the errors have only polynomial moments. In addition, the rate of convergence becomes slower due to the serial dependencies in errors and the covariates. We also consider sign consistency for model selection via Lasso when there are serial correlations in the errors or the covariates or both. Adopting the framework of functional dependence measure, we provide a detailed description on how the rates of convergence and the selection consistencies of the estimators depend on the dependence measures and moment conditions of the errors and the covariates. Simulation results show that Lasso regression can be substantially more powerful than the mixed frequency data sampling regression (MIDAS) in the presence of irrelevant variables. We apply the results obtained for the Lasso method to nowcasting mixing frequency data in which serially correlated errors and a large number of covariates are common. In real examples, the Lasso procedure outperforms the MIDAS in both forecasting and nowcasting.},
archivePrefix = {arXiv},
arxivId = {1706.07899},
author = {Han, Yuefeng and Tsay, Ruey S.},
eprint = {1706.07899},
file = {:home/kian/文件/Mendeley Desktop/Han, Tsay - 2017 - High-dimensional Linear Regression for Dependent Observations with Application to Nowcasting.pdf:pdf},
keywords = {casting,consistency,fore-,high-dimensional time series,lasso,mixed-frequency data,model selection,nowcasting},
title = {{High-dimensional Linear Regression for Dependent Observations with Application to Nowcasting}},
url = {http://arxiv.org/abs/1706.07899},
year = {2017}
}


@article{Chudik2011b,
abstract = {This paper introduces the concepts of time-specific weak and strong cross- section dependence, and investigates how these notions are related to the concepts of weak, strong and semi-strong common factors, frequently used for modelling residual cross-section correlations in panel data models. It then focuses on the problems of estimating slope coefficients in large panels, where cross-section units are subject to possibly a large number of unobserved common factors. It is established that the common correlated effects (CCE) estimator introduced by Pesaran remains asymptotically normal under certain conditions on factor loadings of an infinite factor error structure, including cases where methods relying on principal components fail. The paper concludes with a set of Monte Carlo experiments where the small sample properties of estimators based on principal components and CCE estimators are investigated and compared under various assumptions on the nature of the unobserved common effects.},
author = {Chudik, Alexander and Pesaran, M. Hashem and Tosetti, Elisa},
doi = {10.1111/j.1368-423X.2010.00330.x},
isbn = {1368-423X},
issn = {13684221},
journal = {Econometrics Journal},
keywords = {Common correlated effects (CCE) estimator,Panels,Strong and weak cross-section dependence,Weak and strong factors},
number = {1},
title = {{Weak and strong cross-section dependence and estimation of large panels}},
volume = {14},
year = {2011}
}

@article{Chudik2015,
abstract = {This paper extends the Common Correlated Effects (CCE) approach developed by Pesaran (2006) to heterogeneous panel data models with lagged dependent variables and/or weakly exogenous regressors. We show that the CCE mean group estimator continues to be valid but the following two conditions must be satisfied to deal with the dynamics: a sufficient number of lags of cross section averages must be included in individual equations of the panel, and the number of cross section averages must be at least as large as the number of unobserved common factors. We establish consistency rates, derive the asymptotic distribution, suggest using covariates to deal with the effects of multiple unobserved common factors, and consider jackknife and recursive de-meaning bias correction procedures to mitigate the small sample time series bias. Theoretical findings are accompanied by extensive Monte Carlo experiments, which show that the proposed estimators perform well so long as the time series dimension of the panel is sufficiently large.},
author = {Chudik, Alexander and Pesaran, M. Hashem},
doi = {10.1016/j.jeconom.2015.03.007},
file = {:home/kian/文件/Mendeley Desktop/Chudik, Pesaran - 2015 - Common correlated effects estimation of heterogeneous dynamic panel data models with weakly exogenous regressor.pdf:pdf},
issn = {18726895},
journal = {Journal of Econometrics},
keywords = {Coefficient heterogeneity,Common correlated effects,Cross section dependence,Estimation and inference,Lagged dependent variable,Large panels,Unobserved common factors},
number = {2},
pages = {393--420},
publisher = {Elsevier B.V.},
title = {{Common correlated effects estimation of heterogeneous dynamic panel data models with weakly exogenous regressors}},
url = {http://dx.doi.org/10.1016/j.jeconom.2015.03.007},
volume = {188},
year = {2015}
}

@article{Pesaran2006,
abstract = {This paper presents a new approach to estimation and inference in panel data models with a general multifactor error structure. The unobserved factors and the individual-specific errors are allowed to follow arbitrary stationary processes, and the number of unobserved factors need not be estimated. The basic idea is to filter the individual-specific regressors by means of cross-section averages such that asymptotically as the cross-section dimension (N) tends to infinity, the differential effects of unobserved common factors are eliminated. The estimation procedure has the advantage that it can be computed by least squares applied to auxiliary regressions where the observed regressors are augmented with cross-sectional averages of the dependent variable and the individual-specific regressors. A number of estimators (referred to as common correlated effects (CCE) estimators) are proposed and their asymptotic distributions are derived. The small sample properties of mean group and pooled CCE estimators are investigated by Monte Carlo experiments, showing that the CCE estimators have satisfactory small sample properties even under a substantial degree of heterogeneity and dynamics, and for relatively small values of N and T.},
author = {Pesaran, M. Hashem},
doi = {10.1111/j.1468-0262.2006.00692.x},
file = {:home/kian/文件/Mendeley Desktop/Pesaran - 2006 - Estimation and inference in large heterogeneous panels with a multifactor error structure.pdf:pdf},
issn = {00129682},
journal = {Econometrica},
keywords = {Common correlated effects,Cross-section dependence,Estimation and inference,Heterogeneity,Large panels},
number = {4},
pages = {967--1012},
title = {{Estimation and inference in large heterogeneous panels with a multifactor error structure}},
volume = {74},
year = {2006}
}


@article{Lu2016,
abstract = {We consider the problem of determining the number of factors and selecting the proper regressors in linear dynamic panel data models with interactive fixed effects. Based on the preliminary estimates of the slope parameters and factors a la Bai (2009) and Moon and Weidner (2015), we propose a method for simultaneous selection of regressors and factors and estimation through the method of adaptive group Lasso (least absolute shrinkage and selection operator). We show that with probability approaching one, our method can correctly select all relevant regressors and factors and shrink the coefficients of irrelevant regressors and redundant factors to zero. Further, we demonstrate that our shrinkage estimators of the nonzero slope parameters exhibit some oracle property. We conduct Monte Carlo simulations to demonstrate the superb finite-sample performance of the proposed method. We apply our method to study the determinants of economic growth and find that in addition to three common unobserved factors selected by our method, government consumption share has negative effects, whereas investment share and lagged economic growth have positive effects on economic growth.},
author = {Lu, Xun and Su, Liangjun},
doi = {10.1016/j.jeconom.2015.09.005},
file = {:home/kian/文件/Mendeley Desktop/Lu, Su - 2016 - Shrinkage estimation of dynamic panel data models with interactive fixed effects.pdf:pdf},
issn = {18726895},
journal = {Journal of Econometrics},
keywords = {Adaptive Lasso,Dynamic panel,Factor selection,Group Lasso,Interactive fixed effects,Oracle property,Selection consistency},
number = {1},
pages = {148--175},
publisher = {Elsevier B.V.},
title = {{Shrinkage estimation of dynamic panel data models with interactive fixed effects}},
url = {http://dx.doi.org/10.1016/j.jeconom.2015.09.005},
volume = {190},
year = {2016}
}
@article{Durlauf2005,
abstract = {This paper provides a survey and synthesis of econometric tools that have been employed to study economic growth. While these tools range across a variety of statistical methods, they are united in the common goals of first, identifying interesting contemporaneous patterns in growth data and second, drawing inferences on long-run economic outcomes from cross-section and temporal variation in growth. We describe the main stylized facts that have motivated the development of growth econometrics, the major statistical tools that have been employed to provide structural explanations for these facts, and the primary statistical issues that arise in the study of growth data. An important aspect of the survey is attention to the limits that exist in drawing conclusions from growth data, limits that reflect model uncertainty and the general weakness of available data relative to the sorts of questions for which they are employed. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Durlauf, Steven N. and Johnson, Paul A. and Temple, Jonathan R.W.},
doi = {10.1016/S1574-0684(05)01008-7},
file = {:home/kian/文件/Mendeley Desktop/Durlauf, Johnson, Temple - 2005 - Chapter 8 Growth Econometrics.pdf:pdf},
isbn = {9780444520418},
issn = {15740684},
journal = {Handbook of Economic Growth},
keywords = {convergence,estimation,growth determinants,identification,model uncertainty,nonlinearities,parameter heterogeneity},
number = {SUPPL. PART A},
pages = {555--677},
title = {{Chapter 8 Growth Econometrics}},
volume = {1},
year = {2005}
}

@article{Zhao2006,
abstract = {Sparsity or parsimony of statistical models is crucial for their proper interpretations, as in sciences and social sciences. Model selection is a commonly used method to find such models, but usually involves a computationally heavy combinatorial search. Lasso (Tibshirani, 1996) is now being used as a computationally feasible alternative to model selection. Therefore it is important to study Lasso for model selection purposes. In this paper, we prove that a single condition, which we call the Irrepresentable Condition, is almost necessary and sufficient for Lasso to select the true model both in the classical fixed p setting and in the large p setting as the sample size n gets large. Based on these results, sufficient conditions that are verifiable in practice are given to relate to previous works and help applications of Lasso for feature selection and sparse representation. This Irrepresentable Condition, which depends mainly on the covariance of the predictor variables, states that Lasso selects the true model consistently if and (almost) only if the predictors that are not in the true model are "irrepresentable" (in a sense to be clarified) by predictors that are in the true model. Furthermore, simulations are carried out to provide insights and understanding of this result.},
author = {Zhao, Peng and Yu, Bin},
file = {:home/kian/文件/Mendeley Desktop/Zhao, Yu - 2006 - On model selection consistency of Lasso.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Consistency,Lasso,Model selection,Regularization,Sparsity},
pages = {2541--2563},
title = {{On model selection consistency of Lasso}},
volume = {7},
year = {2006}
}
@article{Yuan2006,
abstract = {We consider the problem of selecting grouped variables (factors) for accurate pre- diction in regression. Such a problem arises naturally in many practical situations with themulti- factor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables.We study and propose effi- cient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems.We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.},
author = {Yuan, Ming and Lin, Yi},
file = {:home/kian/文件/Mendeley Desktop/Yuan, Lin - 2006 - Model selection and estimation in regression with.pdf:pdf},
journal = {J R Statist Soc B},
keywords = {analysis of variance,lasso,least angle regression,non-negative garrotte,piecewise linear solution path},
pages = {49--67},
title = {{Model selection and estimation in regression with}},
volume = {68},
year = {2006}
}
@article{Fan2001,
abstract = {Variable selection is fundamental to high-dimensional statistical modeling, including nonparametric regression. Many approaches in use are stepwise selection procedures, which can be computationally expensive and ignore stochastic errors in the variable selection process. In this article, penalized likelihood approaches are proposed to handle these kinds of problems. The proposed methods select variables and estimate coef? cients simultaneously. Hence they enable us to construct con? dence intervals for estimated parameters. The proposed approaches are distinguished from others in that the penalty functions are symmetric, nonconcave on 401ˆ5, and have singularities at the origin to produce sparse solutions. Furthermore, the penalty functions should be bounded by a constant to reduce bias and satisfy certain conditions to yield continuous solutions. A new algorithm is proposed for optimizing penalized likelihood functions. The proposed ideas are widely applicable. They are readily applied to a variety of parametric models such as generalized linear models and robust regression models. They can also be applied easily to nonparametric modeling by using wavelets and splines. Rates of convergence of the proposed penalized likelihood estimators are established. Furthermore, with proper choice of regularization parameters, we show that the proposed estimators perform as well as the oracle procedure in variable selection; namely, they work as well as if the correct submodel were known. Our simulation shows that the newly proposed methods compare favorably with other variable selection techniques. Furthermore, the standard error formulas are tested to be accurate enough for practical applications.},
author = {Fan, Jianqing and Li, Runze},
doi = {10.1198/016214501753382273},
file = {:home/kian/文件/Mendeley Desktop/Fan, Li - 2001 - Variable selection via nonconcave penalized likelihood and its oracle properties.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Hard thresholding,LASSO,Nonnegative garrote,Oracle estimator,Penalized likelihood,SCAD,Soft thresholding},
number = {456},
pages = {1348--1360},
title = {{Variable selection via nonconcave penalized likelihood and its oracle properties}},
volume = {96},
year = {2001}
}

@article{Efron2004,
abstract = {The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.},
author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
file = {:home/kian/文件/Mendeley Desktop/Efron et al. - 2004 - Least angle regression.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {and phrases,boosting,coefficient paths,lasso,linear regression,variable selection},
number = {2},
pages = {407--499},
title = {{Least angle regression}},
volume = {32},
year = {2004}
}

@article{Zou2006,
abstract = {The lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalizing different coefficients in the ?1 penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegative garotte is shown to be consistent for variable selection. The lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalizing different coefficients in the ?1 penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegative garotte is shown to be consistent for variable selection.},
author = {Zou, Hui},
doi = {10.1198/016214506000000735},
file = {:home/kian/文件/Mendeley Desktop/Zou - 2006 - The adaptive lasso and its oracle properties.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Asymptotic normality,Lasso,Minimax,Oracle inequality,Oracle procedure,Variable selection},
number = {476},
pages = {1418--1429},
title = {{The adaptive lasso and its oracle properties}},
volume = {101},
year = {2006}
}


@article{Bickel2009,
abstract = {We exhibit an approximate equivalence between the Lasso estimator and Dantzig selector. For both methods we derive parallel oracle inequalities for the prediction risk in the general nonparametric regression model, as well as bounds on the {\$}\backslashell{\_}p{\$} estimation loss for {\$}1\backslashle p\backslashle 2{\$} in the linear model when the number of variables can be much larger than the sample size.},
author = {Bickel, Peter J. and Ritov, Ya'acov and Tsybakov, Alexandre B.},
doi = {10.1214/08-AOS620},
file = {:home/kian/文件/Mendeley Desktop/Bickel, Ritov, Tsybakov - 2009 - Simultaneous analysis of lasso and dantzig selector.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Linear models,Model selection,Nonparametric statistics},
number = {4},
pages = {1705--1732},
title = {{Simultaneous analysis of lasso and dantzig selector}},
volume = {37},
year = {2009}
}

@article{Han2020,
author = {Han, Yuefeng and Tsay, Ruey S.},
doi = {10.5705/ss.202018.0044},
file = {:home/kian/文件/Mendeley Desktop/Han, Tsay - 2020 - High-dimensional Linear Regression for Dependent Data with Applications to Nowcasting.pdf:pdf},
issn = {10170405},
journal = {Statistica Sinica},
title = {{High-dimensional Linear Regression for Dependent Data with Applications to Nowcasting}},
year = {2020}
}

@article{Wang2007,
abstract = {The least absolute shrinkage and selection operator ('lasso') has been widely used in regression shrinkage and selection. We extend its application to the regression model with autoregressive errors. Two types of lasso estimators are carefully studied. The first is similar to the traditional lasso estimator with only two tuning parameters (one for regression coefficients and the other for autoregression coefficients). These tuning parameters can be easily calculated via a data-driven method, but the resulting lasso estimator may not be fully efficient. To overcome this limitation, we propose a second lasso estimator which uses different tuning parameters for each coefficient. We show that this modified lasso can produce the estimator as efficiently as the oracle. Moreover, we propose an algorithm for tuning parameter estimates to obtain the modified lasso estimator. Simulation studies demonstrate that the modified estimator is superior to the traditional estimator. One empirical example is also presented to illustrate the usefulness of lasso estimators. The extension of the lasso to the autoregression with exogenous variables model is briefly discussed. {\textcopyright} 2007 Royal Statistical Society.},
author = {Wang, Hansheng and Li, Guodong and {Tsai Chin Ling}},
doi = {10.1111/j.1467-9868.2007.00577.x},
file = {:home/kian/文件/Mendeley Desktop/Wang, Li, Tsai Chin Ling - 2007 - Regression coefficient and autoregressive order shrinkage and selection via the lasso.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Autoregression with exogenous variables,Lasso,Oracle estimator,Regression model with autoregressive errors},
number = {1},
pages = {63--78},
title = {{Regression coefficient and autoregressive order shrinkage and selection via the lasso}},
volume = {69},
year = {2007}
}

@article{Yin2019,
author = {Yin, Shou-Yung and Lin, Chang-Ching},
journal = {Working paper},
title = {{A note on CCE estimation in dynamic panel with a multifactor error structure}},
year = {2019}
}
