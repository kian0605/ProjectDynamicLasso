% -*- LaTeX -*-
\documentclass[11pt,a4paper]{article}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{bm}
\usepackage{natbib}
\usepackage{tabularx}
\usepackage{graphicx,lscape}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{enumitem}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{color}
\usepackage{cleveref}


\def\OPFONT{\rm}
\def\E{\mathop{\OPFONT I\hspace{-1.5pt}E\hspace{.13pt}}}
\def\P{\mathop{\OPFONT I\hspace{-1.5pt}P\hspace{.13pt}}}
\def\CAS{\stackrel{\OPFONT a.s.}{\longrightarrow}}
\def\CMS{\stackrel{\OPFONT m.s.}{\longrightarrow}}
\def\CP{\stackrel{p}{\longrightarrow}}
\def\CD{\stackrel{d}{\longrightarrow}}
\def\ED{\stackrel{d}{=}}
\def\SA{\stackrel{A}{\sim}}
\newcommand{\VAR}{\mathop{\OPFONT Var}\nolimits}
\newcommand{\COV}{\mathop{\OPFONT Cov}\nolimits}
\newcommand{\CORR}{\mathop{\OPFONT Corr}\nolimits}
\newcommand{\SPAN}{\mathop{\OPFONT Span}\nolimits}
\newcommand{\RANK}{\mathop{\OPFONT Rank}\nolimits}
\newcommand{\DIAG}{\mathop{\OPFONT Diag}\nolimits}
\newcommand{\TRACE}{\mathop{\OPFONT Trace}\nolimits}
\newcommand{\VEC}{\mathop{\OPFONT vec}\nolimits}

\newcommand{\veps}{\varepsilon}
\newcommand{\Bveps}{\boldsymbol{\varepsilon}}
\newcommand{\Balpha}{\boldsymbol{\alpha}}
\newcommand{\Bbeta}{\boldsymbol{\beta}}
\newcommand{\Bgamma}{\boldsymbol{\gamma}}
\newcommand{\Bdelta}{\boldsymbol{\delta}}
\newcommand{\Bepsilon}{\boldsymbol{\epsilon}}
\newcommand{\Bzeta}{\boldsymbol{\zeta}}
\newcommand{\Beta}{\boldsymbol{\eta}}
\newcommand{\Btheta}{\boldsymbol{\theta}}
\newcommand{\Bvartheta}{\boldsymbol{\vartheta}}
\newcommand{\Biota}{\boldsymbol{\iota}}
\newcommand{\Bkappa}{\boldsymbol{\kappa}}
\newcommand{\Blambda}{\boldsymbol{\lambda}}
\newcommand{\Bmu}{\boldsymbol{\mu}}
\newcommand{\Bnu}{\boldsymbol{\nu}}
\newcommand{\Bxi}{\boldsymbol{\xi}}
\newcommand{\Bpi}{\boldsymbol{\pi}}
\newcommand{\Bvarpi}{\boldsymbol{\varpi}}
\newcommand{\Brho}{\boldsymbol{\rho}}
\newcommand{\Bvarrho}{\boldsymbol{\varrho}}
\newcommand{\Bsigma}{\boldsymbol{\sigma}}
\newcommand{\Bvarsigma}{\boldsymbol{\varsigma}}
\newcommand{\Btau}{\boldsymbol{\tau}}
\newcommand{\Bupsilon}{\boldsymbol{\upsilon}}
\newcommand{\Bphi}{\boldsymbol{\phi}}
\newcommand{\Bvarphi}{\boldsymbol{\varphi}}
\newcommand{\Bchi}{\boldsymbol{\chi}}
\newcommand{\Bpsi}{\boldsymbol{\psi}}
\newcommand{\Bomega}{\boldsymbol{\omega}}

\newcommand{\BGamma}{\boldsymbol{\Gamma}}
\newcommand{\BDelta}{\boldsymbol{\Delta}}
\newcommand{\BTheta}{\boldsymbol{\Theta}}
\newcommand{\BLambda}{\boldsymbol{\Lambda}}
\newcommand{\BXi}{\boldsymbol{\Xi}}
\newcommand{\BPi}{\boldsymbol{\Pi}}
\newcommand{\BSigma}{\boldsymbol{\Sigma}}
\newcommand{\BUpsilon}{\boldsymbol{\Upsilon}}
\newcommand{\BPhi}{\boldsymbol{\Phi}}
\newcommand{\BPsi}{\boldsymbol{\Psi}}
\newcommand{\BOmega}{\boldsymbol{\Omega}}

\newcommand{\MBzero}{\mathbf{0}}
\newcommand{\MBone}{\mathbf{1}}
\newcommand{\MBA}{\mathbf{A}}
\newcommand{\MBa}{\mathbf{a}}
\newcommand{\MBB}{\mathbf{B}}
\newcommand{\MBb}{\mathbf{b}}
\newcommand{\MBC}{\mathbf{C}}
\newcommand{\MBc}{\mathbf{c}}
\newcommand{\MBD}{\mathbf{D}}
\newcommand{\MBd}{\mathbf{d}}
\newcommand{\MBE}{\mathbf{E}}
\newcommand{\MBe}{\mathbf{e}}
\newcommand{\MBF}{\mathbf{F}}
\newcommand{\MBf}{\mathbf{f}}
\newcommand{\MBG}{\mathbf{G}}
\newcommand{\MBg}{\mathbf{g}}
\newcommand{\MBH}{\mathbf{H}}
\newcommand{\MBh}{\mathbf{h}}
\newcommand{\MBI}{\mathbf{I}}
\newcommand{\MBi}{\mathbf{i}}
\newcommand{\MBJ}{\mathbf{J}}
\newcommand{\MBj}{\mathbf{j}}
\newcommand{\MBK}{\mathbf{K}}
\newcommand{\MBk}{\mathbf{k}}
\newcommand{\MBL}{\mathbf{L}}
\newcommand{\MBl}{\mathbf{l}}
\newcommand{\MBM}{\mathbf{M}}
\newcommand{\MBm}{\mathbf{m}}
\newcommand{\MBN}{\mathbf{N}}
\newcommand{\MBn}{\mathbf{n}}
\newcommand{\MBO}{\mathbf{O}}
\newcommand{\MBo}{\mathbf{o}}
\newcommand{\MBP}{\mathbf{P}}
\newcommand{\MBp}{\mathbf{p}}
\newcommand{\MBQ}{\mathbf{Q}}
\newcommand{\MBq}{\mathbf{q}}
\newcommand{\MBR}{\mathbf{R}}
\newcommand{\MBr}{\mathbf{r}}
\newcommand{\MBS}{\mathbf{S}}
\newcommand{\MBs}{\mathbf{s}}
\newcommand{\MBT}{\mathbf{T}}
\newcommand{\MBt}{\mathbf{t}}
\newcommand{\MBU}{\mathbf{U}}
\newcommand{\MBu}{\mathbf{u}}
\newcommand{\MBV}{\mathbf{V}}
\newcommand{\MBv}{\mathbf{v}}
\newcommand{\MBW}{\mathbf{W}}
\newcommand{\MBw}{\mathbf{w}}
\newcommand{\MBX}{\mathbf{X}}
\newcommand{\MBx}{\mathbf{x}}
\newcommand{\MBY}{\mathbf{Y}}
\newcommand{\MBy}{\mathbf{y}}
\newcommand{\MBZ}{\mathbf{Z}}
\newcommand{\MBz}{\mathbf{z}}
\newcommand{\bed}{\begin{displaymath}}
\newcommand{\eed}{\end{displaymath}}
\newcommand{\bel}{\begin{eqalign}}
\newcommand{\eel}{\end{eqalign}}
\newcommand{\bee}{\begin{equation}}
\newcommand{\eee}{\end{equation}}
\newcommand{\tp}{\mathsf{T}}
\newcommand{\si}{$^{*}$}
\newcommand{\ssi}{$^{**}$}
\newcommand{\sssi}{$^{***}$}
\newcommand{\sn}[1]{$\times10^{#1}$}
\newcommand*\oline[1]{%
  \vbox{%
    \hrule height 0.1pt%                  % Line above with certain width
    \kern0.25ex%                          % Distance between line and content
    \hbox{%
      \kern-0.1em%                        % Distance between content and left side of box, negative values for lines shorter than content
      \ifmmode#1\else\ensuremath{#1}\fi%  % The content, typeset in dependence of mode
      \kern-0.1em%                        % Distance between content and left side of box, negative values for lines shorter than content
    }
  }
}
%\numberwithin{equation}{section}
\theoremstyle{definition}
\newtheorem{Ass}{Assumption}%[section]
\newtheorem{thm}{Theorem}%[section]
\newtheorem{lem}{Lemma}%[section]
\newtheorem{coro}{Corollary}%[section]
\newtheorem{Remark}{Remark}%[section]
\newtheorem{Discussion}{Discussion}%[section]
\newtheorem{Prop}{Proposition}
\newtheorem{defin}{Definition}
\newtheorem{prove}{proof}
\newtheorem{Result}{Result}
\newtheorem{Case}{Case}%[section]
\newtheorem{assump}{Assumption}
\newenvironment{myassump}[2][]
  {\renewcommand\theassump{#2}\begin{assump}[#1]}
  {\end{assump}}

\renewcommand{\baselinestretch}{1.2}
\newcommand{\citeposs}[1]{\citeauthor{#1}'s \citeyearpar{#1}}

\begin{document}

\section*{Grant Proposal:}
\noindent{\textsl{Project's Background, Goals, Methods, Procedures,
Anticipated Achievements, Progress, and Benefits to Participants}}
\\
\begin{center}
{\large{\scshape Regularized Estimation in Dynamic Panel with a Multifactor Error Structure}}
\end{center}
\baselineskip=18pt
\parskip=4pt

\section{Introduction}\label{Sec:Intro}
In this project we propose a regularized common correlated effects~(CCE) estimator for dynamic panel data with a multifactor error structure. In the literature, the presence of multifactor error structure provides not only the phenomenon of cross-sectional dependence but also a possible correlation between regressors and unobserved factors. These issues have been studied in \citet{Pesaran2006} and are extended to dynamic panel in \citet{Chudik2015}. In the latter case, an infinite lagged order of cross-sectional averaged dependent variable and regressors are used as a proxy of unobserved factors and it is practically impossible. Therefore, a truncated version of regression is suggested by \citet{Chudik2015}. However, even though we can truncate the regression for each individual, it is still possible that the number of regressors is greater than the sample size. For example, let $p$ denote the number of regressors and $k_T$ denote the truncated order, we can still have $p\times k_T>T$ when both $p$ and $k_T$ are less than $T$. Accordingly, we refer to this situation as a high-dimensional problem and it has not been discussed under this framework.

Over the last few years, a great deal of attention has been focused on the high-dimensional regression, more specifically, the case that $p>T$. To deal with this issue, sparsity of coefficient space is often imposed then $\ell_1$ penalization procedure can be implemented to obtain a consistent estimate. Several related approaches have been introduced, see \citet{Efron2004}, \citet{Zhao2006}, \citet{Candes2007}, \citet{Zou2006}, \citet{Yuan2006}, \citet{Fan2001}, among others. The details of theoretical properties can be founded in \citet{Bickel2009} and \citet{buhlmann2011statistics}.

However, these researches are based on an important assumption that the samples are independent and identically distributed~(i.i.d). The assumption of i.i.d. apparently is challenged because of the presence of serial correlation and cross-sectional dependence in a dynamic panel with a multifactor structure.  Recently, a few papers have attempted to relax the i.i.d assumption and discussed the impacts on the estimate. \citet{Basu2015} introduced using spectral properties of stationary process to measure the stability which is used to derive the upper bounds of the estimation error and to obtain the consistency under a VAR framework. \citet{Han2020} established the consistency using a dependence adjusted norm and did not assume the restricted eigenvalue condition on either the sample or the population covariance matrix.\footnote{\citet{Wang2007} established the consistency of lasso under time series framework when $p<T$.}

Furthermore, the statistical inference in terms of $p$-values and constructing confidence regions is less developed after a penalization procedure. The lack of statistical inference accounting uncertainty makes the usage of such penalization procedure less appealing. To fill this gap, \citet{Zhang2014}, \citet{VanDeGeer2014} and \citet{Javanmard2014} have suggested several ways including de-biased method and multi-sample splitting approach. However, their methods are also based on i.i.d. assumption.

To handle the high-dimensional problem under dynamic panel data with a multifactor error structure framework, we attempt to propose a regularized CCE estimator. Particularly, the whole project can be organized into two parts: (i) the rate of convergence of the proposed estimator and (ii) the statistical inference for this kind of high-dimensional problem. Below we detail the main ideas of these two parts:

\noindent
\textbf{The First Year:} The first half of this project attempts to propose a regularized CCE estimator. Particularly, we first rewrite the original regression with factor structure as an approximation by using an infinite lagged order of cross-sectional averaged dependent variable and regressors. To apply the regularization, we relax the sparsity of the slope coefficient space by discussing the decay rate of slope coefficients and use the dependence adjusted norm suggested by \citet{Han2020} to characterize the dependence among the regressors caused by common factors~(common correlated effects).  Then we can bound the estimation error without using restricted eigenvalue assumption and establish the consistency. The advantages of the proposed method are threefold: (i) The  approach inherits the advantage of the CCE estimator proposed by \citet{Chudik2015}, it can eliminate the unobserved factors asymtotically as the number of cross-sectional units, $N$ goes to infinity. This implies that there is no endogenous problem caused by the correlation between factors and regressors. (ii) This approach can handle with the problem when  $p\times k_T>T$ or even $p>T$ as $T\rightarrow\infty$ under a certain sparsity approximation. (iii) Instead of using the same approximation of unobserved factors  for all individuals, the proposed approach can have different shrinkage property for each individual. Therefore we can expect that the estimate is much more efficient than the CCE estimator.

\noindent
\textbf{The Second Year:} The second half of this project attempts to construct the statistical inference when dealing with the high-dimensional problem mentioned in the first year. To derive the asymptotic normality, we follow the idea proposed by \citet{Zhang2014}. First we concentrate on estimating one slope coefficient of a certain regressor after filtering out the effects from other regressors.\footnote{When the sample size is greater than the number of regressors, it can be referred to Frisch--Waugh--Lovell theorem.} Then we de-bias this estimate by plug-inning the regularized estimate based on the first year. The asymptotic results of this part are highly relying on the result derived from the first year because it will affect how fast the bias part can be controlled. After controlling the bias, we expect that the proposed method can have the asymptotic normality under regular conditions. Then usual statistical inference can be applied.

The potential contributions of this two-year project are listed as follows. First, we propose a regularized CCE estimator to overcome the difficulty when the approximated regression has a high-dimensional problem. This method also deals with the endogenity caused by unobserved factor structure. Second, we establish the consistency and asymptotic normality. Further statistical inference can be made under a suitable sample size of $T$ and a ratio between $N$ and $T$. Thirdly, we use Monte Carlo simulation to provide guidelines for understanding the performance with different sample sizes. Finally, we apply our proposed estimator to study the determinants of economic growth. 

The remainder of this proposal is organized as follows. In Section 2, we introduce the model and briefly review the CCE estimator. In Section 3, we first build the link between augmented regression used to approximate factors and high-dimensional regression, and introduce the regularized estimator. In Section 4, we address how to obtain a de-biased estimate for high-dimensional regression. In Section 5, we outline the research plan for each year, including the tasks of establishing the consistency and asymptotic normality. We also highlight some difficulties which are expected to be solved in this project. We also detail the specifications of data generating process which is used to investigate the small sample property via simulation. Section 6 summarizes the potential benefits to participants  who cooperate this project.


\section{Model and the Review of CCE Estimator}\label{Sec:model}
In this section, we first introduce the dynamic panel with weak exogenous regressors and a multifactor error structure and briefly discuss the main result from \citet{Chudik2015}.

\subsection{Dynamic Panels with Weak Exogenous Regressors and a Multifactor Error Structure}
Consider the following covariance stationary dynamic panels for $i=1,...,N$ and $t=1,...,T$:
\begin{align}
y_{it}&=\rho_iy_{it-1}+\MBx_{it}^{\tp}\Bbeta_{0i}+\MBx_{it-1}^{\tp}\Bbeta_{1i}+u_{it},\label{eq:y}\\
u_{it}&=\Bgamma_{i}^{\tp}\MBf_t+\epsilon_{it},\label{eq:u}
\end{align}
where $y_{it}$ denotes the dependent variable and $\MBx_{it}$ is a $p\times 1$ vector representing the regressors/covariates. To capture the dynamic effects, the lagged dependent variable and regressors are included. $\Bbeta_{0i}$ and $\Bbeta_{1i}$ represent  the corresponding slope coefficients which are unknown, and $u_{it}$ is an error with a multi-factor structure. More specifically, the factor structure is characterized by an unobserved common factors $\MBf_t$ which is an $r\times 1$ vector with the factor loadings $\Bgamma_i$. $\epsilon_{it}$ is an idiosyncratic error.

We further model $\MBx_{it}$ as the following specification:
\begin{align}
\MBx_{it}=\Balpha_iy_{it-1}+\BGamma_i^{\tp}\MBf_t+\MBv_{it}, \label{eq:x}
\end{align}
where $\Balpha_i$ is a $p\times 1$ vector of unknown coefficients, $\BGamma_i$ is an $r\times p$ matrix of factor loadings and $\MBv_{it}$ is a $p\times 1$ vector of idiosyncratic errors. Equations \eqref{eq:y} to \eqref{eq:x} are general enough for providing features including heterogeneous slope coefficients and possible correlation between regressors and multifactor errors.

\subsection{Common Correlated Effects Estimator}
The main interest of this project is to estimate the unknown coefficients $\Bpi_i=(\rho_i, \Bbeta_{0i}^{\tp}, \Bbeta_{1i}^{\tp})^{\tp}$. However, it is not trivial because the unobserved common factors play an important role leading to an endogeneity and therefore biased estimates could be obtained if these unobserved common factors are not controlled well. Common correlated effects estimator proposed by \cite{Chudik2015} is designed for overcoming this issue. 

Let $\MBz_{it}=(y_{it},\MBx_{it}^{\tp})_{}^{\tp}$ and we can rewrite equations \eqref{eq:y} to \eqref{eq:x}  as
\begin{align}
\MBA_{0i}\MBz_{it}=\MBA_{1i}\MBz_{it-1}+\MBC_{i}\MBf_{t}+\MBe_{it},
\end{align}
where $\MBC_{i}=(\Bgamma_i,\BGamma_i)^{\tp}$,
\begin{align*}
\MBA_{0i}=\begin{bmatrix}
1 & -\Bbeta_{0i}^{\tp}\\
\MBzero & \MBI
\end{bmatrix},\quad \MBA_{1i}=\begin{bmatrix}
\rho_i & \Bbeta_{1i}^{\tp}\\
\Balpha_i & \MBzero
\end{bmatrix},
\end{align*}
and $\MBe_{it}=(\epsilon_{it},\MBv_{it}^{\tp})^{\tp}$. By imposing the assumption that $\MBA_{0i}$ is invertible  for any $i$, we can obtain the following VAR(1) representation on $\MBz_{it}$,
\begin{align}
\MBz_{it}=\MBA_{i}\MBz_{it-1}+\MBA_{0i}^{-1}\MBC_{i}\MBf_{t}+\MBe_{z,it},
\end{align}
where $\MBA_{i}=\MBA_{0i}^{-1}\MBA_{1i}$ and $\MBe_{z,it}=\MBA_{0i}^{-1}\MBe_{it}$. Assume that the support of $\MBA_{i}$ lies in the unit circle for all $i$, this VAR(1) representation can further rewrite as
\begin{align}
\MBz_{it}=\sum_{\ell=0}^{\infty}\MBA_{i}^{\ell}\left(\MBA_{0i}^{-1}\MBC_{i}\MBf_{t-\ell}+\MBe_{z,it-\ell}\right).
\end{align}
Under the large $N$ framework, the cross-sectional average of $\MBz_{it}$ follows that
\begin{align}
\bar\MBz_{t}&=\sum_{\ell=0}^{\infty}\mathbb{E}\left(\MBA_{i}^{\ell}\MBA_{0i}^{-1}\MBC_{i}\right)\MBf_{t-\ell}+O_p(N^{-1/2})\notag\\
                          &=\BLambda(L)\MBC\MBf_{t}+O_p(N^{-1/2}),\label{eq:zbar}
\end{align}
where $\BLambda(L)=\mathbb{E}\left(\sum_{\ell=0}^{\infty}\MBA_{i}^{\ell}\MBA_{0i}^{-1}L^{\ell}\right)$ and $\MBC=\mathbb{E}(\MBC_{i})$. Multiplying the above equation by the inverse of $\BLambda(L)$ yields the key relationship between the unobserved factors and the cross-sectional average of $\MBz_{it}$, that is,
\begin{align}
\MBf_{t}=(\MBC^{\tp}\MBC)^{-1}\MBC^{\tp}\BLambda(L)^{-1}\bar\MBz_{t}+O_p(N^{-1/2}).\label{eq:fhat}
\end{align}
The above equation provides that the linear combination of cross-sectional average of $\MBz_{it}$ and their lags can be regarded as proxies of $\MBf_t$ when the rank condition holds~($\RANK(\MBC)=r$). By plug-inning \eqref{eq:fhat} into \eqref{eq:y}, we can obtain the following result,
\begin{align}
y_{it}&=\rho_iy_{it-1}+\MBx_{it}^{\tp}\Bbeta_{0i}+\MBx_{it-1}^{\tp}\Bbeta_{1i}+\Bdelta_{i}^{\tp}(L)\bar\MBz_{t}+\epsilon_{it}+O_p(N^{-1/2}),\label{eq:y*}
\end{align}
where $\Bdelta_{i}^{\tp}(L)=\sum_{\ell=0}^{\infty}\Bdelta_{i\ell}^{\tp}L^{\ell}=\Bgamma_{i}^{\tp}\left((\MBC^{\tp}\MBC)^{-1}\MBC^{\tp}\BLambda(L)^{-1}\right)$. Practically, it is impossible to consider an infinite lagged $\bar\MBz_{t}$, therefore, a common strategy is to consider a truncated version using a finite lagged $\bar\MBz_{t}$. Theoretically, the truncated order can be much lower than $T$, say $T^{1/3}$, and it still provides nice statistical properties in terms of consistency and asymptotic normality. Therefore, we can consider the following truncated augmented regression:
\begin{align}
y_{it}&=\rho_iy_{it-1}+\MBx_{it}^{\tp}\Bbeta_{0i}+\MBx_{it-1}^{\tp}\Bbeta_{1i}+\sum_{\ell=0}^{k_{T}}\Bdelta_{i\ell}^{\tp}\bar\MBz_{t-\ell}+\tilde\epsilon_{it},\label{eq:yhat}
\end{align}

Now, let
\begin{align}
\MBM_{\mathbf{z}}=\MBI_{T-k_T} - \bar{\MBQ}(\bar{\MBQ}'\bar{\MBQ})^{-}\bar{\MBQ}',
\end{align}
where
\begin{align*}
\bar{\MBQ}=\begin{bmatrix}
1& \bar\MBz_{k_T+1}^{\tp}& \bar\MBz_{k_T}^{\tp}&\dots& \bar\MBz_{1}^{\tp}\\
1& \bar\MBz_{k_T+2}^{\tp}& \bar\MBz_{k_T+1}^{\tp}&\dots& \bar\MBz_{2}^{\tp}\\
\vdots & \vdots & \vdots & &\vdots \\
1& \bar\MBz_{T}^{\tp}& \bar\MBz_{T-1}^{\tp}&\dots& \bar\MBz_{T-k_T}^{\tp}
\end{bmatrix}
\end{align*}
and $(\bar{\MBQ}'\bar{\MBQ})^{-}$ denotes the Moore-Penrose generalized inverse of $\bar{\MBQ}'\bar{\MBQ}$. Also let
\begin{align*}
\BXi_i =
\begin{bmatrix}
y_{ik_T} & \MBx_{ik_T+1}^{\tp} & \MBx_{ik_T}^{\tp}\\
y_{ik_T+1} & \MBx_{ik_T+2}^{\tp} & \MBx_{ik_T+1}^{\tp}\\
\vdots & \vdots & \vdots\\
y_{iT-1} & \MBx_{iT}^{\tp} & \MBx_{iT-1}^{\tp}\\
\end{bmatrix}
\end{align*}
and $\MBy_{i}=(y_{ik_T+1}, \cdots,  y_{iT})^{\tp}$.
We obtain the CCE estimator of $\Bpi_i=(\rho_i, \Bbeta_{0i}^{\tp}, \Bbeta_{1i}^{\tp})^{\tp}$:
\begin{equation}
\hat\Bpi_{i} = (\BXi_i^{\tp} \MBM_{\MBz} \BXi_i)^{-1}(\BXi_i^{\tp} \MBM_{\MBz} \MBy_i).
\label{eq:CCE}
\end{equation}
Thus, the CCEMG estimator is
\begin{equation}
\hat\Bpi_{\mathrm{MG}} = \frac{1}{N}\sum_{i=1}^N \hat\Bpi_{i}.
\label{eq:CCEMG}
\end{equation}

\begin{Remark}
Under the assumptions stated in \citet{Chudik2015} and when the rank condition holds, the CCE estimator defined in equation \eqref{eq:CCE}  is a consistent estimator, that is $\hat\Bpi_{i}\CP\Bpi_{i}$ when $(N,T,k_T)\rightarrow\infty$ jointly such that $k_T^3/T\rightarrow\kappa$, $0<\kappa<\infty$. The intuition is that the effects from unobserved factors can be removed asymptotically when $N\rightarrow\infty$. Therefore, individual's estimation can be treated as a time series regression and the consistency is based on large $T$. This nice property can be directly applied to establish the asymptotic normality of the CCEMG estimator defined in equation \eqref{eq:CCEMG}.  That is $\sqrt{N}(\hat\Bpi_{\mathrm{MG}}-\Bpi)\CD\mathrm{N}(\MBzero, \BOmega_{\Bpi})$ and $\BOmega_{\Bpi}$ is a symmetric nonnegative definite matrix characterizing the random deviation of $\Bpi_i$ and $\Bpi=\mathbb{E}(\Bpi_i)$.
\end{Remark}

\begin{Remark}
The rank condition plays an important role on deriving the consistency and asymptotic normality of $\hat\Bpi_{\mathrm{MG}}$. When the rank condition does not hold, a more restrictive assumption that the unobserved factors are serially uncorrelated should be imposed. \citet{Yin2019} have discussed this assumption and showed that CCEMG estimator is still valid when there is a small number of factors that they are persistent. In the whole paper, we assume that the rank condition holds.
\end{Remark}


\section{Regularized CCE Estimator~(First Year)}\label{Sec:plan1}
The CCE estimator proposed by \cite{Pesaran2006} and \cite{Chudik2015} provides a simple idea by using a projection matrix to control the unobserved factors in static and dynamic panels respectively. However, in the dynamic panel model, when the factors are represented by an infinite lagged order of $\bar\MBz_t$ in equation \eqref{eq:y*}, the infinite order refers to a high-dimensional regression issue that has not been discussed yet in this literature. In their paper, they avoid dealing with this issue by firstly considering a truncated version of infinite lagged order of $\bar\MBz_t$ and secondly calculating the projection matrix with Moore-Penrose generalized inverse when $\bar{\MBQ}^{\tp}\bar{\MBQ}$ is deficient. However, practically, it does not imply that \eqref{eq:yhat} would not suffer from the dramatic loss of degree of freedom. 

To emphasis this problem, we rewrite \eqref{eq:y*} as the following high-dimensional representation:
\begin{align}
y_{it}&=\Xi_{it}^{\tp}\Bpi_i+\tilde\Bdelta_{i}^{\tp}\tilde\MBz_{t}+\epsilon_{it}+O_p(N^{-1/2}),\label{eq:yhigh}
\end{align}
where $\Xi_{it}=(y_{it-1},\MBx_{it}^{\tp},\MBx_{it-1}^{\tp})^{\tp}$, $\tilde\Bdelta_{i}=(\Bdelta_{i0}^{\tp},\Bdelta_{i1}^{\tp},...,\Bdelta_{ik}^{\tp})^{\tp}$ and $\tilde\MBz_{t}=(\bar\MBz_{t}^{\tp},\bar\MBz_{t-1}^{\tp},...,\bar\MBz_{t-k}^{\tp})^{\tp}$. Recall that the dimension of $\MBx_{it}$ is $p$, we can immediately obtain that $\dim(\Xi_{it})=2p+1$ and $\dim(\tilde\MBz_{t})=p(k_{T}+1)$. Since $k_{T}\rightarrow\infty$, equation \eqref{eq:yhigh} can be regarded as a partial high-dimensional problem but  still have a finite dimension of $\MBx_{it}$ when $p$ is fixed. If we further let $p\rightarrow\infty$, \eqref{eq:yhigh} becomes a purely high-dimensional regression.

%\begin{Remark}
%It is worth to mention that under fixed $p$ framework,  
%\end{Remark}

%\subsection{When $p$ Is Fixed}
Under fixed $p$ and large $k_T$ framework, $k_T$ can be assumed that grows as fast as the sample size $T$. Estimation and inference can be made under the sparsity on the coefficients $\tilde\Bdelta_{i}$ for each $i$. In general, we assume that the number of elements in $S_{\delta}=\{j:\tilde\Bdelta_{i}\neq 0\}$ is small and does not grow with the sample size. Then we can impose a regularization with $\ell_1$ norm, that is we add an $\ell_1$ penalty to the linear regression. In this project, the problem can be written as:
\begin{align}
\min_{\Bpi_i\in\mathbb{R}^{2p+1},\tilde\Bdelta_{i}\in\mathbb{R}^{p(k+1)}}\frac{1}{2}\sum_{t=1}^T(y_{it}-\Xi_{it}^{\tp}\Bpi_i-\tilde\Bdelta_{i}^{\tp}\tilde\MBz_{t})^2+\lambda||\tilde\Bdelta_{i}||_1,\label{eq:lasso1}
\end{align}
where $\lambda\geq 0$ is a tuning parameter and it is chosen by a rule depending on the dependency of regressors. 

\begin{Remark}
Instead of pure sparsity, the above problem can be referred as the transformed generalized lasso, see \cite{Tibshirani2011}, where the sparsity has a certain structure corresponding to a desired behavior for the slope coefficients. That is in equation \eqref{eq:lasso1}, the penalty only covers a part of the coefficients. Clearly, we can have a solution for $\Bpi_i$,
\begin{align}
\hat\Bpi_i=(\BXi_i^{\tp}\BXi_i)^{-1}\BXi_i^{\tp}(\MBy-\tilde{\MBZ}\hat{\tilde\Bdelta}_{i}).\label{eq:est_pi}
\end{align}
Therefore, the above problem \eqref{eq:lasso1} can be rewritten as 
\begin{align}
\min_{\tilde\Bdelta_{i}\in\mathbb{R}^{p(k+1)}}\frac{1}{2}||\MBM_{\Xi}\MBy_{i}-\MBM_{\Xi}\tilde{\MBZ}\tilde\Bdelta_{i}||_2^2+\lambda||\tilde\Bdelta_{i}||_1\label{eq:lasso1_1}
\end{align}
where $\MBM_{\Xi}=\MBI-\MBP_{\Xi}$ and $\MBP_{\Xi}=\BXi_i(\BXi_i^{\tp}\BXi_i)^{-1}\BXi_i^{\tp}$ the projection matrix on the column space of $\BXi_i$.
\end{Remark}

\begin{Remark}
In this project, the main interest is $\Bpi_i$. However, to obtain the estimate of $\Bpi_i$, we need to plugin the estimates of $\tilde\Bdelta_{i}$ into equation \eqref{eq:est_pi}.  
Accordingly, we can observe that under fixed $p$ and large $k$ framework, the asymptotic property of $\hat\Bpi_i$ should be affected by the limited behavior of $\hat{\tilde\Bdelta}_{i}$ even though there is no sparsity assumption on the coefficient space of $\Bpi_i$.
\end{Remark}


\begin{Remark}
Alternatively, we can let $p$ grows faster than the sample size $T$. Let $\Btheta_i=(\Bpi_i^{\tp},\tilde\Bdelta_{i}^{\tp})^{\tp}$ and $\MBw_{it}=(\Xi_{it}^{\tp},\tilde\MBz_{t}^{\tp})^{\tp}$, we can further impose the sparsity on $\Btheta_i$, that is $S_{\Btheta}=\{j:\Btheta_i\neq 0\}$ and $|S_{\Btheta}|$ is fixed. Then we have a regular lasso problem,
\begin{align}
\min_{\Btheta_i\in\mathbb{R}^{k}}\frac{1}{2}\sum_{t=1}^T(y_{it}-\MBw_{it}^{\tp}\Btheta_i)^2+\lambda||\Btheta_i||_1.\label{eq:lasso2}
\end{align}
\end{Remark}

\begin{Remark}
The above problem is quite related to \cite{Hansen2019} who consider a panel partial factor model~(PPFM). More specifically, PPFM is defined as,\footnote{For simplicity, we remove time effects and individual effects and use their notations.}
\begin{align}
y_{it} &= \Balpha \MBd_{it}+\Bxi_i^{\tp}\MBf_t+\Btheta^{\tp}\MBu_{it}+\epsilon_{it},\label{eq:HL_y}\\
\MBd_{it} &= \Bdelta_{di}^{\tp}\MBf_t+\Bgamma_d^{\tp}\MBu_{it}+\Beta_{it},\label{eq:HL_d}\\
\MBx_{it} &= \Blambda_i^{\tp}\MBf_t+\MBu_{it}.\label{eq:HL_x}
\end{align}
In equation \eqref{eq:HL_y}, $\MBd_{it}$ represents a low-dimensional treatment variables, $\MBf_t$ denotes unobserved factors, and $\MBu_{it}$ is a high-dimensional unobserved effects from $\MBx_{it}$. In their paper, they focus on estimating $\Balpha$. The main difference is that we do not restrict the dimension of $\MBd_{it}$ and it is an important assumption which is used to obtain asymptotic normality. Instead, we can allow that $\dim(\MBd_{it})\rightarrow\infty$.
\end{Remark}

\section{Valid Condidence Regions and Tests for Regularized CCE Estimator~(Second Year)}\label{Sec:plan2}

In the previous section, we have stated the regularized linear regression under a dynamic panel data framework with a multifactor error structure. While the lasso is expected to provide the consistency for $\Bpi_i$ and $\tilde{\Bdelta}_i$ when $p$ is fixed and for $\Btheta_i$ when $p$ goes to infinity respectively, it is also important to have a statistical inference after estimation.  The corresponding statistical inference involves that how to calculate $p$-values and construct the confidence regions. 

Under high-dimensional framework, several important methods have developed for making the statistical inference. We refer to \citet{Dezeure2015} as a comprehensive review and discussion. Basically, there are two approaches to obtain normality or asymptotic normality. The first one is sample splitting and the second one is de-sparsifying. In this project, we follow the second approach and we will provide the discussion later.

De-sparsifying is introduced by \citet{Zhang2014}. The basic idea is to use low dimensional projection to derive the limiting behavior of estimated coefficients. To illustrate the idea, we consider the following high-dimensional linear regression:
\begin{align*}
\MBy=\MBX\Bbeta+\Bveps.
\end{align*}
Notice that for simplicity, we abuse the notations. More specifically, $\MBy\in\mathbb{R}^{T}$ representing the dependent variable, $\MBX=(\MBx_1,...,\MBx_p)\in\mathbb{R}^{T\times p}$ denoting the regressors with the corresponding slope coefficients $\Bbeta=(\beta_1,...,\beta_p)^{\tp}$ and $\Bveps$ is the error term. Let $\MBz_{j}$ denote the residuals from regressing $\MBx_{j}$ on $\MBX_{-j}$ via lasso estimate, and $\MBX_{-j}$ represents all regressors except $\MBx_{j}$, $j=1,...,p$. Since $\MBz_{j}$ is one dimensional, we can obtain the estimate of $\beta_{j}$ by the following OLS estimate,
\begin{align*}
\hat\beta_{j}^{(ols)}=\frac{\MBz_{j}^{\tp}\MBy}{\MBz_{j}^{\tp}\MBx_{j}},
\end{align*}
and it can be rewritten as 
\begin{align*}
\frac{\MBz_{j}^{\tp}\MBy}{\MBz_{j}^{\tp}\MBx_{j}}&=\frac{\MBz_{j}^{\tp}\left(\MBx_j\beta_j+\sum_{k\neq j}\MBx_{k}\beta_k+\Bveps\right)}{\MBz_{j}^{\tp}\MBx_{j}}\\
&=\beta_j+\sum_{k\neq j}\MBp_{jk}\beta_k+\frac{\MBz_{j}^{\tp}\Bveps}{\MBz_{j}^{\tp}\MBx_{j}},
\end{align*}
where $\MBp_{jk}=\MBz_{j}^{\tp}\MBx_{k}/\MBz_{j}^{\tp}\MBx_{j}$. Since $\MBz_{j}$ is not exactly orthogonal to $ \MBx_{k}$ under lasso type estimation, the second term in the above equation can be regarded as a bias term.  \citet{Zhang2014} proposed to plug a lasso estimate of $\beta_k$ based on using all regressors and to correct the bias term, that is 
\begin{align*}
\hat\beta_{j}=\hat\beta_{j}^{(ols)}-\sum_{k\neq j}\MBp_{jk}\hat\beta_k^{(lasso)}.
\end{align*}
This de-sparsifying estimate can be shown that it has a Gaussian distribution when the errors are assumed to be Gaussian; otherwise we can apply central limit theorem~(CLT) to obtain asymptotic normality when we impose some conditions on errors. An inference can be made for both cases.

\subsection{De-sparsifying the regularized CCE Estimator}
Based on the de-sparsifying estimate, we can apply this approach to the regularized CCE estimator and further make the statistical inference. Recall equation \eqref{eq:lasso1_1} and let $\Bvarpi_{j}$ denote the residuals from regressing $\MBM_{\Xi}\tilde{\MBz}_{j}$ on $\MBM_{\Xi}\tilde\MBZ_{-j}$, and $\tilde\MBZ_{-j}$ represents all regressors except $\tilde{\MBz}_{j}$, $j=1,...,p(k+1)$. Since $\Bvarpi_{j}$ is one dimensional, for each $i$, we can obtain the estimate of $\tilde\delta_{ij}$ by the following OLS estimate,
\begin{align}
\hat{\tilde\delta}_{ij}^{(ols)}=\frac{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\MBy_{i}}{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\tilde{\MBz}_{j}}.
\end{align}
Similarly, we can have the bias corrected version of $\hat{\tilde\delta}_{ij}^{(ols)}$,
\begin{align}
\hat{\tilde\delta}_{ij}=\hat{\tilde\delta}_{ij}^{(ols)}-\frac{\Bvarpi_{j}^{\tp}\left(\sum_{k\neq j}\MBM_{\Xi}\tilde{\MBz}_{k}\hat{\tilde\delta}_{ik}^{(lasso)}\right)}{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\tilde{\MBz}_{j}}.\label{eq:despars}
\end{align}
The above bias corrected estimate of $\hat{\tilde\delta}_{ij}$ is expected to have a nice property as \citet{Zhang2014}. 

\begin{Remark}
Essentially, de-sparsifying estimate does not provide a model selection because $\hat{\tilde\delta}_{ij}$ is estimated based on OLS estimate and this approach does not have the shrinkage property. \citet{Zhang2014} suggested to use a threshold estimator,
\begin{align}
\hat{\tilde\delta}_{ij}^{(thr)}=sgn(\hat{\tilde\delta}_{ij})(|\hat{\tilde\delta}_{ij}|-\hat{t}_{ij})^{+},
\end{align}
where $\hat{\tilde\delta}_{ij}$ is obtained from equation \eqref{eq:despars} and $\hat{t}_{ij}$ is the corresponding  $t$-statistic. They have shown that the threshold estimator can achieve the model selection with a high probability. Moreover, it does not need to assume that $\tilde\delta_{ij}$ cannot be too small in the active set, and it is referred to as signal strength or beta-min condition. Accordingly, threshold estimator can possibly select many small non-zero $|\tilde\delta_{ij}|$ which is not selected by the regularized estimator. 
\end{Remark}

\begin{Remark}
An alternative approach to make a statistical inference as we mentioned before is sample splitting. The intuition is that we can do model selection in the first half sample and make a statistical inference using the second half sample.  To be specifically, let $I_1$ and $I_2$ represent the sample indices for two equal halves. If we assume that two samples are independent, then the variable selection based on the first half of the sample is given by
\begin{align}
\hat{S}(I_1)\subset\{1,...,p\}.
\end{align}
From this approach, usually $|\hat{S}(I_1)|$ is less than the sample size of the second half of sample. Then we can simply use the OLS estimation using the second sample with variables selected by $\hat{S}(I_1)$ and construct the test using $t$-test assuming Gaussian errors or relying on CLT. There are two drawbacks of this approach. First, we need to separate the sample into two halves and it implies that we only have a half of the sample size.  Second, the result is sensitive to the choice of how to split the sample. In order to overcome the second issue, we can use $B$ different splitting samples and calculate $B$ different $p$-values then aggregate these $p$-values by an appropriate approach. The details can be founded in \citet{Wasserman2009}.
\end{Remark}

%\begin{Remark}
%As we mentioned in Reamrk, when $p$ is not fixed, the problem is a purely high-dimensional linear regression.....
%\end{Remark}



\section{The Plan of Future Work}

\subsection{The Research Plan for the First Year}
In the first year, there are two main tasks needed to be finished, including (i) establishing the asymptotic properties of the regularized CCE estimator defined in equations \eqref{eq:est_pi} and \eqref{eq:lasso1_1} and (ii) investigating the finite sample properties in different data generating processes via Monte Carlo simulation.
\subsubsection{Asymptotic Properties}
To understand the limiting behavior of the regularized CCE estimator, we need to deal with two important issues. First, since we approximate the unobserved factors by an infinite lagged $\bar\MBz_t$, there is no clear definition of the sparsity. The reason is that while the corresponding coefficients tend to be zero when $\ell$ becomes larger, they are not exactly zero. Therefore we need to link the sparsity and decaying coefficients  to enable us to use regularized estimator. Second, the presence of cross-sectional dependence. This dependency characterized by the factor structure leads a problem that the maximal eigenvalue may not be bounded as an usual assumption used in the literature. We briefly discuss these two issues as follows:

\begin{Discussion}
Compared to compatibility condition, under high-dimensional setting, another assumption, beta-min assumption can also be imposed to investigate the asymptotic property of lasso estimator. In this project, when $p$ is fixed, the high-dimensional problem comes from that we need an infinite order of lags to approximate the unobserved factors. When the support of $\rho_i$ lies strictly inside the unit circle, following the argument from \citet{Chudik2013}, the coefficients in $\Bdelta_i(L)$ defined in equation \eqref{eq:y*} also decays at an exponential rate. More specifically, 
there is a relationship between $\rho_i$ and $\Bdelta_i(L)$, that is
\begin{align}
|\delta_{ij\ell}|\leq K\rho_i^{\ell},\label{eq:exp_decay}
\end{align}
where $K$ is a constant for $\ell \in\mathbb{N}$. However, the above result implies that in equation \eqref{eq:y*}, the coefficients in $\Bdelta_i(L)$ are non-zero but very small when $\ell$ becomes larger. Therefore, the active set $S_{\Bdelta}$ defined in section 3.1 could be very large. In this project, we aim to find an approximation of this infinite order regression but would not suffer from the loss of approximation error via lasso. We can first define a sparse approximation $\tilde\Bdelta_{i}^{*\tp}\tilde\MBz_{t}$ of $\tilde\Bdelta_{i}^{\tp}\tilde\MBz_{t}$ and $S_{\Bdelta}^{*}=\{j:\tilde\delta_{ij}^*\neq 0\}$ and $|S_{\Bdelta}^{*}|$ is small. The basic idea is that the approximate error in this part should be small. Theoretically, it can be regarded as that the bias term is not larger than the variance term and this condition holds if we assume that
\begin{align}
|\tilde\delta_{ij}|\leq 7\lambda.
\end{align}
where $\lambda=O(\sqrt{\frac{\log T}{T}})$. The details can be founded in \citet{buhlmann2011statistics}. When this assumption holds, we can further plug the result into \eqref{eq:exp_decay} and obtain
\begin{align}
                         &  K\rho_i^{\ell}\leq 7\lambda=O(\sqrt{\frac{\log T}{T}})\\
\Rightarrow &  K^2\rho_i^{2\ell}\leq 49\lambda^2=O(\frac{\log T}{T})\\
\Rightarrow &  TK^2\rho_i^{2\ell}\leq\log(T)\\
\Rightarrow & \ell\geq K(\log(T)-\log\log(T)).
\end{align}
Roughly speaking, as long as $\ell$  is greater than $K(\log(T)-\log\log(T))$, the corresponding coefficients are small enough and they can be considered as a sparse approximation.
\end{Discussion}

Now we turn our focus to the cross-sectional dependence. 
\begin{defin}[Weak and strong cross-sectional dependence, \citet{Chudik2011b}]
Consider a $N$ factor model:
\begin{align}
\mathfrak{z}_{it}=\gamma_{i1}f_{1t}+\gamma_{i2}f_{2t}+...+\gamma_{iN}f_{Nt}+\mathfrak{e}_{it},
\end{align}
then the process $\mathfrak{z}_{it}$ is cross-sectionally weakly dependent if factors are weak~($\lim_{N\rightarrow\infty}\sum_{i=1}^N|\gamma_{i\ell}|<\infty$) and is cross-sectionally strongly dependent if and only if there exists at least one factor satisfying $\lim_{N\rightarrow\infty}N^{-1}\sum_{i=1}^N|\gamma_{i\ell}|<\infty$. 
\end{defin}

\begin{Discussion}
Since in this project the regressors  are constructed as $\tilde\MBz_{t}=(\bar\MBz_{t}^{\tp},\bar\MBz_{t-1}^{\tp},...,\bar\MBz_{t-k}^{\tp})^{\tp}$ for each individual regression, the correlation among these regressors plays a very important role determining the performance of lasso.  An usual assumption of restricted eigenvalue may not easy to be imposed because $\tilde\MBz_{t}$ can also be linked to a factor structure mentioned in equation \eqref{eq:zbar}. 
When the factors are strong, $\lim_{N\rightarrow\infty}N^{-1}||\MBC||_1$ is bounded. Let $\VAR(\tilde\MBz_{t})=\BSigma_t$, then we can obtain that 
\begin{align}
\phi_{\mathrm{max}}^{1/2}(\BSigma_t)\geq \frac{||\MBC||_1}{\sqrt{N}}.
\end{align}
where $\phi_{\mathrm{max}}$ denotes the maximal eigenvalue of submatrices of $\VAR(\tilde\MBz_{t})=\BSigma_t$. The above result implies that the maximal eigenvalue could diverge. This property may lead to a problem when we assume both minimal and maximal eigenvalues of submatrices of $\VAR(\tilde\MBz_{t})=\BSigma_t$ denoted by $\phi_{\mathrm{min}}$ and $\phi_{\mathrm{max}}$ are bounded and they have the following relationship, 
\begin{align}
m\phi_{\mathrm{min}}(s+m)>c_0^2s\phi_{\mathrm{max}}(m),
\end{align}
where $s$ and $m$ are some integers such that $1\leq s\leq p(k+1)/2$, $m\geq s$ and $s+m\leq p(k+1)$. $c_0$ is a constant greater than 0. To avoid the potential issue of unbounded eigenvalues. We plan to follow \citet{Han2020}, and to define the functional dependence measure and its corresponding dependence adjusted norm for $p(k+1)$-dimensional stationary process ($\tilde\MBz_{t}$):
\begin{align}
\omega_{t,q}&=||\max_{1\leq j\leq p(k+1) }|\bar{z}_{tj}-\bar{z}_{tj}^*|||;\\
|||\bar{\MBz}_{\cdot}|_{\infty}||_{q,\alpha}&=\sup_{m\geq 0}(m+1)^{\alpha}\Omega_{m,q},\quad \alpha\geq 0,
\end{align}
where $\bar{z}_{tj}^*$ is the coupled process of $\bar{z}_{tj}$ and $\Omega_{m,q}=\sum_{t=m}^{\infty}\omega_{t,q}$. The above functional dependence measure and dependence adjusted norm provide an good measure which can be used to bound the deviation of the estimate of the gram matrix $\mathbb{E}(\tilde\MBz_{t}\tilde\MBz_{t}^{\tp})$. Therefore by using Lemmas 6.1 and 6.3 in \citet{buhlmann2011statistics}, the consistency of $\hat\Bpi_i$ and $\hat{\tilde{\Bdelta}}_{i}$ should be established.
\end{Discussion}

\subsubsection{Simulation}
Monte Carlo simulations will be adopted to investigate the small sample properties. More specifically, we plan to understand the performance of the proposed estimator under different values of $p$ with different degrees of cross-sectional dependence caused by unobserved factors.

\noindent
\textbf{Data Generating Process:}
We consider the following data generating process,
for $i=1,...,N$ and $t=1,...,T$,
\begin{align*}
y_{it}=&\rho_iy_{it-1}+\sum_{j=1}^p\beta_{0ij}x_{ijt}+\sum_{j=1}^p\beta_{1ij}x_{ijt-1}+u_{it},\\
u_{it}=&\sum_{\ell=1}^{r_{s}}\gamma_{i\ell}f_{\ell t}+\sum_{\ell=1}^{r_{w}}\vartheta_{i\ell}n_{\ell t}+\veps_{it},
\end{align*}
and
\begin{align*}
x_{ijt}=\alpha_{ij}y_{it-1}+\sum_{\ell=1}^{r_s}\Gamma_{ij\ell}f_{\ell t}+\sum_{\ell=1}^{r_{w}}\Theta_{ij\ell}n_{\ell t}+v_{ijt}.
\end{align*}
We define two types of unobserved factors $f_{\ell t}$ and $n_{\ell t}$ representing strong and weak factors. They are generated  as an AR(1) process with unit variance:
\begin{align*}
f_{\ell t} =&\rho_{f\ell}f_{t-1, \ell} +\nu_{f\ell t},\quad \nu_{f\ell t}\sim \mathrm{IIDN} (0, 1-\rho^{2}_{f\ell}), \\
n_{\ell t} =&\rho_{n\ell}n_{t-1, \ell} +\nu_{n\ell t},\quad \nu_{n\ell t}\sim \mathrm{IIDN} (0, 1-\rho^{2}_{n\ell}).
\end{align*}
We assume that there are $r_{s}$ strong factors, therefore, the sum of the absolute values of the corresponding loadings is unbounded in $N$ as we mentioned in Definition 1, and it is generated as
\begin{align*}
\gamma_{i\ell} =& \gamma_{\ell}+\iota_{i\ell},\quad \iota_{i\ell} \sim \mathrm{IIDN} (0, \sigma^{2}_{\gamma\ell}),\\
\Gamma_{ij\ell} =& \Gamma_{j\ell}+\xi_{ij\ell},\quad \xi_{ij\ell} \sim \mathrm{IIDN} (0, \sigma^{2}_{\gamma x \ell}),
 \end{align*}
for $\ell=1,.., r_s$ and $i=1,\ldots,N$. We set $\sigma^{2}_{\gamma\ell}=\sigma^{2}_{\Gamma j\ell}=0.2^2$. To ensure that the variance of $u_{it}$ does not rise with $r_{s}$, we also set
$\gamma_{\ell}=\sqrt{b_{\gamma\ell}}$  and
$\Gamma_{j\ell}=\sqrt{\ell b_{\Gamma \ell}}$ with
$b_{\gamma\ell}=1/r - \sigma^{2}_{\gamma\ell}$ and
$b_{\Gamma\ell}=2/[r(r+1)]-2\sigma^{2}_{\Gamma  \ell}/(r+1)$. As for weak factors, the corresponding loadings are given by
\begin{align*}
\vartheta_{i\ell}=\frac{\eta_{\vartheta i\ell}}{2\sum_{i=1}^N\eta_{\vartheta i\ell}},\quad \eta_{\vartheta i\ell}\sim\mathrm{IIDU}(0,1), \\
\Theta_{ij\ell}=\frac{\eta_{\Theta i\ell}}{2\sum_{i=1}^N\eta_{\Theta i\ell}},\quad \eta_{\Theta i\ell}\sim\mathrm{IIDU}(0,1),
\end{align*}
for $\ell =1,...,r_{w}$ and $i=1,...,N$. We can observe that for each $\ell$, $\sum_{i=1}^N\vartheta_{i\ell}$ and $\sum_{i=1}^N\Theta_{i\ell}$ are $O(1)$. Therefore, as $N\rightarrow\infty$, the $R^2$~(R-squared) for each $i$ is affected by the strong factors even though $r_{w}\rightarrow\infty$.

We generate the dependent variable and regressors based on the following VAR(1) representation:
\[
\MBz_{it} =\MBA_{0i}^{-1}\MBA_{1i}\MBz_{it-1} +  \MBA_{0i}^{-1}\MBC_{i}^{s}\MBf_{t}+ \MBA_{0i}^{-1}\MBC_{i}^{w}\MBn_{t}+ \MBA_{0i}^{-1}\MBe_{it},
\]
where $\MBe_{it}=(\veps_{it}, \MBv_{it}^{\tp})^{\tp}$,
\begin{align*}
\MBA_{0i}= \left(\begin{array}{cc}
         1 & -\Bbeta_{0i}^{\tp} \\
                     0 & \MBI_{1\times p}  \\
                   \end{array}
                 \right),\quad \MBA_{1i}=\left(
        \begin{array}{cc}
          \rho_{i} & \Bbeta_{1i}^{\tp} \\
          \Balpha & \MBzero_{p\times p} \\
        \end{array}
      \right),
\end{align*}
$\MBC_{i}^{s}=(\Bgamma_{i},\BGamma_{i})^{\tp}$, $\MBC_{i}^{w}=(\Bvartheta_{i},\BTheta_{i})^{\tp}$, $\MBf_t=(f_{1 t},...,f_{r_s t})^{\tp}$  and $\MBn_t=(n_{1 t},...,n_{r_w t})^{\tp}$. Particularly, $\Bgamma_{i}=(\gamma_{i1},...,\gamma_{ir_s})^{\tp}$, $\BGamma_{i}=(\BGamma_{i1},...,\BGamma_{ip})$, $\Bvartheta_{i}=(\vartheta_{i1},...,\vartheta_{ir_w})^{\tp}$, $\BTheta_{i}=(\BTheta_{i1},...,\BTheta_{ip})$, $\BGamma_{ij}=(\Gamma_{ij1},...,\Gamma_{ijr_{s}})^{\tp}$ and $\BTheta_{ij}=(\Theta_{ij1},...,\Theta_{ijr_{w}})^{\tp}$.  For the errors in $x_{ijt}$, we assume that  $v_{ijt} =\rho_{xi}v_{ijt} +\nu_{ijt}$, $\Bnu_{it}=(\nu_{i1t},...,\nu_{ipt})^{\tp}$, and
\begin{align*}
\Bnu_{it}\sim \mathrm{IIDN} \left(\MBzero, \BSigma\right),
\end{align*}
where $\BSigma$ is a matrix that the diagonal elements are ones and off-diagonal elements are $\tau$'s. The errors $\veps_{it}$ in $y_{it}$ follows $\mathrm{IIDN} (0, 1)$.

As for the parameters in $\MBA_{0i}$ and $\MBA_{1i}$. To make $\{\MBz_{it}\}$ to be stationary, it is required that the eigenvalues of $\MBA_{0i}^{-1}\MBA_{1i}=\MBA_{i}$ lie inside the unit circle. More specifically, for each replication, we generate $\beta_{0i}$ as $\mathrm{IIDU}(0.3,1)$ and set
$\beta_{1i}=-0.3$. Furthermore, we generate $\phi_i$ from $\mathrm{IIDU}(0.7,0.9)$, and $\alpha_{ij}$ from $\mathrm{IIDU}(0,0.15)$.

\noindent
\textbf{Experiments:} We plan to investigate the performance of the proposed estimator based on the following cases with $r_s=3$~(the rank condition is satisfied) under different sample sizes of $T$ and $N$:
\begin{enumerate}
\item Case 1~(fixed $p$ and $r_w=0$): This case can be regarded as a benchmark because it is the case considered in \citep{Chudik2015} where $p=1$ and no weak factors are imposed. This implies that for each $i$, as long as $T$ is large enough, a truncated version of CCE estimator can be implemented. Therefore, by using this case, we can compare the performance between the the CCE estimator and the regularized CCE estimator.
\item Case 2~(fixed $p$ and $r_w$ or $r_w\rightarrow\infty$): In this case, we follow the idea of \citep{Chudik2011b} and consider the case including fixed number of weak factors and the case that the number of weak factors goes to infinity. Since the sum of  absolute values of loadings of weak factors is bounded, some kind of sparsity of loadings are expected and it will affect the behavior of the augmented regression defined in \eqref{eq:HL_y}. Using regularized CCE estimator is expected to has a good approximation of this sparsity.
\item Case 3~($p>T$ and $r_w=0$): In this case, we allow for diverged $p$. To be more specifically, we consider the cardinality of the active set with $|S_{\Btheta}|=6$. That is for any values of $p$, there are only six non-zero coefficients. For simplicity, we forced that once the coefficient of $x_{ijt}$ is non-zero, the coefficient of $x_{ijt-1}$ is non-zero too. Therefore, when $|S_{\Btheta}|=6$, there are three relevant regressors and thier lags having impacts on $y_{it}$. Since $p>T$ the usual CCE estimator does not work. Instead, we use regularized CCE estimator to estimate the coefficients.
\item Case 4~($p>T$ and $r_w\rightarrow\infty$): In the last case, we want to understand the performance of the regularized CCE estimator under the most complicated case with diverged $p$ and diverged number of weak factors.
\end{enumerate}



\subsection{The Research Plan for the Second Year}
\subsubsection{Asymptotic Properties}
In the second year, we ill devote ourselves on the asymptotic properties of the de-sparsifying estimator defined in equation \eqref{eq:despars}. The analytical work of understanding the limiting behavior is related to \citet{Zhang2014} but with a major difference. That is under our framework, regressors are possibly highly correlated and they have serial correlation. To understand these issues, we first decompose $\hat{\tilde{\delta}}_{ij}^{(ols)}$  as follows,
\begin{align}
\frac{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\MBy_{i}}{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\tilde{\MBz}_{j}}&=\frac{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\left(\Bvarpi_{j}\tilde\delta_{ij}+\sum_{k\neq j}\Bvarpi_{k}\tilde\delta_{ik}+\Bveps_{i}+O_p(N^{-1/2}\right)}{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\tilde{\MBz}_{j}}\\
&=\tilde\delta_{ij}+\frac{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\left(\sum_{k\neq j}\Bvarpi_{k}\tilde\delta_{ik}\right)}{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\tilde{\MBz}_{j}}+\frac{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\Bveps_{i}}{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\tilde{\MBz}_{j}}+\frac{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\left(O_p(N^{-1/2}\right)}{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\tilde{\MBz}_{j}}
\end{align}
The second equality is based on equation \eqref{eq:yhigh} with the notations defined in section 4.1. To correct the bias, we can plug the lasso estimator $\hat{\tilde\delta}_{ik}^{(lasso)}$ into the above equation based on using all regressors $\tilde\MBZ$, that is
\begin{align}
\hat{\tilde\delta}_{ij}&=\hat{\tilde\delta}_{ij}^{(ols)}-\frac{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\left(\sum_{k\neq j}\Bvarpi_{k}\hat{\tilde\delta}_{ik}^{(lasso)}\right)}{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\tilde{\MBz}_{j}}\\
&=\tilde\delta_{ij}+\frac{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\left(\sum_{k\neq j}\Bvarpi_{k}\left(\tilde\delta_{ik}-\hat{\tilde\delta}_{ik}^{(lasso)}\right)\right)}{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\tilde{\MBz}_{j}}+\frac{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\Bveps_{i}}{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\tilde{\MBz}_{j}}+\frac{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\left(O_p(N^{-1/2})\right)}{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\tilde{\MBz}_{j}}
\end{align}
Using the above results, we obtain
\begin{align}
\sqrt{T}\left(\hat{\tilde\delta}_{ij}-\tilde\delta_{ij}\right)=&\sqrt{T}\frac{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\left(\sum_{k\neq j}\Bvarpi_{k}\left(\tilde\delta_{ik}-\hat{\tilde\delta}_{ik}^{(lasso)}\right)\right)}{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\tilde{\MBz}_{j}}+\sqrt{T}\frac{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\Bveps_{i}}{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\tilde{\MBz}_{j}}+\frac{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\left(O_p(\sqrt{\frac{T}{N}})\right)}{\Bvarpi_{j}^{\tp}\MBM_{\Xi}\tilde{\MBz}_{j}}\\
=&\mathcal{I}_1+\mathcal{I}_2+\mathcal{I}_3.
\end{align}
By ignoring $\mathcal{I}_1$ and $\mathcal{I}_3$ we can easily show that $\mathcal{I}_2$ has an asymptotic normality. 

\begin{Remark}
However,  a more challenged part is to bound the error of the estimated bias correction term in $\mathcal{I}_1$. The reason is that the presence of serial correlation and cross correlation among regressors. Therefore, we need to further investigate the properties of $\left(\tilde\delta_{ik}-\hat{\tilde{\delta}}_{ik}^{(lasso)}\right)$ and $T^{-1}\Bvarpi_{j}^{\tp}\MBM_{\Xi}\sum_{k\neq j}\Bvarpi_{k}$. The former one should be established from the result in the first year and the second one is expected to be finished in the second year. 
\end{Remark}

\begin{Remark}
Regardless of the serial correlation and cross correlation among the regressors, we can show that $||\left(\tilde\delta_{ik}-\hat{\tilde{\delta}}_{ik}^{(lasso)}\right)||_1=O_p(s\sqrt{\log p/T})$,
$\max_{k\neq j}2|T^{-1}T^{-1}\Bvarpi_{j}^{\tp}\MBM_{\Xi}\sum_{k\neq j}\Bvarpi_{k}|\leq \lambda_j$ and $\lambda_j\asymp\sqrt{\log p/T}$. Accordingly, we can have $\mathcal{I}_1=O_p\left(s\frac{\log p}{\sqrt{T}}\right)$. Along with $\frac{T}{N}\rightarrow 0$, we can conclude that $\mathcal{I}_1$ and $\mathcal{I}_3$ are negligible asymptotically. 
\end{Remark}


\subsubsection{Simulation}
In this part, Monte Carlo simulation is used to understand the performance in terms of investigating the power and familywise error rate~(FWER) of the proposed method. We follow the same data generating process defined in Section 5.1.2.

As for the $p$-values, we investigate multiple testing $p$-values for two-sided testing of the null hypothesis $H_{0,j}:\beta_{0,j}=0$ for all $j$. We report the power and FWER based on:
\begin{align*}
\mathrm{Power}=\sum_{j\in S}\mathbb{P}[H_{0,j}\mbox{ is rejected}]/s,\\
\mathrm{FWER}=\mathbb{P}[\mbox{all }j\in S^c:H_{0,j}\mbox{ is rejected}].
\end{align*}

We will also consider other approach such as multi-sample splitting and compare the finite sample performance based on the above criteria for all experiments designed for different values of $p$ and $r_w$.
\subsubsection{Empirical Study}

We plan to apply the regularized CCE estimator to study the determinants of economic growth. Investigating the determinants can help us to understand how to stimulate the economy. However, in the literature, too many possible determinants have been discussed under different models, different sample periods and different countries, see \citet{Durlauf2005} for a comprehensive review.
Therefore, there is no guidance for selecting the importance determinants for researcher, especially when the number of the determinants is greater than the sample size. \citet{Lu2016} have considered a panel data framework with interactive effects to understand which variables play an important role on economic growth. However, their results are established on assuming the slope coefficients are homogeneous and the number of regressors are less than the sample size.

Similar to \citet{Lu2016}, we consider a dynamic panel data framework with a multifactor error structure. But we would like to extend the data used in their paper first. To be specifically, more variables should be considered without imposing any restriction. In their paper, only nine variables have been used which is far less than the number mentioned in \citet{Durlauf2005}. Therefore, we shall consider as many as possible. Moreover, the sample period can be extended from 2005 to 2018 which implies that at least we can have $T=49$~(1970--2018). It is expected the proposed  regularized CCE estimator can achieve the model selection and estimation at the same time under a high-dimensional framework.


\section{Potential Benefits to Participants}\label{Sec:expectations}
To conduct the above simulations and application, I plan to collaborate with my research assistant(s). We plan to use \textsc{Matlab} or \textsc{R} to conduct the simulation and empirical applications. Thus, an assistant who is familiar with  programming and data collecting would be extremely helpful.

As I mentioned, participants will learn basic concepts of programming and familiarize themselves with commonly used econometric tools in panel data analysis via investigating the finite sample properties with simulations and conducting the economic applications. Furthermore, this project heavily involves the issues of high-dimensional regression which is a very important issue not only for researches but also for industry nowadays. Therefore, this knowledge can be a good starting point for them to further develop new advanced econometric methods for future study or to apply these tools when they are going to the industry.


\bigskip


\bibliographystyle{ecta}
\bibliography{lasso}

\end{document}












